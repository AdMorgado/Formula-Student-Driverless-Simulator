{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Formula Student Driverless Simulator Welcome to the FSDS documentation. This home page contains an index with a brief description of the different sections in the documentation. Feel free to read in whatever order preferred. In any case, here are a few suggestions for newcomers. Get familiar with the architecture. The system overview introduces you to the ideas and concepts of the system. Launch the simulator and ros bridge. Follow the getting started guide . Participating in FS-Online 2020? Participants of the FS-Online 2020 virtual event should start by reading the system overview and integration handbook . Next, follow the steps in the getting started guide to get you up and running.","title":"Home"},{"location":"#formula-student-driverless-simulator","text":"Welcome to the FSDS documentation. This home page contains an index with a brief description of the different sections in the documentation. Feel free to read in whatever order preferred. In any case, here are a few suggestions for newcomers. Get familiar with the architecture. The system overview introduces you to the ideas and concepts of the system. Launch the simulator and ros bridge. Follow the getting started guide .","title":"Formula Student Driverless Simulator"},{"location":"#participating-in-fs-online-2020","text":"Participants of the FS-Online 2020 virtual event should start by reading the system overview and integration handbook . Next, follow the steps in the getting started guide to get you up and running.","title":"Participating in FS-Online 2020?"},{"location":"camera/","text":"Cameras To add a camera to your vehicle, add the following json to the \"Cameras\" map in your settings.json : \"Camera1\": { \"CaptureSettings\": [{ \"ImageType\": 0, \"Width\": 785, \"Height\": 785, \"FOV_Degrees\": 90 }], \"X\": 1.0, \"Y\": 0.06, \"Z\": -2.20, \"Pitch\": 0.0, \"Roll\": 0.0, \"Yaw\": 180 } Camera1 is the name of the camera. This name will be used in ros topics and in coordinate frame. X , Y and Z are the position of the lidar relative the vehicle pawn center of the car in NED frame. Roll , Pitch and Yaw are rotations in degrees. ImageType describes the type of camera. At this moment only rgb and depth cameras are supported. For rgb camera, set this value to 0 and for depth camera set the value to 2. Depth Cameras (aka DepthPerspective) act as follows: each pixel is given a float value in meters corresponding to the smallest distance from the camera to that point. Images published in ros are encoded in 32FC1 RGB images are just your normal video camera. Images published in ros are encoded using bgr8 FOV_Degrees describes how much the camera sees . The vertical FoV will be automatically calculated using the following formula: vertical FoV = image height / image width * horizontal FoV .","title":"Camera"},{"location":"camera/#cameras","text":"To add a camera to your vehicle, add the following json to the \"Cameras\" map in your settings.json : \"Camera1\": { \"CaptureSettings\": [{ \"ImageType\": 0, \"Width\": 785, \"Height\": 785, \"FOV_Degrees\": 90 }], \"X\": 1.0, \"Y\": 0.06, \"Z\": -2.20, \"Pitch\": 0.0, \"Roll\": 0.0, \"Yaw\": 180 } Camera1 is the name of the camera. This name will be used in ros topics and in coordinate frame. X , Y and Z are the position of the lidar relative the vehicle pawn center of the car in NED frame. Roll , Pitch and Yaw are rotations in degrees. ImageType describes the type of camera. At this moment only rgb and depth cameras are supported. For rgb camera, set this value to 0 and for depth camera set the value to 2. Depth Cameras (aka DepthPerspective) act as follows: each pixel is given a float value in meters corresponding to the smallest distance from the camera to that point. Images published in ros are encoded in 32FC1 RGB images are just your normal video camera. Images published in ros are encoded using bgr8 FOV_Degrees describes how much the camera sees . The vertical FoV will be automatically calculated using the following formula: vertical FoV = image height / image width * horizontal FoV .","title":"Cameras"},{"location":"gcp-remote-workstation/","text":"How to deploy a remote workstation on Google Cloud Platform In this tutorial we create an gcp instance that is configured for running and developing the Formula-Student-Driverless-Simulator. 1. Create a new instance and configure ports We assume you are familiar with google cloud configurations. Create a new instance in the region of your choice. Minimum requirements are: 12 vCPU 24GB memory Any NVIDIA GPU 150GB Disk Recommended specs are: 16 vCPU 32GB memory 1 NVIDIA Tesla P100 300GB SSD Disk CPU platform Sandy Bridge Do not enable NVIDIA GRID Choose Windows Server 2019 Datacenter Check 'Attach display device' and 'enable NVidia Grid' Ensure this instance has a public ip. 2. Access the remote desktop Go to the cloud instance and use the 'Set Windows password' to set a password for your user. From Ubuntu Install a remote desktop client: sudo apt-get install remmina Launch Remmina, add a new connection. Set Server to the ip of the instance username to the name you entered when setting the password password to the password you created before Color depth to `High color (16 bpp) From Windows Click the little arrow next to the 'RDP' button and click 'Download the RDP file'. Double click on the downloaded file. Use the credentials you just created to login. 3. Disable internet security We need to disable internet security protection because we want to download a bunch of tools. Start the Server Manager. Select Local Server (The server you are currently on and the one that needs IE Enhanced Security disabled) On the right side of the Server Manager, find the IE Enhanced Security Configuration Setting. Disable it. Open Internet Options, go to tab 'Security' set the security level for 'internet' to 'Medium'. Disable 'Protection Mode'. Now you can use internet explorer to downlaod firefox. 4. Install NVIDIA drivers Follow this tutorial to install the required nvidia drivers. Restart your computer. Validate the installation by running the following command in a powershell terminal & 'C:\\Program Files\\NVIDIA Corporation\\NVSMI\\nvidia-smi.exe' 5. Install .NET and Windows Subsystem for Linux Start the Server Manager. Click 'Manage', 'Add Roles and Features' Click Next until you find 'Server Roles'. In 'Server Roles', select 'Remote Desktop Services' In 'Features', select '.NET Framework 3.5' and 'Windows Subsystem for Linux'. Click install. You can ignore the warning about missing source files. Restart the computer","title":"Google cloud remote workstation"},{"location":"gcp-remote-workstation/#how-to-deploy-a-remote-workstation-on-google-cloud-platform","text":"In this tutorial we create an gcp instance that is configured for running and developing the Formula-Student-Driverless-Simulator.","title":"How to deploy a remote workstation on Google Cloud Platform"},{"location":"gcp-remote-workstation/#1-create-a-new-instance-and-configure-ports","text":"We assume you are familiar with google cloud configurations. Create a new instance in the region of your choice. Minimum requirements are: 12 vCPU 24GB memory Any NVIDIA GPU 150GB Disk Recommended specs are: 16 vCPU 32GB memory 1 NVIDIA Tesla P100 300GB SSD Disk CPU platform Sandy Bridge Do not enable NVIDIA GRID Choose Windows Server 2019 Datacenter Check 'Attach display device' and 'enable NVidia Grid' Ensure this instance has a public ip.","title":"1. Create a new instance and configure ports"},{"location":"gcp-remote-workstation/#2-access-the-remote-desktop","text":"Go to the cloud instance and use the 'Set Windows password' to set a password for your user.","title":"2. Access the remote desktop"},{"location":"gcp-remote-workstation/#from-ubuntu","text":"Install a remote desktop client: sudo apt-get install remmina Launch Remmina, add a new connection. Set Server to the ip of the instance username to the name you entered when setting the password password to the password you created before Color depth to `High color (16 bpp)","title":"From Ubuntu"},{"location":"gcp-remote-workstation/#from-windows","text":"Click the little arrow next to the 'RDP' button and click 'Download the RDP file'. Double click on the downloaded file. Use the credentials you just created to login.","title":"From Windows"},{"location":"gcp-remote-workstation/#3-disable-internet-security","text":"We need to disable internet security protection because we want to download a bunch of tools. Start the Server Manager. Select Local Server (The server you are currently on and the one that needs IE Enhanced Security disabled) On the right side of the Server Manager, find the IE Enhanced Security Configuration Setting. Disable it. Open Internet Options, go to tab 'Security' set the security level for 'internet' to 'Medium'. Disable 'Protection Mode'. Now you can use internet explorer to downlaod firefox.","title":"3. Disable internet security"},{"location":"gcp-remote-workstation/#4-install-nvidia-drivers","text":"Follow this tutorial to install the required nvidia drivers. Restart your computer. Validate the installation by running the following command in a powershell terminal & 'C:\\Program Files\\NVIDIA Corporation\\NVSMI\\nvidia-smi.exe'","title":"4. Install NVIDIA drivers"},{"location":"gcp-remote-workstation/#5-install-net-and-windows-subsystem-for-linux","text":"Start the Server Manager. Click 'Manage', 'Add Roles and Features' Click Next until you find 'Server Roles'. In 'Server Roles', select 'Remote Desktop Services' In 'Features', select '.NET Framework 3.5' and 'Windows Subsystem for Linux'. Click install. You can ignore the warning about missing source files. Restart the computer","title":"5. Install .NET and Windows Subsystem for Linux"},{"location":"getting-started/","text":"Getting started When running this simulator there are two main components: the simulator and the ros bridge. Your autonomous system and the ros bridge should use the same ros core, either by running on the same computer or through a network connection. It is required that the simulator and the ros bridge have the same version! This page is an overview of the different methods to get these components up and running. Running the simulator To run the simulation smoothly you need quite a fast windows computer with a modern videocard. We highly recommend the following computer specs. You might be able to run with less power but everything will be slower. For developing this project, you need quite a good computer because Unreal Engine is a heavy baby. 8 core 2.3Ghz CPU 12 GB memory 30GB free SSD storage (120GB when building the unreal project from source) Recent NVidia card with Vulkan support and 3 GB of memory. (You can check the video card drivers by running vulkaninfo ) If your computer does not suffice you can use a remote workstation on Google Cloud Platform. Read this tutorial on how to setup your virtual workstation. The simulator will load settings from the file Formula-Student-Driverless-Simulator/settings.json in your home directory . This file is required for the simulator to work and contains the sensor configuration of the car. If you clone the repo you will already have this file in place. If not, copy-paste the contents of the settings.json file at the root of this repository into the ~/Formula-Student-Driverless-Simulator . This should get you started with the default sensor configuration, feel free to try your own custom sensor suite. From release binaries The simulator is distributed as binaries on every release. At this moment only windows binaries are released. For now, if you are on Ubuntu you will have to run the simulator from the Unreal Engine Editor. During competition, the simulation will run on Windows because it offers a bit better performance and stability. Go to releases and download the latest one. Unzip it to anywhere on your computer and launch FSDS.exe. A window with a car should popup! Try driving the car around using the arrowkeys. From source using the Unreal Engine Editor If you want to run the unreal engine project from source you will need unreal engine and visual studio 2019 . On Ubuntu you can skip the visual studio 2019 part. Get the repository You can either download the repo using the big green download button on the github page of this project or clone the repository. For cloning, checkout the documentation on this further down this page. When downloading or cloning, by default you get the latest, unreleased version. This is probably not the version that you want. Make sure you select the version that you need! Compiling the AirSim plugin The Unreal Engine project requires the AirSim plugin. We have to compile this plugin first. The AirSim plugin is made up of AirLib (/AirSim/AirLib) and the plugin code (/UE4Project/Plugins/AirSim/Source). AirLib is the shared code between the ros wrapper and the AirSim Unreal Engine plugin. First build the AirLib code. On Windows, open the Developer Command Prompt for VS 2019 , go to Formula-Student-Driverless-Simulator/AirSim and run build.cmd On Ubuntu, go to folder AirSim and run setup.sh and build.sh . So what does build.cmd or setup.sh+build.sh do? It downloads any nessesary libraries and compiles AirLib. After compilation it places the files in the UE4Project so that these files can be used durcing compilation of the plugin. The first time this takes quite a while. Go walk around a bit, maybe start playing factoryidle . Working with the Unreal Engine project Launch Unreal Engine and open the project file Formula-Student-Driverless-Simulator/UE4Project/FSOnline.uproject It might show an error like 'This project was made with a different version of the Unreal Engine'. In that case select more options and skip conversion . When asked to rebuild the 'Blocks' and 'AirSim' modules, choose 'Yes'. This is the step where the plugin part of AirSim is compiled. After some time Unreal Engine will start and you can launch the game. Run the game in standalone mode or or selected viewport mode, simulate and eject mode do not support camera's. If you make changes to AirLib you have to run build.cmd again. If you make changes to the plugin code or AirLib, you only have to recompile the plugin. This can be done from within the Unreal Editor. go to to Window -> Developer tools -> Modules . Search for AirSim and click Recompile . Launching the game To run the game, click the big Play button. If you want to run it like it would run when packaged, choose 'Run as standalone game'. Running the Ros Bridge The simulator exposes an RPC api that is used by the ros bridge to communicate with the vehicle. The ros bridge should preferable run on the same computer as the simulator to ensure low latency and high bandwidth. It is possible to run the ros bridge on a different computer than the simulator. To get this to work you should use the host argument in the fsds_ros_bridge.launch file. The ros bridge will connect to the simulator on port 41451. The ros bridge only works on Ubuntu. If you have the simulator running on windows we reccommend Windows Subsystem for Linux. This offers a virtual Ubuntu machine within Windows. You can read here how to install it . While you are at it you might also want to install Xming so you can run rviz and rqt_plot from within WSL. Requirements The ros bridge requires ROS Melodic to be installed , as well as the following dependencies: sudo apt-get install ros-melodic-tf2-geometry-msgs python-catkin-tools ros-melodic-rqt-multiplot ros-melodic-joy ros-melodic-cv-bridge ros-melodic-image-transport libyaml-cpp-dev libcurl4-openssl-dev Make sure you have git lfs installed! Cloning the repository Ready? Lets clone the repo into your home directory: git clone git@github.com:FS-Online/Formula-Student-Driverless-Simulator.git --recurse-submodules If you havn't setup your ssh keys, you can clone using https by running the following command: git clone https://github.com/FS-Online/Formula-Student-Driverless-Simulator.git --recurse-submodules THE REPO HAS TO BE CLONED IN THE HOME DIRECTORY! . So the repo location should be $HOME/Formula-Student-Driverless-Simulator . Why you ask? Because we couldn't get relative paths in the C++ code to work so now we have hard-coded some paths to the home directory. I know yes it is ugly but it works. If you are bothered by it I would welcome you to open a pr with a fix. If this folder already exists as a result of any previous step, move the existing folder out of the way and merge the content afterwards. If you are on Windows and cloned this repository in a Windows directory, go into the cloned repo and run git config core.fileMode false to ignore file mode changes. If you want to share the the clone directory with the Ubuntu WSL system, create a symlink within WSL like so: ln -s /mnt/c/Users/developer/Formula-Student-Driverless-Simulator ~/Formula-Student-Driverless-Simulator Now, checkout the version equal to the simulator. If you are running for example simulator packaged version beta-3, run git checkout beta-3 to get the ros brige to the same version Preparing AirLib AirLib is the shared code between the ros wrapper and the AirSim Unreal Engine plugin. We need to stage the source before we can compile it together with the wrapper. If you are working in a WSL shared folder where previously build.cmd was ran, you can skip this step. Open an Ubuntu terminal and run AirSim/setup.sh . This will download the nessesary libraries required to compile AirLib. You will only need to run this once. Everything setup.sh does is also included in build.cmd. Building the workspace cd ros catkin init catkin build Launching the ros bridge The ros bridge consists of a few nodes to achieve the highest performance and keep the codebase clean. Everything can be launched using the fsds_ros_bridge.launch launchfile. cd ros source devel/setup.bash roslaunch fsds_ros_bridge fsds_ros_bridge.launch The ros bridge will read the settings from ~/Formula-Student-Driverless-Simulator/settings.json . Make sure this is the same configuration file as the simulator uses. Read all about configuring the ros bridge here.","title":"Getting started"},{"location":"getting-started/#getting-started","text":"When running this simulator there are two main components: the simulator and the ros bridge. Your autonomous system and the ros bridge should use the same ros core, either by running on the same computer or through a network connection. It is required that the simulator and the ros bridge have the same version! This page is an overview of the different methods to get these components up and running.","title":"Getting started"},{"location":"getting-started/#running-the-simulator","text":"To run the simulation smoothly you need quite a fast windows computer with a modern videocard. We highly recommend the following computer specs. You might be able to run with less power but everything will be slower. For developing this project, you need quite a good computer because Unreal Engine is a heavy baby. 8 core 2.3Ghz CPU 12 GB memory 30GB free SSD storage (120GB when building the unreal project from source) Recent NVidia card with Vulkan support and 3 GB of memory. (You can check the video card drivers by running vulkaninfo ) If your computer does not suffice you can use a remote workstation on Google Cloud Platform. Read this tutorial on how to setup your virtual workstation. The simulator will load settings from the file Formula-Student-Driverless-Simulator/settings.json in your home directory . This file is required for the simulator to work and contains the sensor configuration of the car. If you clone the repo you will already have this file in place. If not, copy-paste the contents of the settings.json file at the root of this repository into the ~/Formula-Student-Driverless-Simulator . This should get you started with the default sensor configuration, feel free to try your own custom sensor suite.","title":"Running the simulator"},{"location":"getting-started/#from-release-binaries","text":"The simulator is distributed as binaries on every release. At this moment only windows binaries are released. For now, if you are on Ubuntu you will have to run the simulator from the Unreal Engine Editor. During competition, the simulation will run on Windows because it offers a bit better performance and stability. Go to releases and download the latest one. Unzip it to anywhere on your computer and launch FSDS.exe. A window with a car should popup! Try driving the car around using the arrowkeys.","title":"From release binaries"},{"location":"getting-started/#from-source-using-the-unreal-engine-editor","text":"If you want to run the unreal engine project from source you will need unreal engine and visual studio 2019 . On Ubuntu you can skip the visual studio 2019 part.","title":"From source using the Unreal Engine Editor"},{"location":"getting-started/#get-the-repository","text":"You can either download the repo using the big green download button on the github page of this project or clone the repository. For cloning, checkout the documentation on this further down this page. When downloading or cloning, by default you get the latest, unreleased version. This is probably not the version that you want. Make sure you select the version that you need!","title":"Get the repository"},{"location":"getting-started/#compiling-the-airsim-plugin","text":"The Unreal Engine project requires the AirSim plugin. We have to compile this plugin first. The AirSim plugin is made up of AirLib (/AirSim/AirLib) and the plugin code (/UE4Project/Plugins/AirSim/Source). AirLib is the shared code between the ros wrapper and the AirSim Unreal Engine plugin. First build the AirLib code. On Windows, open the Developer Command Prompt for VS 2019 , go to Formula-Student-Driverless-Simulator/AirSim and run build.cmd On Ubuntu, go to folder AirSim and run setup.sh and build.sh . So what does build.cmd or setup.sh+build.sh do? It downloads any nessesary libraries and compiles AirLib. After compilation it places the files in the UE4Project so that these files can be used durcing compilation of the plugin. The first time this takes quite a while. Go walk around a bit, maybe start playing factoryidle .","title":"Compiling the AirSim plugin"},{"location":"getting-started/#working-with-the-unreal-engine-project","text":"Launch Unreal Engine and open the project file Formula-Student-Driverless-Simulator/UE4Project/FSOnline.uproject It might show an error like 'This project was made with a different version of the Unreal Engine'. In that case select more options and skip conversion . When asked to rebuild the 'Blocks' and 'AirSim' modules, choose 'Yes'. This is the step where the plugin part of AirSim is compiled. After some time Unreal Engine will start and you can launch the game. Run the game in standalone mode or or selected viewport mode, simulate and eject mode do not support camera's. If you make changes to AirLib you have to run build.cmd again. If you make changes to the plugin code or AirLib, you only have to recompile the plugin. This can be done from within the Unreal Editor. go to to Window -> Developer tools -> Modules . Search for AirSim and click Recompile .","title":"Working with the Unreal Engine project"},{"location":"getting-started/#launching-the-game","text":"To run the game, click the big Play button. If you want to run it like it would run when packaged, choose 'Run as standalone game'.","title":"Launching the game"},{"location":"getting-started/#running-the-ros-bridge","text":"The simulator exposes an RPC api that is used by the ros bridge to communicate with the vehicle. The ros bridge should preferable run on the same computer as the simulator to ensure low latency and high bandwidth. It is possible to run the ros bridge on a different computer than the simulator. To get this to work you should use the host argument in the fsds_ros_bridge.launch file. The ros bridge will connect to the simulator on port 41451. The ros bridge only works on Ubuntu. If you have the simulator running on windows we reccommend Windows Subsystem for Linux. This offers a virtual Ubuntu machine within Windows. You can read here how to install it . While you are at it you might also want to install Xming so you can run rviz and rqt_plot from within WSL.","title":"Running the Ros Bridge"},{"location":"getting-started/#requirements","text":"The ros bridge requires ROS Melodic to be installed , as well as the following dependencies: sudo apt-get install ros-melodic-tf2-geometry-msgs python-catkin-tools ros-melodic-rqt-multiplot ros-melodic-joy ros-melodic-cv-bridge ros-melodic-image-transport libyaml-cpp-dev libcurl4-openssl-dev Make sure you have git lfs installed!","title":"Requirements"},{"location":"getting-started/#cloning-the-repository","text":"Ready? Lets clone the repo into your home directory: git clone git@github.com:FS-Online/Formula-Student-Driverless-Simulator.git --recurse-submodules If you havn't setup your ssh keys, you can clone using https by running the following command: git clone https://github.com/FS-Online/Formula-Student-Driverless-Simulator.git --recurse-submodules THE REPO HAS TO BE CLONED IN THE HOME DIRECTORY! . So the repo location should be $HOME/Formula-Student-Driverless-Simulator . Why you ask? Because we couldn't get relative paths in the C++ code to work so now we have hard-coded some paths to the home directory. I know yes it is ugly but it works. If you are bothered by it I would welcome you to open a pr with a fix. If this folder already exists as a result of any previous step, move the existing folder out of the way and merge the content afterwards. If you are on Windows and cloned this repository in a Windows directory, go into the cloned repo and run git config core.fileMode false to ignore file mode changes. If you want to share the the clone directory with the Ubuntu WSL system, create a symlink within WSL like so: ln -s /mnt/c/Users/developer/Formula-Student-Driverless-Simulator ~/Formula-Student-Driverless-Simulator Now, checkout the version equal to the simulator. If you are running for example simulator packaged version beta-3, run git checkout beta-3 to get the ros brige to the same version","title":"Cloning the repository"},{"location":"getting-started/#preparing-airlib","text":"AirLib is the shared code between the ros wrapper and the AirSim Unreal Engine plugin. We need to stage the source before we can compile it together with the wrapper. If you are working in a WSL shared folder where previously build.cmd was ran, you can skip this step. Open an Ubuntu terminal and run AirSim/setup.sh . This will download the nessesary libraries required to compile AirLib. You will only need to run this once. Everything setup.sh does is also included in build.cmd.","title":"Preparing AirLib"},{"location":"getting-started/#building-the-workspace","text":"cd ros catkin init catkin build","title":"Building the workspace"},{"location":"getting-started/#launching-the-ros-bridge","text":"The ros bridge consists of a few nodes to achieve the highest performance and keep the codebase clean. Everything can be launched using the fsds_ros_bridge.launch launchfile. cd ros source devel/setup.bash roslaunch fsds_ros_bridge fsds_ros_bridge.launch The ros bridge will read the settings from ~/Formula-Student-Driverless-Simulator/settings.json . Make sure this is the same configuration file as the simulator uses. Read all about configuring the ros bridge here.","title":"Launching the ros bridge"},{"location":"gps/","text":"GPS The GPS model in AirSim has been configured to mimic an average GPS receiver used during formula student competitions. The GPS operates at 10 Hz, this value can be altered with update_frequency parameter. At this moment there is no artificial latency added. Still there will be some delay between the creation of the gps points and arrival at the autonomous system because of network latency. In the future, this can be altered with update_frequency parameter. All GPS positions are timestamped. As soon as the car starts, gps becomes available at maximum resolution. There is no warmup time since during physical formula student events cars often start with pre-locked gps. The inaccuracies in the GPS sensor position is generated with adding an offset vector from the ground truth, as a vector [x_err, y_err, z_err]T , where x_err, y_err are generated through a Gaussian distribution with 0 mean and eph variance, and z_err through a Gaussian distribution with 0 mean and epv variance. Currently the eph is set to 4 cm . This noise, however, is only added if the measured velocity is above an arbitrarily chosen threshold (currently 0.1m/s). To velocities below that, this additional inaccuracy is not introduced to avoid \u201cjumpy\u201d positions during standstill of the vehicle. See GpsSimple.hpp (/AirSim/AirLib/include/sensors/gps/GpsSimple.hpp) and GpsSimpleParams.hpp for the implementation of the gps model.","title":"GPS"},{"location":"gps/#gps","text":"The GPS model in AirSim has been configured to mimic an average GPS receiver used during formula student competitions. The GPS operates at 10 Hz, this value can be altered with update_frequency parameter. At this moment there is no artificial latency added. Still there will be some delay between the creation of the gps points and arrival at the autonomous system because of network latency. In the future, this can be altered with update_frequency parameter. All GPS positions are timestamped. As soon as the car starts, gps becomes available at maximum resolution. There is no warmup time since during physical formula student events cars often start with pre-locked gps. The inaccuracies in the GPS sensor position is generated with adding an offset vector from the ground truth, as a vector [x_err, y_err, z_err]T , where x_err, y_err are generated through a Gaussian distribution with 0 mean and eph variance, and z_err through a Gaussian distribution with 0 mean and epv variance. Currently the eph is set to 4 cm . This noise, however, is only added if the measured velocity is above an arbitrarily chosen threshold (currently 0.1m/s). To velocities below that, this additional inaccuracy is not introduced to avoid \u201cjumpy\u201d positions during standstill of the vehicle. See GpsSimple.hpp (/AirSim/AirLib/include/sensors/gps/GpsSimple.hpp) and GpsSimpleParams.hpp for the implementation of the gps model.","title":"GPS"},{"location":"ground-speed-sensor/","text":"Ground Speed Sensor (GSS) The ground speed sensor is modeled around the Kistler ground speed (like the Kistler Correvit SFII). Velocity information is captured in the global world frame in ENU frame. At this moment no extra noise is added to the sensordata since the kistler 250hz data averaged into the 100hz is so close to ground truth that adding noise would be unrealistic. Ros When using the ros bridge, ground speed sensordata will be published on /fsds/gss with the geometry_msgs/TwistStamped message type. Appart from the header fields, only x , y and z of the twist.linear are populated. header: seq: 5747 stamp: secs: 1595325426 nsecs: 617730500 frame_id: \"fsds/FSCar\" twist: linear: x: 4.80838251114 y: -0.0 z: -0.0214105024934 angular: x: 0.0 y: 0.0 z: 0.0","title":"Ground Speed Sensor (GSS)"},{"location":"ground-speed-sensor/#ground-speed-sensor-gss","text":"The ground speed sensor is modeled around the Kistler ground speed (like the Kistler Correvit SFII). Velocity information is captured in the global world frame in ENU frame. At this moment no extra noise is added to the sensordata since the kistler 250hz data averaged into the 100hz is so close to ground truth that adding noise would be unrealistic.","title":"Ground Speed Sensor (GSS)"},{"location":"ground-speed-sensor/#ros","text":"When using the ros bridge, ground speed sensordata will be published on /fsds/gss with the geometry_msgs/TwistStamped message type. Appart from the header fields, only x , y and z of the twist.linear are populated. header: seq: 5747 stamp: secs: 1595325426 nsecs: 617730500 frame_id: \"fsds/FSCar\" twist: linear: x: 4.80838251114 y: -0.0 z: -0.0214105024934 angular: x: 0.0 y: 0.0 z: 0.0","title":"Ros"},{"location":"how-to-release/","text":"How to release a new version Export the Unreal Engine project for release Open the UE4Project in the Unreal Editor Ensure 'File' -> 'Package Project' -> 'Build configuration' is set to 'Shipping, Choose 'File' -> 'Package Project' -> 'Windows (64 bit)' Select any folder on your computer. Wait until it finishes. Go into the WindowsNoEditor folder and rename Blocks.exe to FSDS.exe Zip all files and upload to github release! Deploying documentation For documentation we use mkdocs and mike . Hosting is provided by github pages. To tag a new version of the documentation and release it to github, first checkout the version that you want to deploy. Then run mike deploy VERSION latest -u -p . This will compile the documentation, store it as a new version in the gh-pages branch, update the latest alias to point at the new version and push the gh-pages branch to github and thus making the documentation public. To create a new version without updating the latest tag, omit the latest -u part.","title":"How to release"},{"location":"how-to-release/#how-to-release-a-new-version","text":"","title":"How to release a new version"},{"location":"how-to-release/#export-the-unreal-engine-project-for-release","text":"Open the UE4Project in the Unreal Editor Ensure 'File' -> 'Package Project' -> 'Build configuration' is set to 'Shipping, Choose 'File' -> 'Package Project' -> 'Windows (64 bit)' Select any folder on your computer. Wait until it finishes. Go into the WindowsNoEditor folder and rename Blocks.exe to FSDS.exe Zip all files and upload to github release!","title":"Export the Unreal Engine project for release"},{"location":"how-to-release/#deploying-documentation","text":"For documentation we use mkdocs and mike . Hosting is provided by github pages. To tag a new version of the documentation and release it to github, first checkout the version that you want to deploy. Then run mike deploy VERSION latest -u -p . This will compile the documentation, store it as a new version in the gh-pages branch, update the latest alias to point at the new version and push the gh-pages branch to github and thus making the documentation public. To create a new version without updating the latest tag, omit the latest -u part.","title":"Deploying documentation"},{"location":"import-car-3d-model/","text":"How to import your car 3d model Import your own 3d model of your formula student car into the world and use it during simulation! Preface (a friendly warning) Without experience with blender and unreal engine this whole process is painfull. There are waaaay to many options and buttons in these tools and it never does what you want to do. If you want your 3d model to be added to this repository you can create a github ticket and ask verry nicely if someone wants to help. Maybe you find someone with some experience that will help you. Make sure you have your fbx model prepared beforehand (see below). Are you a great warrior forged in the fires of a collapsing star seeking for the ultimate challenge? Let me introduce you to the world of importing 3d model. 1. Prepare your 3d model First we have to get the 3d model prepared and exported to fbx format. Most 3d editing software support this format. This tutorial however only describes blender becuase it's free (yay) and relatively easy to use. Build your car and get it to the following structure: There should be 1 object for the chassy and 4 child objects for the wheels. You do not need to define bones, those will be auto-created when importing to unreal. Give the wheels easy names because we will be using those names later on. Note: the car should be facing the positive x axis in blender. This is also the time to paint your car. Add surfuces, create textures and go crazy with paint. Export any textures to image format. Before we export the model, check that all deltra transforms have been applied and all scales are set to 1 . Also don't forget to recalculate normals Export the 3d model to FBX format using all default settings. Ensure you are only exporting meshes and armature. 2. Importing the model into unreal engine Create a new folder in AirSim Content/VehicleADV/Cars/ with the name of your car. Use all defaults, except: Check Skeletal Mesh Import content type: Geometry Only Skeleton 'None' Now you should see a mesh file, physics asset and skeleton. 3. Styling your model Go into the mesh file to style your car. There are many ways to make your mesh fancy using materials and textures. Good luck Googling. 4. Configuring the physics asset The physics asset defines which part of the vehicle interacts with the world. It is basically the collision model. If you are not planning to use this car in an online competition then you can configure the bounding boxes as close to the mesh as you want. Some key settings to keep in mind: Each wheel bone needs it's own physics body. All wheel bodies need to be Physics Type 'Kinematic' Only the wheels should touch the ground. Take a look at Suv_Pa for an example: If you are planning to use this car in an online competition (FSOnline) you must use create the bounding boxes exactly the following specification: Width: 100cm Length: 180cm Height: 50cm In competition all cars will be equal in behaviour. So all bounding boxes must be equal. Below picture shows how bounding boxes can be bigger or smaller than the mesh. All bounding boxes will be checked by: placing each vehicle into the virtual world using a SkeletalMeshActor Going into top view mode (Alt-J) Enabling Collision visualisation (Alt-C) Using the middle-mouse button measuring tool to check the width and length The following physics asset are examples of above specification: AirSim/Content/VehicleAdv/Cars/TechnionCar/RacingTechnion_PA.uasset AirSim/Content/VehicleAdv/Cars/ReferenceCar/ReferenceCar_Pa.uasset AirSim/Content/VehicleAdv/Cars/SuvCar/RacingSuv_Pa.uasset 5. Create an animation Go into your car folder and create a new Animation Blueprint. Choose VehicleAnimInstance as parent class and choose your cars skeleton as target. Now make it look like this: 6. Create a pawn Go into your car folder and create create a new Blueprint Class with base class CarPawn. This will be the actual pawn that will be controlled by the autonomous system. In the components browser, select 'Mesh'. Next, in the details pane, set 'Skeletal Mesh' to your car. Set 'Anim Class' to the animation class you created in step 5. In the components browser, select 'VehicleMovement' Next, in the details pane: Set Mass to 255,0 In the Wheel Setups set the first two Wheel Class to FormulaFrontWheel and the last two Wheel Class to FormulaBackWheel In Wheel Setups set the Bone Name of each wheel to the corresponding bone name as found in the skeleton. From top to bottom they are front left, front right, back left, back right From Drag Coefficient onwards all settings should be exactly equal to the settings in SuvCarPawn . Have fun copy-pasting. 7. Selecting a car Update your settings.json PawnPaths -> DefaultCar -> PawnBP to point at the newly created pawn. It should (tm) work.","title":"Import your 3d car model"},{"location":"import-car-3d-model/#how-to-import-your-car-3d-model","text":"Import your own 3d model of your formula student car into the world and use it during simulation!","title":"How to import your car 3d model"},{"location":"import-car-3d-model/#preface-a-friendly-warning","text":"Without experience with blender and unreal engine this whole process is painfull. There are waaaay to many options and buttons in these tools and it never does what you want to do. If you want your 3d model to be added to this repository you can create a github ticket and ask verry nicely if someone wants to help. Maybe you find someone with some experience that will help you. Make sure you have your fbx model prepared beforehand (see below). Are you a great warrior forged in the fires of a collapsing star seeking for the ultimate challenge? Let me introduce you to the world of importing 3d model.","title":"Preface (a friendly warning)"},{"location":"import-car-3d-model/#1-prepare-your-3d-model","text":"First we have to get the 3d model prepared and exported to fbx format. Most 3d editing software support this format. This tutorial however only describes blender becuase it's free (yay) and relatively easy to use. Build your car and get it to the following structure: There should be 1 object for the chassy and 4 child objects for the wheels. You do not need to define bones, those will be auto-created when importing to unreal. Give the wheels easy names because we will be using those names later on. Note: the car should be facing the positive x axis in blender. This is also the time to paint your car. Add surfuces, create textures and go crazy with paint. Export any textures to image format. Before we export the model, check that all deltra transforms have been applied and all scales are set to 1 . Also don't forget to recalculate normals Export the 3d model to FBX format using all default settings. Ensure you are only exporting meshes and armature.","title":"1. Prepare your 3d model"},{"location":"import-car-3d-model/#2-importing-the-model-into-unreal-engine","text":"Create a new folder in AirSim Content/VehicleADV/Cars/ with the name of your car. Use all defaults, except: Check Skeletal Mesh Import content type: Geometry Only Skeleton 'None' Now you should see a mesh file, physics asset and skeleton.","title":"2. Importing the model into unreal engine"},{"location":"import-car-3d-model/#3-styling-your-model","text":"Go into the mesh file to style your car. There are many ways to make your mesh fancy using materials and textures. Good luck Googling.","title":"3. Styling your model"},{"location":"import-car-3d-model/#4-configuring-the-physics-asset","text":"The physics asset defines which part of the vehicle interacts with the world. It is basically the collision model. If you are not planning to use this car in an online competition then you can configure the bounding boxes as close to the mesh as you want. Some key settings to keep in mind: Each wheel bone needs it's own physics body. All wheel bodies need to be Physics Type 'Kinematic' Only the wheels should touch the ground. Take a look at Suv_Pa for an example: If you are planning to use this car in an online competition (FSOnline) you must use create the bounding boxes exactly the following specification: Width: 100cm Length: 180cm Height: 50cm In competition all cars will be equal in behaviour. So all bounding boxes must be equal. Below picture shows how bounding boxes can be bigger or smaller than the mesh. All bounding boxes will be checked by: placing each vehicle into the virtual world using a SkeletalMeshActor Going into top view mode (Alt-J) Enabling Collision visualisation (Alt-C) Using the middle-mouse button measuring tool to check the width and length The following physics asset are examples of above specification: AirSim/Content/VehicleAdv/Cars/TechnionCar/RacingTechnion_PA.uasset AirSim/Content/VehicleAdv/Cars/ReferenceCar/ReferenceCar_Pa.uasset AirSim/Content/VehicleAdv/Cars/SuvCar/RacingSuv_Pa.uasset","title":"4. Configuring the physics asset"},{"location":"import-car-3d-model/#5-create-an-animation","text":"Go into your car folder and create a new Animation Blueprint. Choose VehicleAnimInstance as parent class and choose your cars skeleton as target. Now make it look like this:","title":"5. Create an animation"},{"location":"import-car-3d-model/#6-create-a-pawn","text":"Go into your car folder and create create a new Blueprint Class with base class CarPawn. This will be the actual pawn that will be controlled by the autonomous system. In the components browser, select 'Mesh'. Next, in the details pane, set 'Skeletal Mesh' to your car. Set 'Anim Class' to the animation class you created in step 5. In the components browser, select 'VehicleMovement' Next, in the details pane: Set Mass to 255,0 In the Wheel Setups set the first two Wheel Class to FormulaFrontWheel and the last two Wheel Class to FormulaBackWheel In Wheel Setups set the Bone Name of each wheel to the corresponding bone name as found in the skeleton. From top to bottom they are front left, front right, back left, back right From Drag Coefficient onwards all settings should be exactly equal to the settings in SuvCarPawn . Have fun copy-pasting.","title":"6. Create a pawn"},{"location":"import-car-3d-model/#7-selecting-a-car","text":"Update your settings.json PawnPaths -> DefaultCar -> PawnBP to point at the newly created pawn. It should (tm) work.","title":"7. Selecting a car"},{"location":"imu/","text":"IMU Sensor The IMU sensor in FSDS is using AirSim's built in IMU sensor simulation, that has been modelled and parametrized according to MPU 6000 IMU from InvenSense The maximum achievable internal IMU frequency is 1000 Hz, ouputing information of the vehicle's 3-axis angular velocity, 3-axis linear acceleration, as well as its orientation in quaternions. IMU gyroscope and accelomter bias and accuracy parameters can be found and fine-tuned in AirLib/include/sensors/imu/ImuSimpleParams.hpp, or can be initialized with custom parameters in your settings.json file. The angular velocity, linear acceleration outputs as well as their biases have artificially introduced Gaussian noise (0 mean, standard deviation of 1) updated on each IMU cycle. All of the IMU measurements are timestamped. See ImuSimple.hpp (/AirSim/AirLib/include/sensors/imu/ImuSimple.hpp) and [ImuSimpleParams.hpp] /AirSim/AirLib/include/sensors/imu/ImuSimpleParams.hpp for the implementation of the IMU model.","title":"IMU"},{"location":"imu/#imu-sensor","text":"The IMU sensor in FSDS is using AirSim's built in IMU sensor simulation, that has been modelled and parametrized according to MPU 6000 IMU from InvenSense The maximum achievable internal IMU frequency is 1000 Hz, ouputing information of the vehicle's 3-axis angular velocity, 3-axis linear acceleration, as well as its orientation in quaternions. IMU gyroscope and accelomter bias and accuracy parameters can be found and fine-tuned in AirLib/include/sensors/imu/ImuSimpleParams.hpp, or can be initialized with custom parameters in your settings.json file. The angular velocity, linear acceleration outputs as well as their biases have artificially introduced Gaussian noise (0 mean, standard deviation of 1) updated on each IMU cycle. All of the IMU measurements are timestamped. See ImuSimple.hpp (/AirSim/AirLib/include/sensors/imu/ImuSimple.hpp) and [ImuSimpleParams.hpp] /AirSim/AirLib/include/sensors/imu/ImuSimpleParams.hpp for the implementation of the IMU model.","title":"IMU Sensor"},{"location":"integration-handbook/","text":"Autonomous System Integration Handbook This page describes how to integrate your autonomous system (AS) to the Formula Student Driverless Simulator (FSDS). The rules and procedures set out in this document will be used during the FSOnline competition . High-level overview Your AS is expected to continuously run a ROS master. The simulator will launch a node ( fsds_ros_bridge ) that connects to your ROS system. This node will publish sensor data and listen for vehicle setpoints. When the simulation is done, the node is closed and your AS will no longer be able to interact with the simulator. A more in-depth explanation of the FSDS system can be found here. Integrating your autonomous system with a simulator is a matter of subscribing and publishing to topics and acting accordingly. During testing, your AS and the ROS bridge/simulator will probably run on the same machine (your local computer most likely). However, during the competition, the AS and the ROS bridge/simulator will be on different machines (cloud servers) and communicate over a local network. System flow and Signals Initially, when your AS launches, no simulation is connected. The AS must wait for a GO signal before acting. Staging At some point, your AS will be staged: The vehicle is placed at a staging line prior to the starting line, the fsds_ros_bridge node will connect to the ROS system. From this point onwards, the AS will receive sensor data and can control the vehicle by publishing vehicle setpoints. However, it shouldn't start driving the vehicle just yet! Starting Just like with a physical FS event, the vehicle must only start driving after a GO signal is received. The GO signal indicates that the AS must start the mission and that the vehicle should start driving. This signal is also known as the 'green flag' signal or 'starting' signal. Within this repo, we will reference it as 'GO'. Within the GO signal, the mission and track are added. Currently, autocross and trackdrive are the supported missions. In autocross, the vehicle has to complete a single lap on an unknown track. On the trackdrive, the vehicle has to finish 10 laps on a track it has previously seen. During the competition, on each track, every AS will do an autocross mission before a trackdrive mission. It can occur that multiple autocross runs on different tracks take place before going to trackdrive. It can also happen that multiple autocross runs take place on the same track. For example, the AS might be requested to do: autocross on track A autocross on track B trackdrive on track A autocross on track C autocross on track C (re-run) trackdrive on track C The AS must implement the following behaviour: When the AS is requested to do autocross on a track that it has seen before, it must delete any and all data it gathered during all previous runs on this track. When the AS is requested to do trackdrive on a track where it has done a trackdrive previously, it must delete any and all data it gathered during all previous trackdrive runs on this track. However, the data gathered during the last autocross on this track shouldn't be deleted. An exception to this rule is data recorded with the exclusive intent to analyze the AS's behaviour after the event. This includes all files that the AS only writes to but does not read from. To make the AS aware of which track it is driving, the GO signal includes a unique identifier of the upcoming track. After the initial GO signal, the signal is continuously re-sent at 1 Hz to ensure it arrives at the team's AS. The timestamp of all consecutive GO signals is equal to the first one. Finishing There are two ways to conclude a run: finishing or stopping. When the autonomous system feels it concluded it's run, it should send a FINISHED signal. The FINISHED signal tells the simulator that the AS no longer wants to control the vehicle. As such, the simulator will stop the fsds_ros_bridge and the AS will no longer receive sensor data or be able to control the vehicle. When the official decides that the run is over it will stop the simulation. See the rulebook for a description of when the official does so. When the simulation is stopped the fsds_ros_bridge is stopped immediately and the AS will no longer receive sensor data or be able to control the vehicle. The AS will not receive a signal that this happened. To detect a stop, the AS should keep an eye on the GO signal. The general rule is: If the AS did not receive a GO signal for 4 seconds the AS can assume the fsds_ros_bridge is stopped. When this state is detected, the AS can reset itself and prepare for the next mission. Sensor Suite Every team can configure the sensors on their vehicle. This configuration is called the sensor suite. To ensure the simulation will perform as expected, the sensor suite has some restrictions. Here you can read the requirements and restrictions that apply to every sensor. Camera Be warned, camera framerate cannot be guaranteed, see #43. Only rgb camera's are allowed during competition ( see camera docs ). Depth cameras are not allowed during FSOnline 2020. Every vehicle can have a maximum of 2 camera sensors. These camera(s) can be placed anywhere on the vehicle that would be allowed by FSG 2020 rules. The camera body dimensions are a 4x4x4 cm cube with mounting points at any side except the front-facing side. All camera sensors output images at around 20 FPS. You can choose the resolution of the camera(s). In total, the camera\u2019s can have 1232450 pixels. Every dimension (width or height) must be at least 240px and no greater than 1600px. The horizontal field of view (FoV) is configurable for each camera and must be at least 30 degrees and not be greater than 90 degrees. The camera's auto exposure, motion blur and gamma settings will be equal for all teams. Lidar A vehicle can have between 0 and 5 lidars. The lidar(s) can be placed anywhere on the vehicle that would be allowed by FSG 2020 rules. The body dimension of every lidar is a vertical cylinder, 8 cm height and 8 cm diameter with mounting points at the top and bottom. A single lidar can have between 1 and 128 lasers. The vertical inter-laser angle must be at least 0,18 degrees be no larger than 180 degrees. The rotation frequency must be between 5 and 20 Hz. For each lidar, during a single rotation, the number of collected points for 1 laser must be not higher than 2048 for 360 degrees. The number of collected points per lidar per laser per second cannot exceed 20480. To ensure the simulation keeps running smoothly: Every lidar is limited to collect 10000 points per scan. The total number of points collected per second can be no larger than 100000 The range of each laser is 100 meter. Details on how to configure the lidar sensor can be found here . GPS Every vehicle has 1 GPS, it is located at the centre of gravity of the vehicle. This sensor cannot be removed or moved. The GPS captures the position of the vehicle in the geodetic reference system, namely longitude [deg], latitude [deg], altitude [m]. More detailed technical information about the accuracy of the GPS can be found here . IMU Every vehicle has 1 IMU, it is located at the centre of gravity of the vehicle. This sensor cannot be removed or moved. The IMU captures the acceleration, orientation and angular rate of the vehicle. More detailed technical information about the IMU implementation of the IMU can be found here . Sensor specification Teams are expected to provide their sensor suite as a single AirSim settings.json file. Most of the parameters in the settings.json file will be set by the officials to ensure fairness during competition. You are allowed to configure the following subset of parameters within the boundaries of the above rules. Cameras camera name Width, Height (pixels) FOV_Degrees (degrees) X, Y, Z (meters) Pitch, Roll, Yaw (degrees) Lidars (for more details, see the lidar documentation ) NumberOfLasers PointsPerScan RotationsPerSecond HorizontalFOVStart (degrees) HorizontalFOVEnd (degrees) VerticalFOVUpper (degrees) VerticalFOVLower(degrees) X, Y, Z (meters) Pitch, Roll, Yaw (degrees) The GPS and IMU are configured equally for all teams according to the rules in the previous chapter. X, Y, Z, Pitch, Roll, Yaw have to be specified in a NED frame (it might take some experimentation to understand the correct signs to use). All values are relative to the vehicle pawn center. You can read more about this point in the vehicle model description . The transforms you will get from ROS however, will be in the ENU frame (which is the default ros coordinate system). Distance values are in meters and rotation values are degrees. We recommend to copy the settings.json in this repository as a base and configure the cameras and lidar from thereon. ROS integration Communication between the autonomous system and simulator will take place using ROS topics. Sensor data will be published by the ros bridge and received by the autonomous system. The autonomous system will publish vehicle setpoints and the ROS bridge will listen for those messages. Static transforms between sensors also are being published for usage by the autonomous system. ROS msgs The ROS bridge of this simulator had to make use of several custom msgs (for control commands, the groundtruth track, etc). These messages are defined in a ROS package called fs_msgs which is located in a separate, light repository . To implement publishers and subscibers for these messages types in your autonomous pipeline, you will have to add the fs_msgs repository as a submodule in your codebase (inside de src directory of an existing catkin workspace as done in this repository) or clone it and build it somewhere else in your system. Topics The AS can subscribe to the following sensor topics: /fsds/gps /fsds/imu /fsds/camera/CAMERA_NAME /fsds/camera/CAMERA_NAME/camera_info /fsds/lidar/LIDAR_NAME During testing , the following ground truth topics will also be available: /fsds/testing_only/odom /fsds/testing_only/track These two topics should allow you to run autonomously without a finished perception and state estimation pipeline. The AS will receive the GO signal on the following topic: /fsds/signal/go And it AS can publish the FINISHED on this topic: /fsds/signal/finished The AS must publish vehicle control commands on this topic: /fsds/control_command Read more about the techincal detalis of these topics in the ros-bridge documentation Vehicle dynamic model The vehicle dynamic model is a third-party high-fodelity model and will be the same for all teams. More details and information on this choice can be found here . At this moment only the Technion Formula racecar has these parameters applied ( AirSim/VehicleAdv/Cars/TechnionCar/TechnionCarPawn ). Every team is allowed to create their own car pawn with different looks, given that it behaves exactly like the TechnionCarPawn. 3D vehicle model Read this tutorial on how to import your own 3d car model. To select a different car for a run, change the PawnPaths -> DefaultCar -> PawnBP field in the settings.json . Currently the following values are supported: * Class'/AirSim/VehicleAdv/Cars/TechnionCar/TechnionCarPawn.TechnionCarPawn_C' * Class'/AirSim/VehicleAdv/Cars/SuvCar/SuvCarPawn.SuvCarPawn_C' All vehicles will have a width of 100cm, a length of 180cm and a height of 50cm. This is the bounding box of the vehicle (the part of the vehicle that interacts with the world) The 3d models are not restricted to this size. Competition deployment A few weeks before the competition, each team will receive the ssh credentials to an Ubuntu google cloud instance. This instance will have 8 vCPU cores, 30 GB memory (configuration n1-standard-8), 1 Nvidia Tesla T4 video card and 100GB SSD disk. The teams must install their autonomous system on this computer. During the competition, a separate google cloud instance will run the simulation software and the ROS bridge. One by one the ROS bridge will connect to the different teams' computers and they will do their mission. During the weeks leading up to the competition, FSOnline will host multiple testing moments where the autonomous computers will be connected to the simulator and drive a few test laps. More information about the procedure will be added later.","title":"Integration handbook"},{"location":"integration-handbook/#autonomous-system-integration-handbook","text":"This page describes how to integrate your autonomous system (AS) to the Formula Student Driverless Simulator (FSDS). The rules and procedures set out in this document will be used during the FSOnline competition .","title":"Autonomous System Integration Handbook"},{"location":"integration-handbook/#high-level-overview","text":"Your AS is expected to continuously run a ROS master. The simulator will launch a node ( fsds_ros_bridge ) that connects to your ROS system. This node will publish sensor data and listen for vehicle setpoints. When the simulation is done, the node is closed and your AS will no longer be able to interact with the simulator. A more in-depth explanation of the FSDS system can be found here. Integrating your autonomous system with a simulator is a matter of subscribing and publishing to topics and acting accordingly. During testing, your AS and the ROS bridge/simulator will probably run on the same machine (your local computer most likely). However, during the competition, the AS and the ROS bridge/simulator will be on different machines (cloud servers) and communicate over a local network.","title":"High-level overview"},{"location":"integration-handbook/#system-flow-and-signals","text":"Initially, when your AS launches, no simulation is connected. The AS must wait for a GO signal before acting.","title":"System flow and Signals"},{"location":"integration-handbook/#staging","text":"At some point, your AS will be staged: The vehicle is placed at a staging line prior to the starting line, the fsds_ros_bridge node will connect to the ROS system. From this point onwards, the AS will receive sensor data and can control the vehicle by publishing vehicle setpoints. However, it shouldn't start driving the vehicle just yet!","title":"Staging"},{"location":"integration-handbook/#starting","text":"Just like with a physical FS event, the vehicle must only start driving after a GO signal is received. The GO signal indicates that the AS must start the mission and that the vehicle should start driving. This signal is also known as the 'green flag' signal or 'starting' signal. Within this repo, we will reference it as 'GO'. Within the GO signal, the mission and track are added. Currently, autocross and trackdrive are the supported missions. In autocross, the vehicle has to complete a single lap on an unknown track. On the trackdrive, the vehicle has to finish 10 laps on a track it has previously seen. During the competition, on each track, every AS will do an autocross mission before a trackdrive mission. It can occur that multiple autocross runs on different tracks take place before going to trackdrive. It can also happen that multiple autocross runs take place on the same track. For example, the AS might be requested to do: autocross on track A autocross on track B trackdrive on track A autocross on track C autocross on track C (re-run) trackdrive on track C The AS must implement the following behaviour: When the AS is requested to do autocross on a track that it has seen before, it must delete any and all data it gathered during all previous runs on this track. When the AS is requested to do trackdrive on a track where it has done a trackdrive previously, it must delete any and all data it gathered during all previous trackdrive runs on this track. However, the data gathered during the last autocross on this track shouldn't be deleted. An exception to this rule is data recorded with the exclusive intent to analyze the AS's behaviour after the event. This includes all files that the AS only writes to but does not read from. To make the AS aware of which track it is driving, the GO signal includes a unique identifier of the upcoming track. After the initial GO signal, the signal is continuously re-sent at 1 Hz to ensure it arrives at the team's AS. The timestamp of all consecutive GO signals is equal to the first one.","title":"Starting"},{"location":"integration-handbook/#finishing","text":"There are two ways to conclude a run: finishing or stopping. When the autonomous system feels it concluded it's run, it should send a FINISHED signal. The FINISHED signal tells the simulator that the AS no longer wants to control the vehicle. As such, the simulator will stop the fsds_ros_bridge and the AS will no longer receive sensor data or be able to control the vehicle. When the official decides that the run is over it will stop the simulation. See the rulebook for a description of when the official does so. When the simulation is stopped the fsds_ros_bridge is stopped immediately and the AS will no longer receive sensor data or be able to control the vehicle. The AS will not receive a signal that this happened. To detect a stop, the AS should keep an eye on the GO signal. The general rule is: If the AS did not receive a GO signal for 4 seconds the AS can assume the fsds_ros_bridge is stopped. When this state is detected, the AS can reset itself and prepare for the next mission.","title":"Finishing"},{"location":"integration-handbook/#sensor-suite","text":"Every team can configure the sensors on their vehicle. This configuration is called the sensor suite. To ensure the simulation will perform as expected, the sensor suite has some restrictions. Here you can read the requirements and restrictions that apply to every sensor.","title":"Sensor Suite"},{"location":"integration-handbook/#camera","text":"Be warned, camera framerate cannot be guaranteed, see #43. Only rgb camera's are allowed during competition ( see camera docs ). Depth cameras are not allowed during FSOnline 2020. Every vehicle can have a maximum of 2 camera sensors. These camera(s) can be placed anywhere on the vehicle that would be allowed by FSG 2020 rules. The camera body dimensions are a 4x4x4 cm cube with mounting points at any side except the front-facing side. All camera sensors output images at around 20 FPS. You can choose the resolution of the camera(s). In total, the camera\u2019s can have 1232450 pixels. Every dimension (width or height) must be at least 240px and no greater than 1600px. The horizontal field of view (FoV) is configurable for each camera and must be at least 30 degrees and not be greater than 90 degrees. The camera's auto exposure, motion blur and gamma settings will be equal for all teams.","title":"Camera"},{"location":"integration-handbook/#lidar","text":"A vehicle can have between 0 and 5 lidars. The lidar(s) can be placed anywhere on the vehicle that would be allowed by FSG 2020 rules. The body dimension of every lidar is a vertical cylinder, 8 cm height and 8 cm diameter with mounting points at the top and bottom. A single lidar can have between 1 and 128 lasers. The vertical inter-laser angle must be at least 0,18 degrees be no larger than 180 degrees. The rotation frequency must be between 5 and 20 Hz. For each lidar, during a single rotation, the number of collected points for 1 laser must be not higher than 2048 for 360 degrees. The number of collected points per lidar per laser per second cannot exceed 20480. To ensure the simulation keeps running smoothly: Every lidar is limited to collect 10000 points per scan. The total number of points collected per second can be no larger than 100000 The range of each laser is 100 meter. Details on how to configure the lidar sensor can be found here .","title":"Lidar"},{"location":"integration-handbook/#gps","text":"Every vehicle has 1 GPS, it is located at the centre of gravity of the vehicle. This sensor cannot be removed or moved. The GPS captures the position of the vehicle in the geodetic reference system, namely longitude [deg], latitude [deg], altitude [m]. More detailed technical information about the accuracy of the GPS can be found here .","title":"GPS"},{"location":"integration-handbook/#imu","text":"Every vehicle has 1 IMU, it is located at the centre of gravity of the vehicle. This sensor cannot be removed or moved. The IMU captures the acceleration, orientation and angular rate of the vehicle. More detailed technical information about the IMU implementation of the IMU can be found here .","title":"IMU"},{"location":"integration-handbook/#sensor-specification","text":"Teams are expected to provide their sensor suite as a single AirSim settings.json file. Most of the parameters in the settings.json file will be set by the officials to ensure fairness during competition. You are allowed to configure the following subset of parameters within the boundaries of the above rules. Cameras camera name Width, Height (pixels) FOV_Degrees (degrees) X, Y, Z (meters) Pitch, Roll, Yaw (degrees) Lidars (for more details, see the lidar documentation ) NumberOfLasers PointsPerScan RotationsPerSecond HorizontalFOVStart (degrees) HorizontalFOVEnd (degrees) VerticalFOVUpper (degrees) VerticalFOVLower(degrees) X, Y, Z (meters) Pitch, Roll, Yaw (degrees) The GPS and IMU are configured equally for all teams according to the rules in the previous chapter. X, Y, Z, Pitch, Roll, Yaw have to be specified in a NED frame (it might take some experimentation to understand the correct signs to use). All values are relative to the vehicle pawn center. You can read more about this point in the vehicle model description . The transforms you will get from ROS however, will be in the ENU frame (which is the default ros coordinate system). Distance values are in meters and rotation values are degrees. We recommend to copy the settings.json in this repository as a base and configure the cameras and lidar from thereon.","title":"Sensor specification"},{"location":"integration-handbook/#ros-integration","text":"Communication between the autonomous system and simulator will take place using ROS topics. Sensor data will be published by the ros bridge and received by the autonomous system. The autonomous system will publish vehicle setpoints and the ROS bridge will listen for those messages. Static transforms between sensors also are being published for usage by the autonomous system.","title":"ROS integration"},{"location":"integration-handbook/#ros-msgs","text":"The ROS bridge of this simulator had to make use of several custom msgs (for control commands, the groundtruth track, etc). These messages are defined in a ROS package called fs_msgs which is located in a separate, light repository . To implement publishers and subscibers for these messages types in your autonomous pipeline, you will have to add the fs_msgs repository as a submodule in your codebase (inside de src directory of an existing catkin workspace as done in this repository) or clone it and build it somewhere else in your system.","title":"ROS msgs"},{"location":"integration-handbook/#topics","text":"The AS can subscribe to the following sensor topics: /fsds/gps /fsds/imu /fsds/camera/CAMERA_NAME /fsds/camera/CAMERA_NAME/camera_info /fsds/lidar/LIDAR_NAME During testing , the following ground truth topics will also be available: /fsds/testing_only/odom /fsds/testing_only/track These two topics should allow you to run autonomously without a finished perception and state estimation pipeline. The AS will receive the GO signal on the following topic: /fsds/signal/go And it AS can publish the FINISHED on this topic: /fsds/signal/finished The AS must publish vehicle control commands on this topic: /fsds/control_command Read more about the techincal detalis of these topics in the ros-bridge documentation","title":"Topics"},{"location":"integration-handbook/#vehicle-dynamic-model","text":"The vehicle dynamic model is a third-party high-fodelity model and will be the same for all teams. More details and information on this choice can be found here . At this moment only the Technion Formula racecar has these parameters applied ( AirSim/VehicleAdv/Cars/TechnionCar/TechnionCarPawn ). Every team is allowed to create their own car pawn with different looks, given that it behaves exactly like the TechnionCarPawn.","title":"Vehicle dynamic model"},{"location":"integration-handbook/#3d-vehicle-model","text":"Read this tutorial on how to import your own 3d car model. To select a different car for a run, change the PawnPaths -> DefaultCar -> PawnBP field in the settings.json . Currently the following values are supported: * Class'/AirSim/VehicleAdv/Cars/TechnionCar/TechnionCarPawn.TechnionCarPawn_C' * Class'/AirSim/VehicleAdv/Cars/SuvCar/SuvCarPawn.SuvCarPawn_C' All vehicles will have a width of 100cm, a length of 180cm and a height of 50cm. This is the bounding box of the vehicle (the part of the vehicle that interacts with the world) The 3d models are not restricted to this size.","title":"3D vehicle model"},{"location":"integration-handbook/#competition-deployment","text":"A few weeks before the competition, each team will receive the ssh credentials to an Ubuntu google cloud instance. This instance will have 8 vCPU cores, 30 GB memory (configuration n1-standard-8), 1 Nvidia Tesla T4 video card and 100GB SSD disk. The teams must install their autonomous system on this computer. During the competition, a separate google cloud instance will run the simulation software and the ROS bridge. One by one the ROS bridge will connect to the different teams' computers and they will do their mission. During the weeks leading up to the competition, FSOnline will host multiple testing moments where the autonomous computers will be connected to the simulator and drive a few test laps. More information about the procedure will be added later.","title":"Competition deployment"},{"location":"joystick/","text":"Joystick Controller Node If you are interested in driving the car around to gather training data without having to rely on your autonomous system, you can use an XBox controller to do so. Make sure you have built the ROS workspace . Simply run: cd ros source devel/setup.bash roslaunch joystick joystick.launch This node gets input from a joystick xbox controller (see http://wiki.ros.org/joy) and sends the values to the lowlevel controls (hardware) on topic /fsds_ros_bridge/FSCar/control_command The right trigger (RT) controls the acceleration (gas) The left trigger (LT) controls brake (negative acceleration). The x-axis of the left stick controlls the steering angle. Pressing and holding the B button enables boost mode. When the joy driver initializes it sometimes sends very high values. This would cause the car to move unpredictably. Therefore the controller is 'locked' when it is starts and when the controller is re-plugged in. To unlock both triggers must be fully pressed (gas and brake) after which commands will be sent. This node checks if the controller is plugged in by polling the linux device filename. If the controller is unplugged the file disapears and this node will start sending break. When the controller is locked or the controller is unplugged the node will continuously sending break setpoints at 10hz. During normal operation the maximum setpoints are limited quite a bit to make the car better controllable. If you enable boost mode (press B) you get more power and the car will move faster. To configure the vlaue mappings between xbox controller and car setpoints, go into src/joystick.cpp and change the value of the variables. HELP it doesn't work Make sure you run this on a computer that has the xbox controller attached! You can check if the controller is available by running the below command. $ sudo ls /dev/input/js0 crwxrwxrwx 1 root 993 13, 0 Nov 8 14:43 /dev/input/js0 If you get permisison erros you have to give ROS more permissions. Run sudo chmod 777 /dev/input/js0 If the device is connected but not available at js0 it might be mapped to another device. Find the correct device by running sudo ls -al /dev/input/js* When you find the correct device mapping, update the launchfile accordingly. The message [ERROR] Couldn't open joystick force feedback! is normal. Nothing to worry about. It is a warning that always happens with wired xbox controllers. testing To test this node on your computer just attach an xbox controller and run it as described above. Now chek the /fsds_ros_bridge/FSCar/control_command topic and you should see values corresponding to your controller movements. You can debug the input values from the joy driver by checking the /joy topic. Subscribers: /joy sensor_msgs/Joy Listens to joystick input which is then mapped to the control command msg. The mapping should feel intuitive but in case something is unclear, it is described in detail in /ros/src/fsds_ros_bridge/src/joystick.cpp Publishers: /fsds_ros_bridge/VEHICLE_NAME/control_command fs_msgs/ControlCommand","title":"Joystick"},{"location":"joystick/#joystick-controller-node","text":"If you are interested in driving the car around to gather training data without having to rely on your autonomous system, you can use an XBox controller to do so. Make sure you have built the ROS workspace . Simply run: cd ros source devel/setup.bash roslaunch joystick joystick.launch This node gets input from a joystick xbox controller (see http://wiki.ros.org/joy) and sends the values to the lowlevel controls (hardware) on topic /fsds_ros_bridge/FSCar/control_command The right trigger (RT) controls the acceleration (gas) The left trigger (LT) controls brake (negative acceleration). The x-axis of the left stick controlls the steering angle. Pressing and holding the B button enables boost mode. When the joy driver initializes it sometimes sends very high values. This would cause the car to move unpredictably. Therefore the controller is 'locked' when it is starts and when the controller is re-plugged in. To unlock both triggers must be fully pressed (gas and brake) after which commands will be sent. This node checks if the controller is plugged in by polling the linux device filename. If the controller is unplugged the file disapears and this node will start sending break. When the controller is locked or the controller is unplugged the node will continuously sending break setpoints at 10hz. During normal operation the maximum setpoints are limited quite a bit to make the car better controllable. If you enable boost mode (press B) you get more power and the car will move faster. To configure the vlaue mappings between xbox controller and car setpoints, go into src/joystick.cpp and change the value of the variables.","title":"Joystick Controller Node"},{"location":"joystick/#help-it-doesnt-work","text":"Make sure you run this on a computer that has the xbox controller attached! You can check if the controller is available by running the below command. $ sudo ls /dev/input/js0 crwxrwxrwx 1 root 993 13, 0 Nov 8 14:43 /dev/input/js0 If you get permisison erros you have to give ROS more permissions. Run sudo chmod 777 /dev/input/js0 If the device is connected but not available at js0 it might be mapped to another device. Find the correct device by running sudo ls -al /dev/input/js* When you find the correct device mapping, update the launchfile accordingly. The message [ERROR] Couldn't open joystick force feedback! is normal. Nothing to worry about. It is a warning that always happens with wired xbox controllers.","title":"HELP it doesn't work"},{"location":"joystick/#testing","text":"To test this node on your computer just attach an xbox controller and run it as described above. Now chek the /fsds_ros_bridge/FSCar/control_command topic and you should see values corresponding to your controller movements. You can debug the input values from the joy driver by checking the /joy topic.","title":"testing"},{"location":"joystick/#subscribers","text":"/joy sensor_msgs/Joy Listens to joystick input which is then mapped to the control command msg. The mapping should feel intuitive but in case something is unclear, it is described in detail in /ros/src/fsds_ros_bridge/src/joystick.cpp","title":"Subscribers:"},{"location":"joystick/#publishers","text":"/fsds_ros_bridge/VEHICLE_NAME/control_command fs_msgs/ControlCommand","title":"Publishers:"},{"location":"lidar/","text":"Lidar The lidar sensors are configured in the setting.json. This is an example lidar: \"Lidar1\": { \"SensorType\": 6, \"Enabled\": true, \"X\": 0, \"Y\": 0, \"Z\": -1, \"Roll\": 0, \"Pitch\": 0, \"Yaw\" : 0, \"NumberOfLasers\": 7, \"PointsPerScan\": 2000, \"RotationsPerSecond\": 20, \"VerticalFOVUpper\": 0, \"VerticalFOVLower\": -25, \"HorizontalFOVStart\": 0, \"HorizontalFOVEnd\": 90, \"DrawDebugPoints\": false } Lidar1 is the name of the lidar. This value will be used in the ros topic name and coordinate frame. X , Y and Z are the position of the lidar relative the vehicle pawn center of the car in NED frame. Roll , Pitch and Yaw are rotations in degrees. NumberOfLasers is the - duh - the number of lasers in the lidar. The lasers are stacked vertically and rotate on the horizontal plane. The lasers are distributed equally to cover the specified vertical field of view. The vertical field of view is specified by choosing the upper ( VerticalFOVUpper ) and lower ( VerticalFOVLower ) limit in degrees. The lower limit specifies the vertical angle between the horizontal plane of the lidar and the most bottom laser. The upper limit specifies the vertical angle between the horizontal plane of the lidar and most upper laser. The horizontal field of view of the lidar is specified with an upper ( HorizontalFOVStart ) and lower ( HorizontalFOVEnd ) limit in degree as well. The lower limit specifies the counterclockwise angle on a top view (negative yaw) from the direction the lidar is pointing towards. The upper limit specifies the clockwise angle on a top view (positive yaw) from the direction the lidar is pointing towards. RotationsPerSecond specifies how fast the lasers spins and how often a pointcloud is captured. There might be slight variations in the actual lidar frequency vs the configured rotation frequency. PointsPerScan is the number of firings per scan within the field of view. If all lasers hit, the returned pointcloud will contain this number of points in each pointcloud. DrawDebugPoints enables visualization of the lidar hits inside the unreal engine game. This is known to impact performance quite a bit. it is recommended to to only use this during debugging.","title":"Lidar"},{"location":"lidar/#lidar","text":"The lidar sensors are configured in the setting.json. This is an example lidar: \"Lidar1\": { \"SensorType\": 6, \"Enabled\": true, \"X\": 0, \"Y\": 0, \"Z\": -1, \"Roll\": 0, \"Pitch\": 0, \"Yaw\" : 0, \"NumberOfLasers\": 7, \"PointsPerScan\": 2000, \"RotationsPerSecond\": 20, \"VerticalFOVUpper\": 0, \"VerticalFOVLower\": -25, \"HorizontalFOVStart\": 0, \"HorizontalFOVEnd\": 90, \"DrawDebugPoints\": false } Lidar1 is the name of the lidar. This value will be used in the ros topic name and coordinate frame. X , Y and Z are the position of the lidar relative the vehicle pawn center of the car in NED frame. Roll , Pitch and Yaw are rotations in degrees. NumberOfLasers is the - duh - the number of lasers in the lidar. The lasers are stacked vertically and rotate on the horizontal plane. The lasers are distributed equally to cover the specified vertical field of view. The vertical field of view is specified by choosing the upper ( VerticalFOVUpper ) and lower ( VerticalFOVLower ) limit in degrees. The lower limit specifies the vertical angle between the horizontal plane of the lidar and the most bottom laser. The upper limit specifies the vertical angle between the horizontal plane of the lidar and most upper laser. The horizontal field of view of the lidar is specified with an upper ( HorizontalFOVStart ) and lower ( HorizontalFOVEnd ) limit in degree as well. The lower limit specifies the counterclockwise angle on a top view (negative yaw) from the direction the lidar is pointing towards. The upper limit specifies the clockwise angle on a top view (positive yaw) from the direction the lidar is pointing towards. RotationsPerSecond specifies how fast the lasers spins and how often a pointcloud is captured. There might be slight variations in the actual lidar frequency vs the configured rotation frequency. PointsPerScan is the number of firings per scan within the field of view. If all lasers hit, the returned pointcloud will contain this number of points in each pointcloud. DrawDebugPoints enables visualization of the lidar hits inside the unreal engine game. This is known to impact performance quite a bit. it is recommended to to only use this during debugging.","title":"Lidar"},{"location":"operator/","text":"Operator The operator consists of both a web interface and a webserver. The operator is meant to be used by Formula Student officials to control and keep track of what is happening in the simulation. From this web interface, the official can launch and exit the simulator, select teams and events, start, stop and reset the car and view all logs received by the webserver. All these logs are stored in log files, in the case that the operator crashes. The operator is primarily used during competition by officials. You don't need this for development and testing. Refer to the getting started guide first. Team config The operator uses the team_config.json file located in the /config folder to load all team specific configuration settings into the simulator. This includes the name of each team and their car settings. Whenever the simulation is started via the operator's web interface, the selected team's car settings are written to the settings.json file located in the main folder of the repository. This allows the operator to quickly switch between teams during competition. The contents of the team_config.json file are also used to dynamically load the team selector buttons. Note that the team_config.json file included in this repository is an example file. The team_config.json used during competition will be confidential. Logs Whenever a mission starts, a log file is created in the /operator/logs folder. All logs received by the webserver will be written to this log file, as long as the mission is ongoing. All log files are named using the following naming convention: {team_name}_{mission}_{date}_{time}.txt Prerequisites The operator only works on windows with wsl. The operator and unreal engine simulator will run in Windows, the ros bridge will be launched from the operator inside wsl. Before we start you must build the ros workspace in wsl and clone the repo in windows. Flask - A Python web application framework. To install all dependencies, run the following command inside the /operator folder: $ pip install -r requirements.txt You must have a packaged simulator downloaded. The operator will launch the game when instructed by the user via the web gui. Go to the releases and download the latest version. Extract the zip to the simulator folder. The result should be that the following file and folders exist inside the simulator folder: FSDS.exe FSOnline/ Engine/ Here is described how to export the Unreal project. Usage To start the web server, run the following command in the /operator folder: $ python webserver.py By default, the web interface runs on http://localhost:5000 .","title":"Operator"},{"location":"operator/#operator","text":"The operator consists of both a web interface and a webserver. The operator is meant to be used by Formula Student officials to control and keep track of what is happening in the simulation. From this web interface, the official can launch and exit the simulator, select teams and events, start, stop and reset the car and view all logs received by the webserver. All these logs are stored in log files, in the case that the operator crashes. The operator is primarily used during competition by officials. You don't need this for development and testing. Refer to the getting started guide first.","title":"Operator"},{"location":"operator/#team-config","text":"The operator uses the team_config.json file located in the /config folder to load all team specific configuration settings into the simulator. This includes the name of each team and their car settings. Whenever the simulation is started via the operator's web interface, the selected team's car settings are written to the settings.json file located in the main folder of the repository. This allows the operator to quickly switch between teams during competition. The contents of the team_config.json file are also used to dynamically load the team selector buttons. Note that the team_config.json file included in this repository is an example file. The team_config.json used during competition will be confidential.","title":"Team config"},{"location":"operator/#logs","text":"Whenever a mission starts, a log file is created in the /operator/logs folder. All logs received by the webserver will be written to this log file, as long as the mission is ongoing. All log files are named using the following naming convention: {team_name}_{mission}_{date}_{time}.txt","title":"Logs"},{"location":"operator/#prerequisites","text":"The operator only works on windows with wsl. The operator and unreal engine simulator will run in Windows, the ros bridge will be launched from the operator inside wsl. Before we start you must build the ros workspace in wsl and clone the repo in windows. Flask - A Python web application framework. To install all dependencies, run the following command inside the /operator folder: $ pip install -r requirements.txt You must have a packaged simulator downloaded. The operator will launch the game when instructed by the user via the web gui. Go to the releases and download the latest version. Extract the zip to the simulator folder. The result should be that the following file and folders exist inside the simulator folder: FSDS.exe FSOnline/ Engine/ Here is described how to export the Unreal project.","title":"Prerequisites"},{"location":"operator/#usage","text":"To start the web server, run the following command in the /operator folder: $ python webserver.py By default, the web interface runs on http://localhost:5000 .","title":"Usage"},{"location":"ros-bridge/","text":"FSDS ROS bridge A ROS wrapper over the AirSim C++ Car client library. This code is based on the original AirSim ROS wrapper for the Multirotor API and provides an interface between AirSim + Unreal Engine and your ros-based autonomous system. The fsds_ros_bridge is supposed to be launched pointing at the Autonomous System's ROS master so that it can publish and subscribe to topics within the autonomous system. Physically this node should run on the airsim simulation server (that is the one that also runs the Unreal) project. The node connects to the AirSim plugin, periodically retrieves sensor data (images, lidar, imu, gps) and publishes it on ROS topics. It listens for car setpoints on other another and forwards these to the AirSim plugin. Nodes The fsds_ros_bridge.launch launches the following nodes: * /fsds/ros_bridge node responsible for IMU, GPS, lidar, vehicle setpoints and go/finish signals. * /fsds/camera/CAMERANAME node is run for each camera configured in the settings.json . The nodes are launched using the cameralauncher.py script. Published topics Topic name Description Message Rate (hz) /fsds/gps This the current GPS coordinates of the drone in airsim. Read all about the gps simulation model here . Data is in the fsds/FSCar frame. sensor_msgs/NavSatFix 10 /fsds/imu Velocity, orientation and acceleration information. Read all about the IMU model here . Data is in the fsds/FSCar (enu) frame. sensor_msgs/Imu 250 /fsds/gss Ground speed sensor provide linear velocity of the vehicle ( fsds/FSCar ). Velocity is m/s. geometry_msgs/TwistStamped 100 /fsds/testing_only/odom Ground truth car position and orientation in ENU frame about the CoG of the car ( fsds/FSCar ). The units are m for distance. The orientation are expressed in terms of quaternions. The message is in the fsds/map frame. This is a frame that is not (yet) used anywhere else and is just here so you can easely reference it if needed. THIS WILL NOT BE STREAMED DURING COMPETITION. nav_msgs/Odometry 250 /fsds/testing_only/track Ground truth cone position and color with respect to the starting location of the car in ENU. Currently this only publishes the initial position of cones that are part of the track spline. Any cones placed manually in the world are not published here. Additionally, the track is published once and the message is latched (meaning it is always available for a newly created subscriber). THIS WILL NOT BE STREAMED DURING COMPETITION. fs_msgs/Track Latched /fsds/camera/CAMERA_NAME One of this topic type will exist for every camera specified in the settings.json file. On this topic, camera frames are published. The format will be bgra8. CAMERA_NAME will be replaced by the corresponding in the Cameras object in the settings.json file. IMAGE_TYPE is determand by the SensorType field. When choosing 0, it will be 'Scene'. sensor_msgs/Image ~18 /fsds/lidar/LIDARNAME Publishes the lidar points for each lidar sensor. All points are in the fsds/LIDARNAME frame. Transformations between the fsds/LIDARNAME and fsds/FSCar frame are being published regularly. More info on the lidar sensor can be found here sensor_msgs/PointCloud `RotationsPerSecond param in settings.json /fsds/signal/go GO signal that is sent every second by the ROS bridge.The car is only allowed to drive once this message has been received. If no GO signal is received for more than 4 seconds, the AS can assume that fsds_ros_bridge has been shut down. This message also includes the mission type and track. More info about signal topics can be found in the integration handbook fs_msgs/GoSignal 1 /tf_static See Coordinate frames and transforms tf2_msgs/TFMessage 1 Subscribed topics Topic name Description Message /fsds/control_command This message includes the dimensionless values throttle, steering and brake. Throttle and brake range from 0 to 1. For steering -1 steers full to the left and +1 steers full to the right. The contents of this message fill the essential parts of the msr::airlib::CarApiBase::CarControl struct. This is the only way to control the car when the airsim ROS client is connected (keyboard will no longer work!). fs_msgs/ControlCommand /fsds/signal/finished Finished signal that is sent by the AS to stop the mission. The ros bridge will forward the signal to the operator which in turn will stop the ros bridge and finish the run. fs_msgs/FinishedSignal Services /fsds/reset fsds_ros_bridge/Reset Resets car to start location. Units If a topic streams a standard ROS message (like sensor_msgs/Imu ) then the units will be the recommended units in the message documentation. Custom messages (from the fs_msgs package) use the units specified in the message documentation as well. If in doubt, interpret distances in meters, angles in radians and rates in m/s and rad/s, etc. Coordinate frames and transforms The primary frame is the fsds/FSCar frame, which is fixed at the center of the car following the ENU coordinate system convention. The center of the car is the Unreal Engine car pawn position, which in turn is also the center of gravity. The ros bridge regularly publishes static transforms between the fsds/FSCar frame and each of the cameras and lidars. Naming of these frames is fsds/SENSORNAME . For example, a lidar named Example will publish it's points in the fsds/Example frame. The position and orientation of a camera named Test will become available in the frame /fsds/Test . PLEASE NOTE : the transforms published on the /tf_static topic are a direct conversion of the transforms specified in the settings.json file but expressed in a ENU coordinate system instead of in a NED coordinate system (which is what the settings.json file uses). Read more about the differences between ENU and NED here . For a quick illustration of the two frames, see the image below: Only static transforms within the vehicle are published. Transforms to the ground truth are disabled because this would take away the state estimation challenge of the competition. Parameters /fsds/ros_bridge/update_gps_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.1 seconds (10hz). Timer callback frequency for updating and publishing the gps sensordata. This value must be equal or higher to the update frequency of the sensor configured in the settings.json /fsds/ros_bridge/update_imu_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.004 seconds (250hz). Timer callback frequency for updating and publishing the imu messages. This value must be equal or higher to the minimual sample rate of the sensor configured in the settings.json /fsds/ros_bridge/update_gss_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.01 seconds (100hz). Timer callback frequency for updating and publishing the ground speed sensor messages. /fsds/ros_bridge/update_odom_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.004 seconds (250hz). Timer callback frequency for updating and publishing the odometry. /fsds/ros_bridge/publish_static_tf_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 1 seconds (1 hz). The frequency at which the static transforms are published. /fsds/ros_bridge/update_lidar_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.1 seconds (10 hz). The frequency at which the lidar is publshed. /fsds/ros_bridge/competition_mode [bool] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: false , during competition set to true If competition mode is enabled, the testing_only topics won't be available. /fsds/ros_bridge/manul_mode [bool] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: false Do not enable vehicle api control. You can controll the car using the keyboard in the simulator. Visualization This package contains some useful launch and config files which will help you in visualizing the data being streamed through the above topics. To open Rviz with this configuration file, run roslaunch fsds_ros_bridge fsds_ros_bridge.launch rviz:=true . To open Multiplot with this configuration file, run roslaunch fsds_ros_bridge fsds_ros_bridge.launch plot:=true . Monitoring Performance monitoring of the ROS Bridge is described here","title":"ROS Bridge"},{"location":"ros-bridge/#fsds-ros-bridge","text":"A ROS wrapper over the AirSim C++ Car client library. This code is based on the original AirSim ROS wrapper for the Multirotor API and provides an interface between AirSim + Unreal Engine and your ros-based autonomous system. The fsds_ros_bridge is supposed to be launched pointing at the Autonomous System's ROS master so that it can publish and subscribe to topics within the autonomous system. Physically this node should run on the airsim simulation server (that is the one that also runs the Unreal) project. The node connects to the AirSim plugin, periodically retrieves sensor data (images, lidar, imu, gps) and publishes it on ROS topics. It listens for car setpoints on other another and forwards these to the AirSim plugin.","title":"FSDS ROS bridge"},{"location":"ros-bridge/#nodes","text":"The fsds_ros_bridge.launch launches the following nodes: * /fsds/ros_bridge node responsible for IMU, GPS, lidar, vehicle setpoints and go/finish signals. * /fsds/camera/CAMERANAME node is run for each camera configured in the settings.json . The nodes are launched using the cameralauncher.py script.","title":"Nodes"},{"location":"ros-bridge/#published-topics","text":"Topic name Description Message Rate (hz) /fsds/gps This the current GPS coordinates of the drone in airsim. Read all about the gps simulation model here . Data is in the fsds/FSCar frame. sensor_msgs/NavSatFix 10 /fsds/imu Velocity, orientation and acceleration information. Read all about the IMU model here . Data is in the fsds/FSCar (enu) frame. sensor_msgs/Imu 250 /fsds/gss Ground speed sensor provide linear velocity of the vehicle ( fsds/FSCar ). Velocity is m/s. geometry_msgs/TwistStamped 100 /fsds/testing_only/odom Ground truth car position and orientation in ENU frame about the CoG of the car ( fsds/FSCar ). The units are m for distance. The orientation are expressed in terms of quaternions. The message is in the fsds/map frame. This is a frame that is not (yet) used anywhere else and is just here so you can easely reference it if needed. THIS WILL NOT BE STREAMED DURING COMPETITION. nav_msgs/Odometry 250 /fsds/testing_only/track Ground truth cone position and color with respect to the starting location of the car in ENU. Currently this only publishes the initial position of cones that are part of the track spline. Any cones placed manually in the world are not published here. Additionally, the track is published once and the message is latched (meaning it is always available for a newly created subscriber). THIS WILL NOT BE STREAMED DURING COMPETITION. fs_msgs/Track Latched /fsds/camera/CAMERA_NAME One of this topic type will exist for every camera specified in the settings.json file. On this topic, camera frames are published. The format will be bgra8. CAMERA_NAME will be replaced by the corresponding in the Cameras object in the settings.json file. IMAGE_TYPE is determand by the SensorType field. When choosing 0, it will be 'Scene'. sensor_msgs/Image ~18 /fsds/lidar/LIDARNAME Publishes the lidar points for each lidar sensor. All points are in the fsds/LIDARNAME frame. Transformations between the fsds/LIDARNAME and fsds/FSCar frame are being published regularly. More info on the lidar sensor can be found here sensor_msgs/PointCloud `RotationsPerSecond param in settings.json /fsds/signal/go GO signal that is sent every second by the ROS bridge.The car is only allowed to drive once this message has been received. If no GO signal is received for more than 4 seconds, the AS can assume that fsds_ros_bridge has been shut down. This message also includes the mission type and track. More info about signal topics can be found in the integration handbook fs_msgs/GoSignal 1 /tf_static See Coordinate frames and transforms tf2_msgs/TFMessage 1","title":"Published topics"},{"location":"ros-bridge/#subscribed-topics","text":"Topic name Description Message /fsds/control_command This message includes the dimensionless values throttle, steering and brake. Throttle and brake range from 0 to 1. For steering -1 steers full to the left and +1 steers full to the right. The contents of this message fill the essential parts of the msr::airlib::CarApiBase::CarControl struct. This is the only way to control the car when the airsim ROS client is connected (keyboard will no longer work!). fs_msgs/ControlCommand /fsds/signal/finished Finished signal that is sent by the AS to stop the mission. The ros bridge will forward the signal to the operator which in turn will stop the ros bridge and finish the run. fs_msgs/FinishedSignal","title":"Subscribed topics"},{"location":"ros-bridge/#services","text":"/fsds/reset fsds_ros_bridge/Reset Resets car to start location.","title":"Services"},{"location":"ros-bridge/#units","text":"If a topic streams a standard ROS message (like sensor_msgs/Imu ) then the units will be the recommended units in the message documentation. Custom messages (from the fs_msgs package) use the units specified in the message documentation as well. If in doubt, interpret distances in meters, angles in radians and rates in m/s and rad/s, etc.","title":"Units"},{"location":"ros-bridge/#coordinate-frames-and-transforms","text":"The primary frame is the fsds/FSCar frame, which is fixed at the center of the car following the ENU coordinate system convention. The center of the car is the Unreal Engine car pawn position, which in turn is also the center of gravity. The ros bridge regularly publishes static transforms between the fsds/FSCar frame and each of the cameras and lidars. Naming of these frames is fsds/SENSORNAME . For example, a lidar named Example will publish it's points in the fsds/Example frame. The position and orientation of a camera named Test will become available in the frame /fsds/Test . PLEASE NOTE : the transforms published on the /tf_static topic are a direct conversion of the transforms specified in the settings.json file but expressed in a ENU coordinate system instead of in a NED coordinate system (which is what the settings.json file uses). Read more about the differences between ENU and NED here . For a quick illustration of the two frames, see the image below: Only static transforms within the vehicle are published. Transforms to the ground truth are disabled because this would take away the state estimation challenge of the competition.","title":"Coordinate frames and transforms"},{"location":"ros-bridge/#parameters","text":"/fsds/ros_bridge/update_gps_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.1 seconds (10hz). Timer callback frequency for updating and publishing the gps sensordata. This value must be equal or higher to the update frequency of the sensor configured in the settings.json /fsds/ros_bridge/update_imu_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.004 seconds (250hz). Timer callback frequency for updating and publishing the imu messages. This value must be equal or higher to the minimual sample rate of the sensor configured in the settings.json /fsds/ros_bridge/update_gss_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.01 seconds (100hz). Timer callback frequency for updating and publishing the ground speed sensor messages. /fsds/ros_bridge/update_odom_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.004 seconds (250hz). Timer callback frequency for updating and publishing the odometry. /fsds/ros_bridge/publish_static_tf_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 1 seconds (1 hz). The frequency at which the static transforms are published. /fsds/ros_bridge/update_lidar_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.1 seconds (10 hz). The frequency at which the lidar is publshed. /fsds/ros_bridge/competition_mode [bool] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: false , during competition set to true If competition mode is enabled, the testing_only topics won't be available. /fsds/ros_bridge/manul_mode [bool] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: false Do not enable vehicle api control. You can controll the car using the keyboard in the simulator.","title":"Parameters"},{"location":"ros-bridge/#visualization","text":"This package contains some useful launch and config files which will help you in visualizing the data being streamed through the above topics. To open Rviz with this configuration file, run roslaunch fsds_ros_bridge fsds_ros_bridge.launch rviz:=true . To open Multiplot with this configuration file, run roslaunch fsds_ros_bridge fsds_ros_bridge.launch plot:=true .","title":"Visualization"},{"location":"ros-bridge/#monitoring","text":"Performance monitoring of the ROS Bridge is described here","title":"Monitoring"},{"location":"software-install-instructions/","text":"Required software installation instructions This page helps you install software required for running and developing the simulator. You probably only need parts of it. The other guides in this documentation will tell you what you need. Install Unreal Engine (Windows) Ensure your windows user has no special characters! If it contains special characters there will be a conflict with your Win10 user folder name, and mess up unreal engine reading the directory path. Go to unrealengine.com and download the epic installer. You need an account for this. Install the epic installer. Launch the epic installer and install Unreal Engine 4.25 Install Unreal Engine (Ubuntu) This project uses Unreal Engine 4.25. Before you can use unreal engine on Ubuntu, you must register at unrealengine.com and get access to the UnrealEngine github repo. After you get access, clone the repo and build it: git clone --depth=1 -b 4.25 https://github.com/EpicGames/UnrealEngine.git cd UnrealEngine ./Setup.sh && ./GenerateProjectFiles.sh && make Run it by executing ./Engine/Binaries/Linux/UE4Editor . Install visual studio 2019 (Windows) Download visual studio 2019 (community edition) During installation, choose the following components: Desktop development with C++ Game development with C++ Linux development with C++ At 'Invidual Components select: C++ CMake tools for Windows Windows 10 SDK 10.0.18.362.0 .NET Framework 4.7 SDK Windows Subsystem for Linux (WSL) In case you want to run the ros bridge on Windows, you will need Windows Subsystem for Linux Enable Windows Subsystem for Linux 1 (WSL1) . No need to update to version 2. Install ubuntu 18.04 LTS. If you are on windows server, enable windows susbsystem for linux in the server manager and install ubuntu . Install ROS Melodic (Ubuntu / WSL) sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list' sudo apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654 curl -sSL 'http://keyserver.ubuntu.com/pks/lookup?op=get&search=0xC1CF6E31E6BADE8868B172B4F42ED6FBAB17C654' | sudo apt-key add - sudo apt update sudo apt install ros-melodic-desktop Add the following line to end of your ~/.bashrc file: source /opt/ros/melodic/setup.bash source ~/Formula-Student-Driverless-Simulator/ros/devel/setup.bash Gui applications from WSL (Xming) By default, if you are running Windows Subsystem for Linux with Ubuntu, you can't run gui applications. This is super annoying if you want to use rqt applicatoins like rviz or rqt_plot. It is easy to get this working though! Just install Xming on windows, and run it. Next, go into the Ubuntu terminal and run export DISPLAY=:0 . Now you can run any all them gui apps! You can even add export DISPLAY=:0 to your ~/.bashrc to always be able to use gui apps without having to manually run export.","title":"Software install instructions"},{"location":"software-install-instructions/#required-software-installation-instructions","text":"This page helps you install software required for running and developing the simulator. You probably only need parts of it. The other guides in this documentation will tell you what you need.","title":"Required software installation instructions"},{"location":"software-install-instructions/#install-unreal-engine-windows","text":"Ensure your windows user has no special characters! If it contains special characters there will be a conflict with your Win10 user folder name, and mess up unreal engine reading the directory path. Go to unrealengine.com and download the epic installer. You need an account for this. Install the epic installer. Launch the epic installer and install Unreal Engine 4.25","title":"Install Unreal Engine (Windows)"},{"location":"software-install-instructions/#install-unreal-engine-ubuntu","text":"This project uses Unreal Engine 4.25. Before you can use unreal engine on Ubuntu, you must register at unrealengine.com and get access to the UnrealEngine github repo. After you get access, clone the repo and build it: git clone --depth=1 -b 4.25 https://github.com/EpicGames/UnrealEngine.git cd UnrealEngine ./Setup.sh && ./GenerateProjectFiles.sh && make Run it by executing ./Engine/Binaries/Linux/UE4Editor .","title":"Install Unreal Engine (Ubuntu)"},{"location":"software-install-instructions/#install-visual-studio-2019-windows","text":"Download visual studio 2019 (community edition) During installation, choose the following components: Desktop development with C++ Game development with C++ Linux development with C++ At 'Invidual Components select: C++ CMake tools for Windows Windows 10 SDK 10.0.18.362.0 .NET Framework 4.7 SDK","title":"Install visual studio 2019 (Windows)"},{"location":"software-install-instructions/#windows-subsystem-for-linux-wsl","text":"In case you want to run the ros bridge on Windows, you will need Windows Subsystem for Linux Enable Windows Subsystem for Linux 1 (WSL1) . No need to update to version 2. Install ubuntu 18.04 LTS. If you are on windows server, enable windows susbsystem for linux in the server manager and install ubuntu .","title":"Windows Subsystem for Linux (WSL)"},{"location":"software-install-instructions/#install-ros-melodic-ubuntu-wsl","text":"sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list' sudo apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654 curl -sSL 'http://keyserver.ubuntu.com/pks/lookup?op=get&search=0xC1CF6E31E6BADE8868B172B4F42ED6FBAB17C654' | sudo apt-key add - sudo apt update sudo apt install ros-melodic-desktop Add the following line to end of your ~/.bashrc file: source /opt/ros/melodic/setup.bash source ~/Formula-Student-Driverless-Simulator/ros/devel/setup.bash","title":"Install ROS Melodic (Ubuntu / WSL)"},{"location":"software-install-instructions/#gui-applications-from-wsl-xming","text":"By default, if you are running Windows Subsystem for Linux with Ubuntu, you can't run gui applications. This is super annoying if you want to use rqt applicatoins like rviz or rqt_plot. It is easy to get this working though! Just install Xming on windows, and run it. Next, go into the Ubuntu terminal and run export DISPLAY=:0 . Now you can run any all them gui apps! You can even add export DISPLAY=:0 to your ~/.bashrc to always be able to use gui apps without having to manually run export.","title":"Gui applications from WSL (Xming)"},{"location":"spectator/","text":"Spectator The spectator provides an eye into the virtual world from an external computer. Through a network connection the virtual world state is replicated and shown on the screen. The user can operate the camera with its mouse and keyboard. Launching the spectator A simulator launched as 'server' will accept external viewers (spectators) to join the game. The spectators are not able to interact with the world. Spectators can enter the ip of the server in the main menu to connect to a running simulator. Multiple simultaneous spectators in a single world should work as well. If a spectator loses connection to the server it will go back to the main menu. If you want to give access to clients from external computers you must ensure the specified port is accessible. In most cases you will have to add a firewall rule to allow the traffic. When behind a router you might need to do some port forwarding. Both TCP and UDP traffic must be able to travel from client to server on port 7777. To skip the menu and launch the game as server or spectator directly, you can use the following commandline options: Run the game as a server FSDS.exe /Game/TrainingMap?listen -log This runs the simulator like normal but now external clients (spectators) are welcome to connect. Running the spectator FSDS.exe 0.0.0.0 -log Where 0.0.0.0 is replaced by the external ip of the server. Using the spectator When starting the spectator it will be launched in follow-car mode. In this mode the camera will always point at the vehicle. When the vehicle crosses a triggerline, the camera will move to another viewpoint. You can toggle the follow-car mode using the F key on your keyboard. When not following the car you can use the wsdaqe keys to move around and your mouse to look around. Adding spectator points to the map Spectator viewpoints are places in the world where the spectator can be teleported to. These locations are defined in the map and cannot be changed during the simulation. To create a new viewpoint, add a new CameraActor to the world and place it wherever you like. Next, add a AirsimSpectatorTeleportTrigger to the world. In the settings, set the camera to the one you just created. Whenever the vehicle touches this trigger object, the spectator will be teleported to the selected camera (in follow-car mode).","title":"Spectator"},{"location":"spectator/#spectator","text":"The spectator provides an eye into the virtual world from an external computer. Through a network connection the virtual world state is replicated and shown on the screen. The user can operate the camera with its mouse and keyboard.","title":"Spectator"},{"location":"spectator/#launching-the-spectator","text":"A simulator launched as 'server' will accept external viewers (spectators) to join the game. The spectators are not able to interact with the world. Spectators can enter the ip of the server in the main menu to connect to a running simulator. Multiple simultaneous spectators in a single world should work as well. If a spectator loses connection to the server it will go back to the main menu. If you want to give access to clients from external computers you must ensure the specified port is accessible. In most cases you will have to add a firewall rule to allow the traffic. When behind a router you might need to do some port forwarding. Both TCP and UDP traffic must be able to travel from client to server on port 7777. To skip the menu and launch the game as server or spectator directly, you can use the following commandline options: Run the game as a server FSDS.exe /Game/TrainingMap?listen -log This runs the simulator like normal but now external clients (spectators) are welcome to connect. Running the spectator FSDS.exe 0.0.0.0 -log Where 0.0.0.0 is replaced by the external ip of the server.","title":"Launching the spectator"},{"location":"spectator/#using-the-spectator","text":"When starting the spectator it will be launched in follow-car mode. In this mode the camera will always point at the vehicle. When the vehicle crosses a triggerline, the camera will move to another viewpoint. You can toggle the follow-car mode using the F key on your keyboard. When not following the car you can use the wsdaqe keys to move around and your mouse to look around.","title":"Using the spectator"},{"location":"spectator/#adding-spectator-points-to-the-map","text":"Spectator viewpoints are places in the world where the spectator can be teleported to. These locations are defined in the map and cannot be changed during the simulation. To create a new viewpoint, add a new CameraActor to the world and place it wherever you like. Next, add a AirsimSpectatorTeleportTrigger to the world. In the settings, set the camera to the one you just created. Whenever the vehicle touches this trigger object, the spectator will be teleported to the selected camera (in follow-car mode).","title":"Adding spectator points to the map"},{"location":"statistics/","text":"ROS Bridge Monitoring ROS Bridge and Rpc performance monitoring is done by the Statistics (/ros/src/fsds_ros_bridge/include/statistics.h) class. This class is used by the AirsimROSWrapper class to monitor latency of rpc calls as well as the frequency of ceratain topics in the ROS network. The statistics gathered by the methods of this class and temporarily stored by private class members will be printed live at about 1Hz. This will be useful both for development of the simulator as well as for ensuring fairness in the competition, where a team will be allowed to retry a run if problems are diagnosed on the simulator side. The following describes how the class and the auxiliary classes Timer and ROSMsgCounter measure performance and are implemented, so that as a developer you can monitor new publishers, subscribers, rpc calls, or actually any line of code in the AirSimROSWrapper class. We are gathering statistics about: Rpc calls: getGpsData getCarState getImuData (vector) simGetImages (vector) getLidarData (vector) setCarControls ROS publishing frequency of the following publishers: odom_local_ned_pub global_gps_pub cam_pub_vec_ (vector) lidar_pub_vec_ (vector) imu_pub_vec_ (vector) ROS callback frequency of the following subscriber(s): control_cmd_sub There will be one instance of this class for each Rpc call to be monitored. To make a latency measurement, the appropriate instance pointer has to be passed to a new Timer class (see below): ros_bridge::Statistics rpcCallStatistics = ros_bridge::Statistics(\"rpcCallName\"); { // Enter scope to be timed ros_bridge::Timer timer(&rpcCallStatistics); // Start timing // do the Rpc Call } // Go out of scope -> call the timer destructor // which automatically stores the time elapsed in the instance of // the class that was passed There will also be an instance of this class for each ROS publisher/subscriber. To count a new incoming or outgoing message the simple construct below can be used: ros_bridge::Statistics pubSubStatistics = ros_bridge::Statistics(\"pubSubName\");; // For a publisher: { // pass persistent Statistics object (by reference) ros_bridge::ROSMsgCounter counter(&pubSubStatistics); pubSub.publish(data); } // For a subscriber: void callback(msg) { // pass persistent Statistics object (by reference) ros_bridge::ROSMsgCounter counter(&pubSubStatistics); // Do something with msg } // scope ends, destructor is called and count is incremented for // the Statistics object In the 1Hz ROS timer, the Print function will be called (the wrapper which applies this action to all the instances) followed by the Reset function (the wrapper which applies this action to all the instances) which ensures that counters are set to 0 and that vectors of durations (latencies) are emptied.","title":"ROS Bridge statistics"},{"location":"statistics/#ros-bridge-monitoring","text":"ROS Bridge and Rpc performance monitoring is done by the Statistics (/ros/src/fsds_ros_bridge/include/statistics.h) class. This class is used by the AirsimROSWrapper class to monitor latency of rpc calls as well as the frequency of ceratain topics in the ROS network. The statistics gathered by the methods of this class and temporarily stored by private class members will be printed live at about 1Hz. This will be useful both for development of the simulator as well as for ensuring fairness in the competition, where a team will be allowed to retry a run if problems are diagnosed on the simulator side. The following describes how the class and the auxiliary classes Timer and ROSMsgCounter measure performance and are implemented, so that as a developer you can monitor new publishers, subscribers, rpc calls, or actually any line of code in the AirSimROSWrapper class. We are gathering statistics about: Rpc calls: getGpsData getCarState getImuData (vector) simGetImages (vector) getLidarData (vector) setCarControls ROS publishing frequency of the following publishers: odom_local_ned_pub global_gps_pub cam_pub_vec_ (vector) lidar_pub_vec_ (vector) imu_pub_vec_ (vector) ROS callback frequency of the following subscriber(s): control_cmd_sub There will be one instance of this class for each Rpc call to be monitored. To make a latency measurement, the appropriate instance pointer has to be passed to a new Timer class (see below): ros_bridge::Statistics rpcCallStatistics = ros_bridge::Statistics(\"rpcCallName\"); { // Enter scope to be timed ros_bridge::Timer timer(&rpcCallStatistics); // Start timing // do the Rpc Call } // Go out of scope -> call the timer destructor // which automatically stores the time elapsed in the instance of // the class that was passed There will also be an instance of this class for each ROS publisher/subscriber. To count a new incoming or outgoing message the simple construct below can be used: ros_bridge::Statistics pubSubStatistics = ros_bridge::Statistics(\"pubSubName\");; // For a publisher: { // pass persistent Statistics object (by reference) ros_bridge::ROSMsgCounter counter(&pubSubStatistics); pubSub.publish(data); } // For a subscriber: void callback(msg) { // pass persistent Statistics object (by reference) ros_bridge::ROSMsgCounter counter(&pubSubStatistics); // Do something with msg } // scope ends, destructor is called and count is incremented for // the Statistics object In the 1Hz ROS timer, the Print function will be called (the wrapper which applies this action to all the instances) followed by the Reset function (the wrapper which applies this action to all the instances) which ensures that counters are set to 0 and that vectors of durations (latencies) are emptied.","title":"ROS Bridge Monitoring"},{"location":"system-overview/","text":"This document is intended to read top-to-bottom. Do yourself a favour and read the whole thing without skipping ;) Formula Student Driverless Simulation: System overview FSDS is built around Unreal Engine 4 (the game engine) and the AirSim plugin. The game engine does all the graphical rendering, collision simulation and car movement simulation. Autonomous systems connect to the simulation using ROS topics provided by the ros-bridge. A separate component - the operator - will provide an interface to officials to controll the simulation during competition. Autonomous systems Every Autonomous System (AS) will run on its separated environment. Ideally, this would be separate virtual machines but also Docker containers could be used. The ASs are expected to continuously run a ROS master. When the simulator is ready to do a drive for the given AS it will launch a ROS node connected to this AS's ROS master. This ROS node will exist outside of the AS's environment. Instead, it will run on the simulator computer. The simulator ROS node will publish sensor data and listen for car setpoints on a set of topics defined here . When the simulation is finished, the ROS node will disconnect from the AS's ROS master and sensor data stops coming in. During the competition, the teams will not be allowed to access their ASs. All remote access to the environments will be cut off completely. So every AS must be able to run multiple missions without human interference. To let the simulation know what is expected from it, the simulator will send signal messages a few seconds before the event start. These messages contain information about the mission (trackdrive, autocross, etc). When the AS receives a mission message it can expect to receive sensor data shortly after. Since the lap timer for all events start whenever the car crosses the start-line, the ASs can take all the time they need to launch their relevant algorithms for the mission (within a reasonable time). The Operator The operator is a continuously running program that is like the spider in the web. The operator offers a web interface to the event's officials. Using this web interface the officials can choose which team/car is going to drive and on which track. The official will also be able to send the start signal, view lap times, down or out cones, car off course's. There is also an emergency stop button in case the car is uncontrollable. Only 1 car at the time will be able to run on this AS. The official can select which team, and thus which AS, is currently selected. When the team changes, not only the AS but also the car inside the virtual world will change. The sensor suite (sensor types and locations, defined in a custom settings.json file) and the car livery (looks of the car) are updated. The operator keeps track of these details and passes them along to the virtual world to ensure accurate representation. When the operator wants to connect an AS to the simulated world, it launches the ROS bridge. Read more about the ROS bridge below. If the operator wants to disconnect the AS from the simulated world it stops the bridge node and the connections are stopped. During a mission, the operator keeps polling the world for 'referee state'. This is information that in a physical world would be relevant to the referee. Currently, this includes a list of down or out cones and the timestamp of when they went down or out, a list of lap times and a list of when the car went off-track. More information will for sure be added. What happens inside the simulation is stored in a single logbook. This includes all referee updates, which ASs were selected and which tracks were used. If something unexpected went wrong like a system's crash or error, a short description of what happened is included in this logbook. It gives a timeline of everything that happened to always go back afterwards and check what happened. The logbook is stored on disk so that in the event of a whole system crash we will still have the logbook. It is also shown within the referee's web interface. You can run the simulation stack yourself perfectly fine without the operator. The operator is just a tool for officials to easely manage the competition. The ROS Bridge The ROS bridge node connects to the simulated world inside Unreal Engine using AirSim (more on that later). On the one hand, it requests sensor data and passes it along on ROS topics to the current AS. On the other hand, it receives car control commands from the AS and forwards it to the virtual world. So it acts as a bridge between the two. The node that is launched pointing at the AS's ROS master so that it can publish and subscribe to topics within the AS. Physically this node runs on the server where the Unreal world is being simulated. The node is launched by the operator. When the operator launches the ROS bridge it passes along some mission variables. This includes mission type (trackdrive or autocross) and information about how it can use data collected in previous runs. For example, first the AS will receive \"autocross on track A\" and it knows it cannot use any previous collected information. Then it receives \"trackdrive on track A\" and it knows it can use data collected in the first autocross drive to go faster. It is the responsibilty of the teams to detect when they are 'done'. After the required number of laps, the car has to come to a full stop. If the AS wants to store things (like track information), this is the time to wrap those up. In case of a successfull run the official will instruct the operator to stop the ROS bridge and the AS won't receive sensor data anymore. When the official presses the emergency brake, the connection between the ROS bridge is stopped immediately and the operator will send one last car setpoint to make the car come to a stop. Thers is no 'stop' signal from the simulator to the AS. At this point only ROS is supported, at this moment there are no plans to support other technologies. The virtual world (Unreal Engine and AirSim) The actual simulation takes place inside an Unreal Engine 4 world. Unreal takes care of the heavy lifting involved with a real-life simulation. All physics, lighting and world-building are handled by Unreal. AirSim is used to connect Unreal with the operator and ROS bridge. This plugin is added inside the Unreal world and takes control of most of the game logic. It receives the sensor suite and simulates the sensors, it moves the car according to trajectory setpoints and exposes an RPC API for external management. This RPC API is used by the simulator to interact with the world. The Spectator The spectator provides an eye into the virtual world from an external computer. Through a network connection the virtual world state is replicated and shown on the screen. The user can operate the camera with its mouse and keyboard. Read more about usage of the spectator here .","title":"System overview"},{"location":"system-overview/#formula-student-driverless-simulation-system-overview","text":"FSDS is built around Unreal Engine 4 (the game engine) and the AirSim plugin. The game engine does all the graphical rendering, collision simulation and car movement simulation. Autonomous systems connect to the simulation using ROS topics provided by the ros-bridge. A separate component - the operator - will provide an interface to officials to controll the simulation during competition.","title":"Formula Student Driverless Simulation: System overview"},{"location":"system-overview/#autonomous-systems","text":"Every Autonomous System (AS) will run on its separated environment. Ideally, this would be separate virtual machines but also Docker containers could be used. The ASs are expected to continuously run a ROS master. When the simulator is ready to do a drive for the given AS it will launch a ROS node connected to this AS's ROS master. This ROS node will exist outside of the AS's environment. Instead, it will run on the simulator computer. The simulator ROS node will publish sensor data and listen for car setpoints on a set of topics defined here . When the simulation is finished, the ROS node will disconnect from the AS's ROS master and sensor data stops coming in. During the competition, the teams will not be allowed to access their ASs. All remote access to the environments will be cut off completely. So every AS must be able to run multiple missions without human interference. To let the simulation know what is expected from it, the simulator will send signal messages a few seconds before the event start. These messages contain information about the mission (trackdrive, autocross, etc). When the AS receives a mission message it can expect to receive sensor data shortly after. Since the lap timer for all events start whenever the car crosses the start-line, the ASs can take all the time they need to launch their relevant algorithms for the mission (within a reasonable time).","title":"Autonomous systems"},{"location":"system-overview/#the-operator","text":"The operator is a continuously running program that is like the spider in the web. The operator offers a web interface to the event's officials. Using this web interface the officials can choose which team/car is going to drive and on which track. The official will also be able to send the start signal, view lap times, down or out cones, car off course's. There is also an emergency stop button in case the car is uncontrollable. Only 1 car at the time will be able to run on this AS. The official can select which team, and thus which AS, is currently selected. When the team changes, not only the AS but also the car inside the virtual world will change. The sensor suite (sensor types and locations, defined in a custom settings.json file) and the car livery (looks of the car) are updated. The operator keeps track of these details and passes them along to the virtual world to ensure accurate representation. When the operator wants to connect an AS to the simulated world, it launches the ROS bridge. Read more about the ROS bridge below. If the operator wants to disconnect the AS from the simulated world it stops the bridge node and the connections are stopped. During a mission, the operator keeps polling the world for 'referee state'. This is information that in a physical world would be relevant to the referee. Currently, this includes a list of down or out cones and the timestamp of when they went down or out, a list of lap times and a list of when the car went off-track. More information will for sure be added. What happens inside the simulation is stored in a single logbook. This includes all referee updates, which ASs were selected and which tracks were used. If something unexpected went wrong like a system's crash or error, a short description of what happened is included in this logbook. It gives a timeline of everything that happened to always go back afterwards and check what happened. The logbook is stored on disk so that in the event of a whole system crash we will still have the logbook. It is also shown within the referee's web interface. You can run the simulation stack yourself perfectly fine without the operator. The operator is just a tool for officials to easely manage the competition.","title":"The Operator"},{"location":"system-overview/#the-ros-bridge","text":"The ROS bridge node connects to the simulated world inside Unreal Engine using AirSim (more on that later). On the one hand, it requests sensor data and passes it along on ROS topics to the current AS. On the other hand, it receives car control commands from the AS and forwards it to the virtual world. So it acts as a bridge between the two. The node that is launched pointing at the AS's ROS master so that it can publish and subscribe to topics within the AS. Physically this node runs on the server where the Unreal world is being simulated. The node is launched by the operator. When the operator launches the ROS bridge it passes along some mission variables. This includes mission type (trackdrive or autocross) and information about how it can use data collected in previous runs. For example, first the AS will receive \"autocross on track A\" and it knows it cannot use any previous collected information. Then it receives \"trackdrive on track A\" and it knows it can use data collected in the first autocross drive to go faster. It is the responsibilty of the teams to detect when they are 'done'. After the required number of laps, the car has to come to a full stop. If the AS wants to store things (like track information), this is the time to wrap those up. In case of a successfull run the official will instruct the operator to stop the ROS bridge and the AS won't receive sensor data anymore. When the official presses the emergency brake, the connection between the ROS bridge is stopped immediately and the operator will send one last car setpoint to make the car come to a stop. Thers is no 'stop' signal from the simulator to the AS. At this point only ROS is supported, at this moment there are no plans to support other technologies.","title":"The ROS Bridge"},{"location":"system-overview/#the-virtual-world-unreal-engine-and-airsim","text":"The actual simulation takes place inside an Unreal Engine 4 world. Unreal takes care of the heavy lifting involved with a real-life simulation. All physics, lighting and world-building are handled by Unreal. AirSim is used to connect Unreal with the operator and ROS bridge. This plugin is added inside the Unreal world and takes control of most of the game logic. It receives the sensor suite and simulates the sensors, it moves the car according to trajectory setpoints and exposes an RPC API for external management. This RPC API is used by the simulator to interact with the world.","title":"The virtual world (Unreal Engine and AirSim)"},{"location":"system-overview/#the-spectator","text":"The spectator provides an eye into the virtual world from an external computer. Through a network connection the virtual world state is replicated and shown on the screen. The user can operate the camera with its mouse and keyboard. Read more about usage of the spectator here .","title":"The Spectator"},{"location":"vehicle_model/","text":"Vehicle Dynamic model One of the most controversial subjects of any competition simulator is the vehicle dynamic model. This is the piece of the simulation that actually changes the state of the vehicle. In building this simulator for the FSOnline competition, our design philosophy was the following: All teams will use the same vehicle dynamic model . We are well aware that all teams have put effort into developing dynamic models of their own FSCar for controls and simulation purposes. However, we want the FSOnline DV Dynamic event to purely be a battle of autonomous software. Even if this will require teams to tweak their path planning and control algorithms, it will make sure that the winner of this event is truly the team that can take any race car and push it to its limits the most. The dynamic model will have a high enough fidelity such that it is virtually impossible to overfit to it/reverse the plant or run open loop . This will force teams to use system identification techniques similar to the ones that are used on a real car and no cheating or unfair advantage will be given to any teams. A third-party, open-source model would be ideal . This way, not even the developers of the simulation (Formula Student Team Delft) would have an edge over other teams. Everyone has access to the same code and has had no experience working with it or been involved in developing it. Unreal Engine provides the PhysXVehicles that was developed by Nvidia. This seemed like the perfect solution for our simulator, given that it complies with all the criteria of our design philosophy above. Airsim simply interacts with the PhysXCar API in these files. Vehicle pawn center Just like everything inside the simulated world, the vehicle has a position within the world (x, y, z). Relative to this point the rest of the vehicle is constructed. The collision box, the center of gravity and the sensor positions all are relative to this center point. The center position is located at the bottom of the car and on the horizontal plane in the geometric center. When car is stationary the height offset between the world track and the pawn position is 0. In below image the blue dot represents the location of the pawn center: Vehicle Layout Overview Vehicle collision model The collision model (bounding box) defines which parts of the car interact with other parts of the world. For cars used in competition (FSOnline) the bounding boxes must use the following specification: Width: 100cm Length: 180cm Height: 50cm The bounding boxes will be centered ontop of the vehicle pawn center. In the Vehicle Layout Overview picture above the orange lines show the bounding box. These boundries are unrealted to the vehicle 3d model. The car may look smaller or bigger but they will hit cones all the same. Vehicle center of gravity The center of gravity of the car sits in the center of the collision model. For cars used in competition (FSOnline), the center of gravity is 25cm above the vehicle pawn center. In turn, when the car is stationary, the height offset between the world ground and the center of gravity is 25 cm. Within the Vehicle Layout Overview picture the red dot represents the center of gravity. More properties of competition vehicles Mass: 255 kg Max speed: ~27 m/s Drag coefficient: 0.3 [-] How to configure vehicle properties? All vehicle proprties are set in the unreal vehicle pawn classes. You can view and edit these configuratoin files using the Unreal Editor in /UE4Project/Plugins/AirSim/Content/VehicleAdv/ .","title":"Vehicle model"},{"location":"vehicle_model/#vehicle-dynamic-model","text":"One of the most controversial subjects of any competition simulator is the vehicle dynamic model. This is the piece of the simulation that actually changes the state of the vehicle. In building this simulator for the FSOnline competition, our design philosophy was the following: All teams will use the same vehicle dynamic model . We are well aware that all teams have put effort into developing dynamic models of their own FSCar for controls and simulation purposes. However, we want the FSOnline DV Dynamic event to purely be a battle of autonomous software. Even if this will require teams to tweak their path planning and control algorithms, it will make sure that the winner of this event is truly the team that can take any race car and push it to its limits the most. The dynamic model will have a high enough fidelity such that it is virtually impossible to overfit to it/reverse the plant or run open loop . This will force teams to use system identification techniques similar to the ones that are used on a real car and no cheating or unfair advantage will be given to any teams. A third-party, open-source model would be ideal . This way, not even the developers of the simulation (Formula Student Team Delft) would have an edge over other teams. Everyone has access to the same code and has had no experience working with it or been involved in developing it. Unreal Engine provides the PhysXVehicles that was developed by Nvidia. This seemed like the perfect solution for our simulator, given that it complies with all the criteria of our design philosophy above. Airsim simply interacts with the PhysXCar API in these files.","title":"Vehicle Dynamic model"},{"location":"vehicle_model/#vehicle-pawn-center","text":"Just like everything inside the simulated world, the vehicle has a position within the world (x, y, z). Relative to this point the rest of the vehicle is constructed. The collision box, the center of gravity and the sensor positions all are relative to this center point. The center position is located at the bottom of the car and on the horizontal plane in the geometric center. When car is stationary the height offset between the world track and the pawn position is 0. In below image the blue dot represents the location of the pawn center: Vehicle Layout Overview","title":"Vehicle pawn center"},{"location":"vehicle_model/#vehicle-collision-model","text":"The collision model (bounding box) defines which parts of the car interact with other parts of the world. For cars used in competition (FSOnline) the bounding boxes must use the following specification: Width: 100cm Length: 180cm Height: 50cm The bounding boxes will be centered ontop of the vehicle pawn center. In the Vehicle Layout Overview picture above the orange lines show the bounding box. These boundries are unrealted to the vehicle 3d model. The car may look smaller or bigger but they will hit cones all the same.","title":"Vehicle collision model"},{"location":"vehicle_model/#vehicle-center-of-gravity","text":"The center of gravity of the car sits in the center of the collision model. For cars used in competition (FSOnline), the center of gravity is 25cm above the vehicle pawn center. In turn, when the car is stationary, the height offset between the world ground and the center of gravity is 25 cm. Within the Vehicle Layout Overview picture the red dot represents the center of gravity.","title":"Vehicle center of gravity"},{"location":"vehicle_model/#more-properties-of-competition-vehicles","text":"Mass: 255 kg Max speed: ~27 m/s Drag coefficient: 0.3 [-]","title":"More properties of competition vehicles"},{"location":"vehicle_model/#how-to-configure-vehicle-properties","text":"All vehicle proprties are set in the unreal vehicle pawn classes. You can view and edit these configuratoin files using the Unreal Editor in /UE4Project/Plugins/AirSim/Content/VehicleAdv/ .","title":"How to configure vehicle properties?"}]}