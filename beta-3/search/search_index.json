{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Formula Student Driverless Simulator Welcome to the FSDS documentation. This home page contains an index with a brief description of the different sections in the documentation. Feel free to read in whatever order preferred. In any case, here are a few suggestions for newcomers. Get familiar with the architecture. The system overview introduces you to the ideas and concepts of the system. Run the simulation. Either follow the simulation guide or build from source by following the development guide Integrate ROS. Build the ros bridge node and launch the ros bridge Participating in FS-Online 2020? Participants of the FS-Online 2020 virtual event should start by reading the system overview and integration handbook . Next, follow the steps in the simulation guide to get your simulation up and running.","title":"Home"},{"location":"#formula-student-driverless-simulator","text":"Welcome to the FSDS documentation. This home page contains an index with a brief description of the different sections in the documentation. Feel free to read in whatever order preferred. In any case, here are a few suggestions for newcomers. Get familiar with the architecture. The system overview introduces you to the ideas and concepts of the system. Run the simulation. Either follow the simulation guide or build from source by following the development guide Integrate ROS. Build the ros bridge node and launch the ros bridge","title":"Formula Student Driverless Simulator"},{"location":"#participating-in-fs-online-2020","text":"Participants of the FS-Online 2020 virtual event should start by reading the system overview and integration handbook . Next, follow the steps in the simulation guide to get your simulation up and running.","title":"Participating in FS-Online 2020?"},{"location":"building-ros/","text":"Building the ROS workspace This guide describes how to use the ROS workspace on an Ubuntu machine. It also works on Windows Subsystem for Linux 1 (WSL1) If you do not have ROS Melodic installed, read the relevant instructions in the get-ready-to-develop guide. Before we can build the workspace, install the workspace c++ library dependencies: sudo apt-get install ros-melodic-tf2-geometry-msgs python-catkin-tools ros-melodic-rqt-multiplot ros-melodic-joy ros-melodic-cv-bridge ros-melodic-image-transport libyaml-cpp-dev libcurl4-openssl-dev Then: Build AirSim shared c++ code. From the root of this repository, run: cd AirSim ./setup.sh ./build.sh Build ROS package cd ros catkin build If your default GCC isn't 8 or greater (check using gcc --version ), then compilation will fail. In that case, use gcc-8 explicitly as follows: catkin build -DCMAKE_C_COMPILER=gcc-8 -DCMAKE_CXX_COMPILER=g++-8","title":"Build your ros workspace"},{"location":"building-ros/#building-the-ros-workspace","text":"This guide describes how to use the ROS workspace on an Ubuntu machine. It also works on Windows Subsystem for Linux 1 (WSL1) If you do not have ROS Melodic installed, read the relevant instructions in the get-ready-to-develop guide. Before we can build the workspace, install the workspace c++ library dependencies: sudo apt-get install ros-melodic-tf2-geometry-msgs python-catkin-tools ros-melodic-rqt-multiplot ros-melodic-joy ros-melodic-cv-bridge ros-melodic-image-transport libyaml-cpp-dev libcurl4-openssl-dev Then: Build AirSim shared c++ code. From the root of this repository, run: cd AirSim ./setup.sh ./build.sh Build ROS package cd ros catkin build If your default GCC isn't 8 or greater (check using gcc --version ), then compilation will fail. In that case, use gcc-8 explicitly as follows: catkin build -DCMAKE_C_COMPILER=gcc-8 -DCMAKE_CXX_COMPILER=g++-8","title":"Building the ROS workspace"},{"location":"gcp-remote-workstation/","text":"How to deploy a remote workstation on Google Cloud Platform In this tutorial we create an gcp instance that is configured for running and developing the Formula-Student-Driverless-Simulator. 1. Create a new instance and configure ports We assume you are familiar with google cloud configurations. Create a new instance in the region of your choice. Minimum requirements are: 12 vCPU 24GB memory Any NVIDIA GPU 150GB Disk Recommended specs are: 16 vCPU 32GB memory 1 NVIDIA Tesla P100 300GB SSD Disk CPU platform Sandy Bridge Do not enable NVIDIA GRID Choose Windows Server 2019 Datacenter Check 'Attach display device' and 'enable NVidia Grid' Ensure this instance has a public ip. 2. Access the remote desktop Go to the cloud instance and use the 'Set Windows password' to set a password for your user. From Ubuntu Install a remote desktop client: sudo apt-get install remmina Launch Remmina, add a new connection. Set Server to the ip of the instance username to the name you entered when setting the password password to the password you created before Color depth to `High color (16 bpp) From Windows Click the little arrow next to the 'RDP' button and click 'Download the RDP file'. Double click on the downloaded file. Use the credentials you just created to login. 3. Disable internet security We need to disable internet security protection because we want to download a bunch of tools. Start the Server Manager. Select Local Server (The server you are currently on and the one that needs IE Enhanced Security disabled) On the right side of the Server Manager, find the IE Enhanced Security Configuration Setting. Disable it. Open Internet Options, go to tab 'Security' set the security level for 'internet' to 'Medium'. Disable 'Protection Mode'. Now you can use internet explorer to downlaod firefox. 4. Install NVIDIA drivers Follow this tutorial to install the required nvidia drivers. Restart your computer. Validate the installation by running the following command in a powershell terminal & 'C:\\Program Files\\NVIDIA Corporation\\NVSMI\\nvidia-smi.exe' 5. Install .NET and Windows Subsystem for Linux Start the Server Manager. 1. Click 'Manage', 'Add Roles and Features' 2. Click Next until you find 'Server Roles'. 3. In 'Server Roles', select 'Remote Desktop Services' 4. In 'Features', select '.NET Framework 3.5' and 'Windows Subsystem for Linux'. 5. Click install. You can ignore the warning about missing source files. Restart the computer","title":"Google cloud remote workstation"},{"location":"gcp-remote-workstation/#how-to-deploy-a-remote-workstation-on-google-cloud-platform","text":"In this tutorial we create an gcp instance that is configured for running and developing the Formula-Student-Driverless-Simulator.","title":"How to deploy a remote workstation on Google Cloud Platform"},{"location":"gcp-remote-workstation/#1-create-a-new-instance-and-configure-ports","text":"We assume you are familiar with google cloud configurations. Create a new instance in the region of your choice. Minimum requirements are: 12 vCPU 24GB memory Any NVIDIA GPU 150GB Disk Recommended specs are: 16 vCPU 32GB memory 1 NVIDIA Tesla P100 300GB SSD Disk CPU platform Sandy Bridge Do not enable NVIDIA GRID Choose Windows Server 2019 Datacenter Check 'Attach display device' and 'enable NVidia Grid' Ensure this instance has a public ip.","title":"1. Create a new instance and configure ports"},{"location":"gcp-remote-workstation/#2-access-the-remote-desktop","text":"Go to the cloud instance and use the 'Set Windows password' to set a password for your user.","title":"2. Access the remote desktop"},{"location":"gcp-remote-workstation/#from-ubuntu","text":"Install a remote desktop client: sudo apt-get install remmina Launch Remmina, add a new connection. Set Server to the ip of the instance username to the name you entered when setting the password password to the password you created before Color depth to `High color (16 bpp)","title":"From Ubuntu"},{"location":"gcp-remote-workstation/#from-windows","text":"Click the little arrow next to the 'RDP' button and click 'Download the RDP file'. Double click on the downloaded file. Use the credentials you just created to login.","title":"From Windows"},{"location":"gcp-remote-workstation/#3-disable-internet-security","text":"We need to disable internet security protection because we want to download a bunch of tools. Start the Server Manager. Select Local Server (The server you are currently on and the one that needs IE Enhanced Security disabled) On the right side of the Server Manager, find the IE Enhanced Security Configuration Setting. Disable it. Open Internet Options, go to tab 'Security' set the security level for 'internet' to 'Medium'. Disable 'Protection Mode'. Now you can use internet explorer to downlaod firefox.","title":"3. Disable internet security"},{"location":"gcp-remote-workstation/#4-install-nvidia-drivers","text":"Follow this tutorial to install the required nvidia drivers. Restart your computer. Validate the installation by running the following command in a powershell terminal & 'C:\\Program Files\\NVIDIA Corporation\\NVSMI\\nvidia-smi.exe'","title":"4. Install NVIDIA drivers"},{"location":"gcp-remote-workstation/#5-install-net-and-windows-subsystem-for-linux","text":"Start the Server Manager. 1. Click 'Manage', 'Add Roles and Features' 2. Click Next until you find 'Server Roles'. 3. In 'Server Roles', select 'Remote Desktop Services' 4. In 'Features', select '.NET Framework 3.5' and 'Windows Subsystem for Linux'. 5. Click install. You can ignore the warning about missing source files. Restart the computer","title":"5. Install .NET and Windows Subsystem for Linux"},{"location":"get-ready-to-develop/","text":"Let's get ready to develop this project So you want to make changes to this project, amazing! We always love new developers \ud83d\udc93 Using this tutorial you can set up your computer to development the simulator. If you just want to run the simulator, you can follow this guide on how to simulate . Prerequisites For developing this project, you need quite a good computer because Unreal Engine is a heavy baby. We highly recommend the following computer specs. You might be able to run with less power but everything will be slower. 8 core 3Ghz CPU 12 GB memory 100GB free SSD storage Recent NVidia card with Vulkan support and 3 GB of memory. If your computer does not suffice you can use a remote workstation on Google Cloud Platform. Read this tutorial on how to setup your virtual workstation. To check the video card drivers, run vulkaninfo . It should output a bunch of lines without errors. You should also enable Windows Subsystem for Linux 1 (WSL1) . No need to update to version 2. Install ubuntu 18.04 LTS. If you are on windows server, enable windows susbsystem for linux in the server manager and install ubuntu . Install Unreal Engine (Windows) Go to unrealengine.com and download the epic installer. You need an account for this. Install the epic installer. Launch the epic installer and install Unreal Engine 4.25 Install visual studio 2019 (Windows) Download visual studio 2019 (community edition) During installation, choose the following components: Desktop development with C++ Game development with C++ Linux development with C++ At 'Invidual Components select: C++ CMake tools for Windows Windows 10 SDK 10.0.18.362.0 .NET Framework 4.7 SDK Install ROS Melodic (WSL Ubuntu) sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list' sudo apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654 curl -sSL 'http://keyserver.ubuntu.com/pks/lookup?op=get&search=0xC1CF6E31E6BADE8868B172B4F42ED6FBAB17C654' | sudo apt-key add - sudo apt update sudo apt install ros-melodic-desktop Add the following line to end of your ~/.bashrc file: source /opt/ros/melodic/setup.bash source ~/Formula-Student-Driverless-Simulator/ros/devel/setup.bash Gui applications from WSL Ubuntu in Windows By default, if you are running Windows Subsystem for Linux with Ubuntu, you can't run gui applications. This is super annoying if you want to use rqt applicatoins like rviz or rqt_plot. It is easy to get this working though! Just install Xming on windows, and run it. Next, go into the Ubuntu terminal and run export DISPLAY=:0 . Now you can run any all them gui apps! You can even add export DISPLAY=:0 to your ~/.bashrc to always be able to use gui apps without having to manually run export. Clone the project In Windows, install git and git lfs . Once both are installed, open git bash and run git lfs install . Now clone the repo in the windows home directory git clone git@github.com:FS-Online/Formula-Student-Driverless-Simulator.git --recurse-submodules Go into the cloned repo and run git config core.fileMode false to ignore file mode changes. THE REPO HAS TO BE CLONED IN THE HOME DIRECTORY! . So the repo location should be $HOME/Formula-Student-Driverless-Simulator . Why you ask? Because we couldn't get relative paths in the C++ code to work so now we have hard-coded some paths to the home directory. I know yes it is ugly but it works. If you are bothered by it I would welcome you to open a pr with a fix. In Ubuntu wsl, create a symlink from ~/Formula-Student-Driverless-Simulator to ~/Formula-Student-Driverless-Simulator ln -s /mnt/c/Users/developer/Formula-Student-Driverless-Simulator ~/Formula-Student-Driverless-Simulator What's next? Read here how to develop!","title":"Setup your development environment"},{"location":"get-ready-to-develop/#lets-get-ready-to-develop-this-project","text":"So you want to make changes to this project, amazing! We always love new developers \ud83d\udc93 Using this tutorial you can set up your computer to development the simulator. If you just want to run the simulator, you can follow this guide on how to simulate .","title":"Let's get ready to develop this project"},{"location":"get-ready-to-develop/#prerequisites","text":"For developing this project, you need quite a good computer because Unreal Engine is a heavy baby. We highly recommend the following computer specs. You might be able to run with less power but everything will be slower. 8 core 3Ghz CPU 12 GB memory 100GB free SSD storage Recent NVidia card with Vulkan support and 3 GB of memory. If your computer does not suffice you can use a remote workstation on Google Cloud Platform. Read this tutorial on how to setup your virtual workstation. To check the video card drivers, run vulkaninfo . It should output a bunch of lines without errors. You should also enable Windows Subsystem for Linux 1 (WSL1) . No need to update to version 2. Install ubuntu 18.04 LTS. If you are on windows server, enable windows susbsystem for linux in the server manager and install ubuntu .","title":"Prerequisites"},{"location":"get-ready-to-develop/#install-unreal-engine-windows","text":"Go to unrealengine.com and download the epic installer. You need an account for this. Install the epic installer. Launch the epic installer and install Unreal Engine 4.25","title":"Install Unreal Engine (Windows)"},{"location":"get-ready-to-develop/#install-visual-studio-2019-windows","text":"Download visual studio 2019 (community edition) During installation, choose the following components: Desktop development with C++ Game development with C++ Linux development with C++ At 'Invidual Components select: C++ CMake tools for Windows Windows 10 SDK 10.0.18.362.0 .NET Framework 4.7 SDK","title":"Install visual studio 2019 (Windows)"},{"location":"get-ready-to-develop/#install-ros-melodic-wsl-ubuntu","text":"sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" > /etc/apt/sources.list.d/ros-latest.list' sudo apt-key adv --keyserver 'hkp://keyserver.ubuntu.com:80' --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654 curl -sSL 'http://keyserver.ubuntu.com/pks/lookup?op=get&search=0xC1CF6E31E6BADE8868B172B4F42ED6FBAB17C654' | sudo apt-key add - sudo apt update sudo apt install ros-melodic-desktop Add the following line to end of your ~/.bashrc file: source /opt/ros/melodic/setup.bash source ~/Formula-Student-Driverless-Simulator/ros/devel/setup.bash","title":"Install ROS Melodic (WSL Ubuntu)"},{"location":"get-ready-to-develop/#gui-applications-from-wsl-ubuntu-in-windows","text":"By default, if you are running Windows Subsystem for Linux with Ubuntu, you can't run gui applications. This is super annoying if you want to use rqt applicatoins like rviz or rqt_plot. It is easy to get this working though! Just install Xming on windows, and run it. Next, go into the Ubuntu terminal and run export DISPLAY=:0 . Now you can run any all them gui apps! You can even add export DISPLAY=:0 to your ~/.bashrc to always be able to use gui apps without having to manually run export.","title":"Gui applications from WSL Ubuntu in Windows"},{"location":"get-ready-to-develop/#clone-the-project","text":"In Windows, install git and git lfs . Once both are installed, open git bash and run git lfs install . Now clone the repo in the windows home directory git clone git@github.com:FS-Online/Formula-Student-Driverless-Simulator.git --recurse-submodules Go into the cloned repo and run git config core.fileMode false to ignore file mode changes. THE REPO HAS TO BE CLONED IN THE HOME DIRECTORY! . So the repo location should be $HOME/Formula-Student-Driverless-Simulator . Why you ask? Because we couldn't get relative paths in the C++ code to work so now we have hard-coded some paths to the home directory. I know yes it is ugly but it works. If you are bothered by it I would welcome you to open a pr with a fix. In Ubuntu wsl, create a symlink from ~/Formula-Student-Driverless-Simulator to ~/Formula-Student-Driverless-Simulator ln -s /mnt/c/Users/developer/Formula-Student-Driverless-Simulator ~/Formula-Student-Driverless-Simulator","title":"Clone the project"},{"location":"get-ready-to-develop/#whats-next","text":"Read here how to develop!","title":"What's next?"},{"location":"gps/","text":"GPS The GPS model in AirSim has been configured to mimic an average GPS receiver used during formula student competitions. The GPS operates at 10 Hz, this value can be altered with update_frequency parameter. At this moment there is no artificial latency added. Still there will be some delay between the creation of the gps points and arrival at the autonomous system because of network latency. In the future, this can be altered with update_frequency parameter. All GPS positions are timestamped. As soon as the car starts, gps becomes available at maximum resolution. There is no warmup time since during physical formula student events cars often start with pre-locked gps. The inaccuracies in the GPS sensor position is generated with adding an offset vector from the ground truth, as a vector [x_err, y_err, z_err]T , where x_err, y_err are generated through a Gaussian distribution with 0 mean and eph variance, and z_err through a Gaussian distribution with 0 mean and epv variance. Currently the eph is set to 4 cm . This noise, however, is only added if the measured velocity is above an arbitrarily chosen threshold (currently 0.1m/s). To velocities below that, this additional inaccuracy is not introduced to avoid \u201cjumpy\u201d positions during standstill of the vehicle. See GpsSimple.hpp (/AirSim/AirLib/include/sensors/gps/GpsSimple.hpp) and GpsSimpleParams.hpp for the implementation of the gps model.","title":"GPS"},{"location":"gps/#gps","text":"The GPS model in AirSim has been configured to mimic an average GPS receiver used during formula student competitions. The GPS operates at 10 Hz, this value can be altered with update_frequency parameter. At this moment there is no artificial latency added. Still there will be some delay between the creation of the gps points and arrival at the autonomous system because of network latency. In the future, this can be altered with update_frequency parameter. All GPS positions are timestamped. As soon as the car starts, gps becomes available at maximum resolution. There is no warmup time since during physical formula student events cars often start with pre-locked gps. The inaccuracies in the GPS sensor position is generated with adding an offset vector from the ground truth, as a vector [x_err, y_err, z_err]T , where x_err, y_err are generated through a Gaussian distribution with 0 mean and eph variance, and z_err through a Gaussian distribution with 0 mean and epv variance. Currently the eph is set to 4 cm . This noise, however, is only added if the measured velocity is above an arbitrarily chosen threshold (currently 0.1m/s). To velocities below that, this additional inaccuracy is not introduced to avoid \u201cjumpy\u201d positions during standstill of the vehicle. See GpsSimple.hpp (/AirSim/AirLib/include/sensors/gps/GpsSimple.hpp) and GpsSimpleParams.hpp for the implementation of the gps model.","title":"GPS"},{"location":"how-to-develop/","text":"How to develop this project Just looking to run the simulation and not interested in development of the simulator? Get started faster with the simulation guide . This guide describes how to work on this project. Before you continue reading, make sure you have finished the get-ready-to-develop guide. As you can read in the system overview this system consists of multiple components. To develop any of these components you have to run Unreal Engine with the AirSim plugin. So we need to compile the AirSim plugin. Compile the AirSim plugin and launch Unreal Engine The AirSim plugin sourcode is made up of AirLib (/AirSim/AirLib) and the plugin code (/UE4Project/Plugins/AirSim/Source). AirLib is also used by the ROS bridge. First build the AirLib code. Open the Developer Command Prompt for VS 2019 , go to Formula-Student-Driverless-Simulator/AirSim and run build.cmd The first time this takes quite a while. Go walk around a bit, maybe start playing factoryidle . After it is finished, launch unreal engine and open the project file Formula-Student-Driverless-Simulator/UE4Project/FSOnline.uproject It might show an error like 'This project was made with a different version of the Unreal Engine'. In that case select more options and skip conversion . When asked to rebuild the 'Blocks' and 'AirSim' modules, choose 'Yes'. This is the step where the plugin part of AirSim is compiled. After some time Unreal Engine will start and you can launch the game. If you make changes to the plugin code you only have to recompile the plugin. This can be done from within the Unreal Editor. go to to Window -> Developer tools -> Modules . Search for AirSim and click Recompile . If you make changes to AirLib you have to run build.cmd again. ROS development To set up the ROS workspace, cd into the ros folder and run catkin init catkin build Now you can run the ros bridge . Export the Unreal Engine project for release Open the UE4Project in the Unreal Editor Ensure 'File' -> 'Package Project' -> 'Build configuration' is set to 'Shipping, Choose 'File' -> 'Package Project' -> 'Windows (64 bit)' Select any folder on your computer. Wait until it finishes. Go into the WindowsNoEditor folder and rename Blocks.exe to FSDS.exe Zip all files and upload to github release! Deploying documentation For documentation we use mkdocs and mike . Hosting is provided by github pages. To tag a new version of the documentation and release it to github, first checkout the version that you want to deploy. Then run mike deploy VERSION latest -u -p . This will compile the documentation, store it as a new version in the gh-pages branch, update teh latest alias to point at the new version and push the gh-pages branch to github and thus making the documentation public. To create a new version without updating the latest tag, omit the latest -u part.","title":"How to develop the FSDS project"},{"location":"how-to-develop/#how-to-develop-this-project","text":"Just looking to run the simulation and not interested in development of the simulator? Get started faster with the simulation guide . This guide describes how to work on this project. Before you continue reading, make sure you have finished the get-ready-to-develop guide. As you can read in the system overview this system consists of multiple components. To develop any of these components you have to run Unreal Engine with the AirSim plugin. So we need to compile the AirSim plugin.","title":"How to develop this project"},{"location":"how-to-develop/#compile-the-airsim-plugin-and-launch-unreal-engine","text":"The AirSim plugin sourcode is made up of AirLib (/AirSim/AirLib) and the plugin code (/UE4Project/Plugins/AirSim/Source). AirLib is also used by the ROS bridge. First build the AirLib code. Open the Developer Command Prompt for VS 2019 , go to Formula-Student-Driverless-Simulator/AirSim and run build.cmd The first time this takes quite a while. Go walk around a bit, maybe start playing factoryidle . After it is finished, launch unreal engine and open the project file Formula-Student-Driverless-Simulator/UE4Project/FSOnline.uproject It might show an error like 'This project was made with a different version of the Unreal Engine'. In that case select more options and skip conversion . When asked to rebuild the 'Blocks' and 'AirSim' modules, choose 'Yes'. This is the step where the plugin part of AirSim is compiled. After some time Unreal Engine will start and you can launch the game. If you make changes to the plugin code you only have to recompile the plugin. This can be done from within the Unreal Editor. go to to Window -> Developer tools -> Modules . Search for AirSim and click Recompile . If you make changes to AirLib you have to run build.cmd again.","title":"Compile the AirSim plugin and launch Unreal Engine"},{"location":"how-to-develop/#ros-development","text":"To set up the ROS workspace, cd into the ros folder and run catkin init catkin build Now you can run the ros bridge .","title":"ROS development"},{"location":"how-to-develop/#export-the-unreal-engine-project-for-release","text":"Open the UE4Project in the Unreal Editor Ensure 'File' -> 'Package Project' -> 'Build configuration' is set to 'Shipping, Choose 'File' -> 'Package Project' -> 'Windows (64 bit)' Select any folder on your computer. Wait until it finishes. Go into the WindowsNoEditor folder and rename Blocks.exe to FSDS.exe Zip all files and upload to github release!","title":"Export the Unreal Engine project for release"},{"location":"how-to-develop/#deploying-documentation","text":"For documentation we use mkdocs and mike . Hosting is provided by github pages. To tag a new version of the documentation and release it to github, first checkout the version that you want to deploy. Then run mike deploy VERSION latest -u -p . This will compile the documentation, store it as a new version in the gh-pages branch, update teh latest alias to point at the new version and push the gh-pages branch to github and thus making the documentation public. To create a new version without updating the latest tag, omit the latest -u part.","title":"Deploying documentation"},{"location":"how-to-simulate/","text":"Let's get ready to run the simulation Interested in building the project from source and contributing to the simulator? Check out the development guide To run the simulation smoothly you need quite a fast windows computer with a modern videocard. Minimally you will need: 8 core 2.3Ghz CPU 12 GB memory 30GB free SSD storage Recent NVidia card with Vulkan support and 3 GB of memory. To check the video card drivers, run vulkaninfo . It should output a bunch of lines without errors. Launching the simulator To launch the simulator, go to releases and download the latest one. Before you start, you have to create the folder and file Formula-Student-Driverless-Simulator/settings.json in your home directory . This file contains the sensor configuration of the car. Copy-paste the contents of the settings.json file at the root of this repository inside. This should get you started with the default sensor configuration, feel free to try your own custom sensor suite. Note that the naming of the sensors will be reflected in the topic names as elaborated here . Now launch FSDS.exe and a window with a car should popup! Try driving the car around using the arrowkeys. Launching the ROS bridge To connect your autonomous ROS system, you have to clone this repository and run the fsds_ros_bridge ROS node. Most likely, your autonomous system is running on Ubuntu and has already ROS Melodic installed. If this is not the case, read the relevant install instructions in the get-ready-to-develop guide that will help you get the requried software installed. Ready? Lets clone the repo into your home directory : git clone git@github.com:FS-Online/Formula-Student-Driverless-Simulator.git --recurse-submodules The repository will be placed in ~/Formula-Student-Driverless-Simulator . If this folder already exists as a result of the previous step, you can get your settings.json out, delete the folder (or rename it to something like UEProject or UESim) and after cloning the repo place the settings.json file back (this time inside the root of the repository where the default settings.json lives). Read about how to build the ROS workspace here . Then, launch the ros bridge .","title":"How to simulate"},{"location":"how-to-simulate/#lets-get-ready-to-run-the-simulation","text":"Interested in building the project from source and contributing to the simulator? Check out the development guide To run the simulation smoothly you need quite a fast windows computer with a modern videocard. Minimally you will need: 8 core 2.3Ghz CPU 12 GB memory 30GB free SSD storage Recent NVidia card with Vulkan support and 3 GB of memory. To check the video card drivers, run vulkaninfo . It should output a bunch of lines without errors.","title":"Let's get ready to run the simulation"},{"location":"how-to-simulate/#launching-the-simulator","text":"To launch the simulator, go to releases and download the latest one. Before you start, you have to create the folder and file Formula-Student-Driverless-Simulator/settings.json in your home directory . This file contains the sensor configuration of the car. Copy-paste the contents of the settings.json file at the root of this repository inside. This should get you started with the default sensor configuration, feel free to try your own custom sensor suite. Note that the naming of the sensors will be reflected in the topic names as elaborated here . Now launch FSDS.exe and a window with a car should popup! Try driving the car around using the arrowkeys.","title":"Launching the simulator"},{"location":"how-to-simulate/#launching-the-ros-bridge","text":"To connect your autonomous ROS system, you have to clone this repository and run the fsds_ros_bridge ROS node. Most likely, your autonomous system is running on Ubuntu and has already ROS Melodic installed. If this is not the case, read the relevant install instructions in the get-ready-to-develop guide that will help you get the requried software installed. Ready? Lets clone the repo into your home directory : git clone git@github.com:FS-Online/Formula-Student-Driverless-Simulator.git --recurse-submodules The repository will be placed in ~/Formula-Student-Driverless-Simulator . If this folder already exists as a result of the previous step, you can get your settings.json out, delete the folder (or rename it to something like UEProject or UESim) and after cloning the repo place the settings.json file back (this time inside the root of the repository where the default settings.json lives). Read about how to build the ROS workspace here . Then, launch the ros bridge .","title":"Launching the ROS bridge"},{"location":"imu/","text":"IMU Sensor The IMU sensor in FSDS is using AirSim's built in IMU sensor simulation, that has been modelled and parametrized according to MPU 6000 IMU from InvenSense The maximum achievable internal IMU frequency is 1000 Hz, ouputing information of the vehicle's 3-axis angular velocity, 3-axis linear acceleration, as well as its orientation in quaternions. IMU gyroscope and accelomter bias and accuracy parameters can be found and fine-tuned in AirLib/include/sensors/imu/ImuSimpleParams.hpp, or can be initialized with custom parameters in your settings.json file. The angular velocity, linear acceleration outputs as well as their biases have artificially introduced Gaussian noise (0 mean, standard deviation of 1) updated on each IMU cycle. All of the IMU measurements are timestamped. See ImuSimple.hpp (/AirSim/AirLib/include/sensors/imu/ImuSimple.hpp) and [ImuSimpleParams.hpp] /AirSim/AirLib/include/sensors/imu/ImuSimpleParams.hpp for the implementation of the IMU model.","title":"IMU"},{"location":"imu/#imu-sensor","text":"The IMU sensor in FSDS is using AirSim's built in IMU sensor simulation, that has been modelled and parametrized according to MPU 6000 IMU from InvenSense The maximum achievable internal IMU frequency is 1000 Hz, ouputing information of the vehicle's 3-axis angular velocity, 3-axis linear acceleration, as well as its orientation in quaternions. IMU gyroscope and accelomter bias and accuracy parameters can be found and fine-tuned in AirLib/include/sensors/imu/ImuSimpleParams.hpp, or can be initialized with custom parameters in your settings.json file. The angular velocity, linear acceleration outputs as well as their biases have artificially introduced Gaussian noise (0 mean, standard deviation of 1) updated on each IMU cycle. All of the IMU measurements are timestamped. See ImuSimple.hpp (/AirSim/AirLib/include/sensors/imu/ImuSimple.hpp) and [ImuSimpleParams.hpp] /AirSim/AirLib/include/sensors/imu/ImuSimpleParams.hpp for the implementation of the IMU model.","title":"IMU Sensor"},{"location":"integration-handbook/","text":"Autonomous System Integration Handbook This page describes how to integrate your autonomous system (AS) to the Formula Student Driverless Simulator (FSDS). The rules and procedures set out in this document will be used during the FSOnline competition. High-level overview Your AS is expected to continuously run a ROS master. The simulator will launch a node ( fsds_ros_bridge ) that connects to your ROS system. This node will publish sensor data and listen for vehicle setpoints. When the simulation is done, the node is closed and your AS will no longer be able to interact with the simulator. A more in-depth explanation of the FSDS system can be found here. Integrating your autonomous system with a simulator is a matter of subscribing and publishing to topics and acting accordingly. During testing, your AS and the ROS bridge/simulator will probably run on the same machine (your local computer most likely). However, during the competition, the AS and the ROS bridge/simulator will be on different machines (cloud servers) and communicate over a local network. System flow and Signals Initially, when your AS launches, no simulation is connected. The AS must wait for a GO signal before acting. Staging At some point, your AS will be staged: The vehicle is placed at a staging line prior to the starting line, the fsds_ros_bridge node will connect to the ROS system. From this point onwards, the AS will receive sensor data and can control the vehicle by publishing vehicle setpoints. However, it shouldn't start driving the vehicle just yet! Starting Just like with a physical FS event, the vehicle must only start driving after a GO signal is received. The GO signal indicates that the AS must start the mission and that the vehicle should start driving. This signal is also known as the 'green flag' signal or 'starting' signal. Within this repo, we will reference it as 'GO'. Within the GO signal, the mission and track are added. Currently, autocross and trackdrive are the supported missions. In autocross, the vehicle has to complete a single lap on an unknown track. On the trackdrive, the vehicle has to finish 10 laps on a track it has previously seen. During the competition, on each track, every AS will do an autocross mission before a trackdrive mission. It can occur that multiple autocross runs on different tracks take place before going to trackdrive. It can also happen that multiple autocross runs take place on the same track. For example, the AS might be requested to do: autocross on track A autocross on track B trackdrive on track A autocross on track C autocross on track C (re-run) trackdrive on track C The AS must implement the following behaviour: When the AS is requested to do autocross on a track that it has seen before, it must delete any and all data it gathered during all previous runs on this track. When the AS is requested to do trackdrive on a track where it has done a trackdrive previously, it must delete any and all data it gathered during all previous trackdrive runs on this track. However, the data gathered during the last autocross on this track shouldn't be deleted. An exception to this rule is data recorded with the exclusive intent to analyze the AS's behaviour after the event. This includes all files that the AS only writes to but does not read from. To make the AS aware of which track it is driving, the GO signal includes a unique identifier of the upcoming track. After the initial GO signal, the signal is continuously re-sent at 1 Hz to ensure it arrives at the team's AS. The timestamp of all consecutive GO signals is equal to the first one. Finishing There are two ways to conclude a run: finishing or stopping. When the autonomous system feels it concluded it's run, it should send a FINISHED signal. The FINISHED signal tells the simulator that the AS no longer wants to control the vehicle. As such, the simulator will stop the fsds_ros_bridge and the AS will no longer receive sensor data or be able to control the vehicle. When the official decides that the run is over it will stop the simulation. See the rulebook for a description of when the official does so. When the simulation is stopped the fsds_ros_bridge is stopped immediately and the AS will no longer receive sensor data or be able to control the vehicle. The AS will not receive a signal that this happened. To detect a stop, the AS should keep an eye on the GO signal. The general rule is: If the AS did not receive a GO signal for 4 seconds the AS can assume the fsds_ros_bridge is stopped. When this state is detected, the AS can reset itself and prepare for the next mission. Sensor Suite Every team can configure the sensors on their vehicle. This configuration is called the sensor suite. To ensure the simulation will perform as expected, the sensor suite has some restrictions. Here you can read the requirements and restrictions that apply to every sensor. Camera A this moment camera's are a bit broken. You can use the camera's but the framerate and topic names might change. See #43 and #85. Every vehicle can have a maximum of 2 camera sensors. These camera(s) can be placed anywhere on the vehicle that would be allowed by FSG 2020 rules. The camera body dimensions are a 4x4x4 cm cube with mounting points at any side except the front-facing side. All camera sensors output uncompressed RGBA8 images at 30 FPS. You can choose the resolution of the camera(s). In total, the camera\u2019s can have 1232450 pixels. Every dimension (width or height) must be at least 240px and no greater than 1600px. The horizontal field of view (FoV) is configurable for each camera and must be at least 30 degrees and not be greater than 90 degrees. The vertical FoV will be automatically calculated using the following formula: vertical FoV = image height / image width * horizontal FoV . The camera's auto exposure, motion blur and gamma settings will be equal for all teams. Lidar A vehicle can have between 0 and 5 lidars. The lidar(s) can be placed anywhere on the vehicle that would be allowed by FSG 2020 rules. The body dimension of every lidar is a vertical cylinder, 8 cm height and 8 cm diameter with mounting points at the top and bottom. A single lidar can have between 1 and 500 lasers. The lasers are stacked vertically and rotate on the horizontal plane. The lasers are distributed equally to cover the specified vertical field of view. The vertical field of view is specified by choosing the upper and lower limit in degrees. The lower limit specifies the vertical angle between the horizontal plane of the lidar and the most bottom laser. The upper limit specifies the vertical angle between the horizontal plane of the lidar and most upper laser. The horizontal field of view of the lidar is specified with an upper and lower limit in degree as well. Only points within this FOV will be returned. The lower limit specifies the counterclockwise angle on a top view from the direction the lidar is pointing towards. The upper limit specifies the clockwise angle on a top view from the direction the lidar is pointing towards. For every lidar, the rotation speed (Hz) and capture frequency (Hz) must be chosen. The rotation speed specifies how fast the lasers spin and the capture frequency specifies how often a point cloud is created. While rotating, only lasers within the horizontal field of view are captured. For example, a lidar with 190 degrees horizontal field of view, rotating at 10hz with a capture frequency of 20 Hz will receive point clouds covering anything from 10 to 180 degrees of the field of view. There is no guarantee that the rotation speed and capture frequency stay in sync. You won\u2019t be able to rely on synchronization of rotation speed and capture frequency. A lidar rotating at 5 Hz would theoretically have rotated 50 times after 10 seconds but in reality, this will be somewhere between 45 and 55 times. For every lidar, you can specify the resolution: the total number of points collected if the lasers would do a 360 field of view sweep scan. This value is used to calculate the number of points in each laser and the spacing between the points. Every lidar capture is limited to collecting 10000 points. The maximum number of points collected during a capture is calculated by dividing the lidar\u2019s resolution by the horizontal field of view fraction. For example, a lidar with a 30 degrees horizontal field (horizontal FOV fraction of 30/360 = 1/12) can have a maximum resolution of 120000. The total number of points per second is limited to 100000 points. For example, a first lidar collects 10000 points per capture at 5 hz, a second lidar collects 8000 points per capture at 5 hz. This is valid because in total they collect 90000 points per second. GPS Every vehicle has 1 GPS, it is located at the centre of gravity of the vehicle. This sensor cannot be removed or moved. The GPS captures the position of the vehicle in the geodetic reference system, namely longitude [deg], latitude [deg], altitude [m]. More detailed technical information about the accuracy of the GPS can be found here . IMU Every vehicle has 1 IMU, it is located at the centre of gravity of the vehicle. This sensor cannot be removed or moved. The IMU captures the acceleration, orientation and angular rate of the vehicle. More detailed technical information about the IMU implementation of the IMU can be found here . Sensor specification Teams are expected to provide their sensor suite as a single AirSim settings.json file. Most of the parameters in the settings.json file will be set by the officials to ensure fairness during competition. You are allowed to configure the following subset of parameters within the boundaries of the above rules. Cameras camera name Width, Height (pixels) FOV_Degrees (degrees) X, Y, Z (meters) Pitch, Roll, Yaw (degrees) Lidars NumberOfChannels PointsPerSecond RotationsPerSecond HorizontalFOVStart (degrees) HorizontalFOVEnd (degrees) VerticalFOVUpper (degrees) VerticalFOVLower(degrees) X, Y, Z (meters) Pitch, Roll, Yaw (degrees) The GPS and IMU are configured equally for all teams according to the rules in the previous chapter. X, Y, Z, Pitch, Roll, Yaw have to be specified in a NED frame (it might take some experimentation to understand the correct signs to use). The transforms you will get from ROS however, will be in the ENU frame (which is the default ros coordinate system). Distance values are in meters and rotation values are degrees. We recommend to copy the settings.json in this repository as a base and configure the cameras and lidar from thereon. Launching the simulator To run the simulation, read the simulation guide . ROS integration Communication between the autonomous system and simulator will take place using ROS topics. Sensor data will be published by the ros bridge and received by the autonomous system. The autonomous system will publish vehicle setpoints and the ROS bridge will listen for those messages. Static transforms between sensors also are being published for usage by the autonomous system. ROS msgs The ROS bridge of this simulator had to make use of several custom msgs (for control commands, the groundtruth track, etc). These messages are defined in a ROS package called fs_msgs which is located in a separate, light repository . To implement publishers and subscibers for these messages types in your autonomous pipeline, you will have to add the fs_msgs repository as a submodule in your codebase (inside de src directory of an existing catkin workspace as done in this repository) or clone it and build it somewhere else in your system. Topics The AS can subscribe to the following sensor topics: /fsds/gps /fsds/imu /fsds/camera/CAMERA_NAME /fsds/camera/CAMERA_NAME/camera_info /fsds/lidar/LIDAR_NAME During testing , the following ground truth topics will also be available: /fsds/testing_only/odom /fsds/testing_only/track These two topics should allow you to run autonomously without a finished perception and state estimation pipeline. The AS will receive the GO signal on the following topic: /fsds/signal/go And it AS can publish the FINISHED on this topic: /fsds/signal/finished The AS must publish vehicle control commands on this topic: /fsds/control_command Read more about the techincal detalis of these topics in the ros-bridge documentation Vehicle dynamic model The vehicle dynamic model is a third-party high-fodelity model and will be the same for all teams. More details and information on this choice can be found here . 3D vehicle model //todo This chapter will describe how to change the 3d model of the vehicle and how to provide the 3d model for usage during the competition. At this moment we have no idea how this works sooooo when we figure it out this will be filled in. Competition deployment A few weeks before the competition, each team will receive the ssh credentials to an Ubuntu google cloud instance. This instance will have 8 vCPU cores, 30 GB memory (configuration n1-standard-8), 1 Nvidia Tesla T4 video card and 100GB SSD disk. The teams must install their autonomous system on this computer. During the competition, a separate google cloud instance will run the simulation software and the ROS bridge. One by one the ROS bridge will connect to the different teams' computers and they will do their mission. During the weeks leading up to the competition, FSOnline will host multiple testing moments where the autonomous computers will be connected to the simulator and drive a few test laps. More information about the procedure will be added later.","title":"Integration handbook"},{"location":"integration-handbook/#autonomous-system-integration-handbook","text":"This page describes how to integrate your autonomous system (AS) to the Formula Student Driverless Simulator (FSDS). The rules and procedures set out in this document will be used during the FSOnline competition.","title":"Autonomous System Integration Handbook"},{"location":"integration-handbook/#high-level-overview","text":"Your AS is expected to continuously run a ROS master. The simulator will launch a node ( fsds_ros_bridge ) that connects to your ROS system. This node will publish sensor data and listen for vehicle setpoints. When the simulation is done, the node is closed and your AS will no longer be able to interact with the simulator. A more in-depth explanation of the FSDS system can be found here. Integrating your autonomous system with a simulator is a matter of subscribing and publishing to topics and acting accordingly. During testing, your AS and the ROS bridge/simulator will probably run on the same machine (your local computer most likely). However, during the competition, the AS and the ROS bridge/simulator will be on different machines (cloud servers) and communicate over a local network.","title":"High-level overview"},{"location":"integration-handbook/#system-flow-and-signals","text":"Initially, when your AS launches, no simulation is connected. The AS must wait for a GO signal before acting.","title":"System flow and Signals"},{"location":"integration-handbook/#staging","text":"At some point, your AS will be staged: The vehicle is placed at a staging line prior to the starting line, the fsds_ros_bridge node will connect to the ROS system. From this point onwards, the AS will receive sensor data and can control the vehicle by publishing vehicle setpoints. However, it shouldn't start driving the vehicle just yet!","title":"Staging"},{"location":"integration-handbook/#starting","text":"Just like with a physical FS event, the vehicle must only start driving after a GO signal is received. The GO signal indicates that the AS must start the mission and that the vehicle should start driving. This signal is also known as the 'green flag' signal or 'starting' signal. Within this repo, we will reference it as 'GO'. Within the GO signal, the mission and track are added. Currently, autocross and trackdrive are the supported missions. In autocross, the vehicle has to complete a single lap on an unknown track. On the trackdrive, the vehicle has to finish 10 laps on a track it has previously seen. During the competition, on each track, every AS will do an autocross mission before a trackdrive mission. It can occur that multiple autocross runs on different tracks take place before going to trackdrive. It can also happen that multiple autocross runs take place on the same track. For example, the AS might be requested to do: autocross on track A autocross on track B trackdrive on track A autocross on track C autocross on track C (re-run) trackdrive on track C The AS must implement the following behaviour: When the AS is requested to do autocross on a track that it has seen before, it must delete any and all data it gathered during all previous runs on this track. When the AS is requested to do trackdrive on a track where it has done a trackdrive previously, it must delete any and all data it gathered during all previous trackdrive runs on this track. However, the data gathered during the last autocross on this track shouldn't be deleted. An exception to this rule is data recorded with the exclusive intent to analyze the AS's behaviour after the event. This includes all files that the AS only writes to but does not read from. To make the AS aware of which track it is driving, the GO signal includes a unique identifier of the upcoming track. After the initial GO signal, the signal is continuously re-sent at 1 Hz to ensure it arrives at the team's AS. The timestamp of all consecutive GO signals is equal to the first one.","title":"Starting"},{"location":"integration-handbook/#finishing","text":"There are two ways to conclude a run: finishing or stopping. When the autonomous system feels it concluded it's run, it should send a FINISHED signal. The FINISHED signal tells the simulator that the AS no longer wants to control the vehicle. As such, the simulator will stop the fsds_ros_bridge and the AS will no longer receive sensor data or be able to control the vehicle. When the official decides that the run is over it will stop the simulation. See the rulebook for a description of when the official does so. When the simulation is stopped the fsds_ros_bridge is stopped immediately and the AS will no longer receive sensor data or be able to control the vehicle. The AS will not receive a signal that this happened. To detect a stop, the AS should keep an eye on the GO signal. The general rule is: If the AS did not receive a GO signal for 4 seconds the AS can assume the fsds_ros_bridge is stopped. When this state is detected, the AS can reset itself and prepare for the next mission.","title":"Finishing"},{"location":"integration-handbook/#sensor-suite","text":"Every team can configure the sensors on their vehicle. This configuration is called the sensor suite. To ensure the simulation will perform as expected, the sensor suite has some restrictions. Here you can read the requirements and restrictions that apply to every sensor.","title":"Sensor Suite"},{"location":"integration-handbook/#camera","text":"A this moment camera's are a bit broken. You can use the camera's but the framerate and topic names might change. See #43 and #85. Every vehicle can have a maximum of 2 camera sensors. These camera(s) can be placed anywhere on the vehicle that would be allowed by FSG 2020 rules. The camera body dimensions are a 4x4x4 cm cube with mounting points at any side except the front-facing side. All camera sensors output uncompressed RGBA8 images at 30 FPS. You can choose the resolution of the camera(s). In total, the camera\u2019s can have 1232450 pixels. Every dimension (width or height) must be at least 240px and no greater than 1600px. The horizontal field of view (FoV) is configurable for each camera and must be at least 30 degrees and not be greater than 90 degrees. The vertical FoV will be automatically calculated using the following formula: vertical FoV = image height / image width * horizontal FoV . The camera's auto exposure, motion blur and gamma settings will be equal for all teams.","title":"Camera"},{"location":"integration-handbook/#lidar","text":"A vehicle can have between 0 and 5 lidars. The lidar(s) can be placed anywhere on the vehicle that would be allowed by FSG 2020 rules. The body dimension of every lidar is a vertical cylinder, 8 cm height and 8 cm diameter with mounting points at the top and bottom. A single lidar can have between 1 and 500 lasers. The lasers are stacked vertically and rotate on the horizontal plane. The lasers are distributed equally to cover the specified vertical field of view. The vertical field of view is specified by choosing the upper and lower limit in degrees. The lower limit specifies the vertical angle between the horizontal plane of the lidar and the most bottom laser. The upper limit specifies the vertical angle between the horizontal plane of the lidar and most upper laser. The horizontal field of view of the lidar is specified with an upper and lower limit in degree as well. Only points within this FOV will be returned. The lower limit specifies the counterclockwise angle on a top view from the direction the lidar is pointing towards. The upper limit specifies the clockwise angle on a top view from the direction the lidar is pointing towards. For every lidar, the rotation speed (Hz) and capture frequency (Hz) must be chosen. The rotation speed specifies how fast the lasers spin and the capture frequency specifies how often a point cloud is created. While rotating, only lasers within the horizontal field of view are captured. For example, a lidar with 190 degrees horizontal field of view, rotating at 10hz with a capture frequency of 20 Hz will receive point clouds covering anything from 10 to 180 degrees of the field of view. There is no guarantee that the rotation speed and capture frequency stay in sync. You won\u2019t be able to rely on synchronization of rotation speed and capture frequency. A lidar rotating at 5 Hz would theoretically have rotated 50 times after 10 seconds but in reality, this will be somewhere between 45 and 55 times. For every lidar, you can specify the resolution: the total number of points collected if the lasers would do a 360 field of view sweep scan. This value is used to calculate the number of points in each laser and the spacing between the points. Every lidar capture is limited to collecting 10000 points. The maximum number of points collected during a capture is calculated by dividing the lidar\u2019s resolution by the horizontal field of view fraction. For example, a lidar with a 30 degrees horizontal field (horizontal FOV fraction of 30/360 = 1/12) can have a maximum resolution of 120000. The total number of points per second is limited to 100000 points. For example, a first lidar collects 10000 points per capture at 5 hz, a second lidar collects 8000 points per capture at 5 hz. This is valid because in total they collect 90000 points per second.","title":"Lidar"},{"location":"integration-handbook/#gps","text":"Every vehicle has 1 GPS, it is located at the centre of gravity of the vehicle. This sensor cannot be removed or moved. The GPS captures the position of the vehicle in the geodetic reference system, namely longitude [deg], latitude [deg], altitude [m]. More detailed technical information about the accuracy of the GPS can be found here .","title":"GPS"},{"location":"integration-handbook/#imu","text":"Every vehicle has 1 IMU, it is located at the centre of gravity of the vehicle. This sensor cannot be removed or moved. The IMU captures the acceleration, orientation and angular rate of the vehicle. More detailed technical information about the IMU implementation of the IMU can be found here .","title":"IMU"},{"location":"integration-handbook/#sensor-specification","text":"Teams are expected to provide their sensor suite as a single AirSim settings.json file. Most of the parameters in the settings.json file will be set by the officials to ensure fairness during competition. You are allowed to configure the following subset of parameters within the boundaries of the above rules. Cameras camera name Width, Height (pixels) FOV_Degrees (degrees) X, Y, Z (meters) Pitch, Roll, Yaw (degrees) Lidars NumberOfChannels PointsPerSecond RotationsPerSecond HorizontalFOVStart (degrees) HorizontalFOVEnd (degrees) VerticalFOVUpper (degrees) VerticalFOVLower(degrees) X, Y, Z (meters) Pitch, Roll, Yaw (degrees) The GPS and IMU are configured equally for all teams according to the rules in the previous chapter. X, Y, Z, Pitch, Roll, Yaw have to be specified in a NED frame (it might take some experimentation to understand the correct signs to use). The transforms you will get from ROS however, will be in the ENU frame (which is the default ros coordinate system). Distance values are in meters and rotation values are degrees. We recommend to copy the settings.json in this repository as a base and configure the cameras and lidar from thereon.","title":"Sensor specification"},{"location":"integration-handbook/#launching-the-simulator","text":"To run the simulation, read the simulation guide .","title":"Launching the simulator"},{"location":"integration-handbook/#ros-integration","text":"Communication between the autonomous system and simulator will take place using ROS topics. Sensor data will be published by the ros bridge and received by the autonomous system. The autonomous system will publish vehicle setpoints and the ROS bridge will listen for those messages. Static transforms between sensors also are being published for usage by the autonomous system.","title":"ROS integration"},{"location":"integration-handbook/#ros-msgs","text":"The ROS bridge of this simulator had to make use of several custom msgs (for control commands, the groundtruth track, etc). These messages are defined in a ROS package called fs_msgs which is located in a separate, light repository . To implement publishers and subscibers for these messages types in your autonomous pipeline, you will have to add the fs_msgs repository as a submodule in your codebase (inside de src directory of an existing catkin workspace as done in this repository) or clone it and build it somewhere else in your system.","title":"ROS msgs"},{"location":"integration-handbook/#topics","text":"The AS can subscribe to the following sensor topics: /fsds/gps /fsds/imu /fsds/camera/CAMERA_NAME /fsds/camera/CAMERA_NAME/camera_info /fsds/lidar/LIDAR_NAME During testing , the following ground truth topics will also be available: /fsds/testing_only/odom /fsds/testing_only/track These two topics should allow you to run autonomously without a finished perception and state estimation pipeline. The AS will receive the GO signal on the following topic: /fsds/signal/go And it AS can publish the FINISHED on this topic: /fsds/signal/finished The AS must publish vehicle control commands on this topic: /fsds/control_command Read more about the techincal detalis of these topics in the ros-bridge documentation","title":"Topics"},{"location":"integration-handbook/#vehicle-dynamic-model","text":"The vehicle dynamic model is a third-party high-fodelity model and will be the same for all teams. More details and information on this choice can be found here .","title":"Vehicle dynamic model"},{"location":"integration-handbook/#3d-vehicle-model","text":"//todo This chapter will describe how to change the 3d model of the vehicle and how to provide the 3d model for usage during the competition. At this moment we have no idea how this works sooooo when we figure it out this will be filled in.","title":"3D vehicle model"},{"location":"integration-handbook/#competition-deployment","text":"A few weeks before the competition, each team will receive the ssh credentials to an Ubuntu google cloud instance. This instance will have 8 vCPU cores, 30 GB memory (configuration n1-standard-8), 1 Nvidia Tesla T4 video card and 100GB SSD disk. The teams must install their autonomous system on this computer. During the competition, a separate google cloud instance will run the simulation software and the ROS bridge. One by one the ROS bridge will connect to the different teams' computers and they will do their mission. During the weeks leading up to the competition, FSOnline will host multiple testing moments where the autonomous computers will be connected to the simulator and drive a few test laps. More information about the procedure will be added later.","title":"Competition deployment"},{"location":"joystick/","text":"Joystick Controller Node If you are interested in driving the car around to gather training data without having to rely on your autonomous system, you can use an XBox controller to do so. Make sure you have built the ROS workspace . Simply run: cd ros source devel/setup.bash roslaunch joystick joystick.launch This node gets input from a joystick xbox controller (see http://wiki.ros.org/joy) and sends the values to the lowlevel controls (hardware) on topic /fsds_ros_bridge/FSCar/control_command The right trigger (RT) controls the acceleration (gas) The left trigger (LT) controls brake (negative acceleration). The x-axis of the left stick controlls the steering angle. Pressing and holding the B button enables boost mode. When the joy driver initializes it sometimes sends very high values. This would cause the car to move unpredictably. Therefore the controller is 'locked' when it is starts and when the controller is re-plugged in. To unlock both triggers must be fully pressed (gas and brake) after which commands will be sent. This node checks if the controller is plugged in by polling the linux device filename. If the controller is unplugged the file disapears and this node will start sending break. When the controller is locked or the controller is unplugged the node will continuously sending break setpoints at 10hz. During normal operation the maximum setpoints are limited quite a bit to make the car better controllable. If you enable boost mode (press B) you get more power and the car will move faster. To configure the vlaue mappings between xbox controller and car setpoints, go into src/joystick.cpp and change the value of the variables. HELP it doesn't work Make sure you run this on a computer that has the xbox controller attached! You can check if the controller is available by running the below command. $ sudo ls /dev/input/js0 crwxrwxrwx 1 root 993 13, 0 Nov 8 14:43 /dev/input/js0 If you get permisison erros you have to give ROS more permissions. Run sudo chmod 777 /dev/input/js0 If the device is connected but not available at js0 it might be mapped to another device. Find the correct device by running sudo ls -al /dev/input/js* When you find the correct device mapping, update the launchfile accordingly. The message [ERROR] Couldn't open joystick force feedback! is normal. Nothing to worry about. It is a warning that always happens with wired xbox controllers. testing To test this node on your computer just attach an xbox controller and run it as described above. Now chek the /fsds_ros_bridge/FSCar/control_command topic and you should see values corresponding to your controller movements. You can debug the input values from the joy driver by checking the /joy topic. Subscribers: /joy sensor_msgs/Joy Listens to joystick input which is then mapped to the control command msg. The mapping should feel intuitive but in case something is unclear, it is described in detail in /ros/src/fsds_ros_bridge/src/joystick.cpp Publishers: /fsds_ros_bridge/VEHICLE_NAME/control_command fs_msgs/ControlCommand","title":"Joystick"},{"location":"joystick/#joystick-controller-node","text":"If you are interested in driving the car around to gather training data without having to rely on your autonomous system, you can use an XBox controller to do so. Make sure you have built the ROS workspace . Simply run: cd ros source devel/setup.bash roslaunch joystick joystick.launch This node gets input from a joystick xbox controller (see http://wiki.ros.org/joy) and sends the values to the lowlevel controls (hardware) on topic /fsds_ros_bridge/FSCar/control_command The right trigger (RT) controls the acceleration (gas) The left trigger (LT) controls brake (negative acceleration). The x-axis of the left stick controlls the steering angle. Pressing and holding the B button enables boost mode. When the joy driver initializes it sometimes sends very high values. This would cause the car to move unpredictably. Therefore the controller is 'locked' when it is starts and when the controller is re-plugged in. To unlock both triggers must be fully pressed (gas and brake) after which commands will be sent. This node checks if the controller is plugged in by polling the linux device filename. If the controller is unplugged the file disapears and this node will start sending break. When the controller is locked or the controller is unplugged the node will continuously sending break setpoints at 10hz. During normal operation the maximum setpoints are limited quite a bit to make the car better controllable. If you enable boost mode (press B) you get more power and the car will move faster. To configure the vlaue mappings between xbox controller and car setpoints, go into src/joystick.cpp and change the value of the variables.","title":"Joystick Controller Node"},{"location":"joystick/#help-it-doesnt-work","text":"Make sure you run this on a computer that has the xbox controller attached! You can check if the controller is available by running the below command. $ sudo ls /dev/input/js0 crwxrwxrwx 1 root 993 13, 0 Nov 8 14:43 /dev/input/js0 If you get permisison erros you have to give ROS more permissions. Run sudo chmod 777 /dev/input/js0 If the device is connected but not available at js0 it might be mapped to another device. Find the correct device by running sudo ls -al /dev/input/js* When you find the correct device mapping, update the launchfile accordingly. The message [ERROR] Couldn't open joystick force feedback! is normal. Nothing to worry about. It is a warning that always happens with wired xbox controllers.","title":"HELP it doesn't work"},{"location":"joystick/#testing","text":"To test this node on your computer just attach an xbox controller and run it as described above. Now chek the /fsds_ros_bridge/FSCar/control_command topic and you should see values corresponding to your controller movements. You can debug the input values from the joy driver by checking the /joy topic.","title":"testing"},{"location":"joystick/#subscribers","text":"/joy sensor_msgs/Joy Listens to joystick input which is then mapped to the control command msg. The mapping should feel intuitive but in case something is unclear, it is described in detail in /ros/src/fsds_ros_bridge/src/joystick.cpp","title":"Subscribers:"},{"location":"joystick/#publishers","text":"/fsds_ros_bridge/VEHICLE_NAME/control_command fs_msgs/ControlCommand","title":"Publishers:"},{"location":"operator/","text":"Operator The operator consists of both a web interface and a webserver. The operator is meant to be used by Formula Student officials to control and keep track of what is happening in the simulation. From this web interface, the official can launch and exit the simulator, select teams and events, start, stop and reset the car and view all logs received by the webserver. All these logs are stored in log files, in the case that the operator crashes. The operator is primarily used during competition by officials. You don't need this for development and testing. Refer to the how-to-simulate and how-to-develop guide first. Team config The operator uses the team_config.json file located in the /config folder to load all team specific configuration settings into the simulator. This includes the name of each team and their car settings. Whenever the simulation is started via the operator's web interface, the selected team's car settings are written to the settings.json file located in the main folder of the repository. This allows the operator to quickly switch between teams during competition. The contents of the team_config.json file are also used to dynamically load the team selector buttons. Note that the team_config.json file included in this repository is an example file. The team_config.json used during competition will be confidential. Logs Whenever a mission starts, a log file is created in the /operator/logs folder. All logs received by the webserver will be written to this log file, as long as the mission is ongoing. All log files are named using the following naming convention: {team_name}_{mission}_{date}_{time}.txt Prerequisites The operator only works on windows with wsl. The operator and unreal engine simulator will run in Windows, the ros bridge will be launched from the operator inside wsl. Before we start you must build the ros workspace in wsl and clone the repo in windows. Flask - A Python web application framework. To install all dependencies, run the following command inside the /operator folder: $ pip install -r requirements.txt You must have a packaged simulator downloaded. The operator will launch the game when instructed by the user via the web gui. Go to the releases and download the latest version. Extract the zip to the simulator folder. The result should be that the following file and folders exist inside the simulator folder: FSDS.exe FSOnline/ Engine/ The how-to-develop guide guide describes how to create an export. Usage To start the web server, run the following command in the /operator folder: $ python webserver.py By default, the web interface runs on http://localhost:5000 .","title":"Operator"},{"location":"operator/#operator","text":"The operator consists of both a web interface and a webserver. The operator is meant to be used by Formula Student officials to control and keep track of what is happening in the simulation. From this web interface, the official can launch and exit the simulator, select teams and events, start, stop and reset the car and view all logs received by the webserver. All these logs are stored in log files, in the case that the operator crashes. The operator is primarily used during competition by officials. You don't need this for development and testing. Refer to the how-to-simulate and how-to-develop guide first.","title":"Operator"},{"location":"operator/#team-config","text":"The operator uses the team_config.json file located in the /config folder to load all team specific configuration settings into the simulator. This includes the name of each team and their car settings. Whenever the simulation is started via the operator's web interface, the selected team's car settings are written to the settings.json file located in the main folder of the repository. This allows the operator to quickly switch between teams during competition. The contents of the team_config.json file are also used to dynamically load the team selector buttons. Note that the team_config.json file included in this repository is an example file. The team_config.json used during competition will be confidential.","title":"Team config"},{"location":"operator/#logs","text":"Whenever a mission starts, a log file is created in the /operator/logs folder. All logs received by the webserver will be written to this log file, as long as the mission is ongoing. All log files are named using the following naming convention: {team_name}_{mission}_{date}_{time}.txt","title":"Logs"},{"location":"operator/#prerequisites","text":"The operator only works on windows with wsl. The operator and unreal engine simulator will run in Windows, the ros bridge will be launched from the operator inside wsl. Before we start you must build the ros workspace in wsl and clone the repo in windows. Flask - A Python web application framework. To install all dependencies, run the following command inside the /operator folder: $ pip install -r requirements.txt You must have a packaged simulator downloaded. The operator will launch the game when instructed by the user via the web gui. Go to the releases and download the latest version. Extract the zip to the simulator folder. The result should be that the following file and folders exist inside the simulator folder: FSDS.exe FSOnline/ Engine/ The how-to-develop guide guide describes how to create an export.","title":"Prerequisites"},{"location":"operator/#usage","text":"To start the web server, run the following command in the /operator folder: $ python webserver.py By default, the web interface runs on http://localhost:5000 .","title":"Usage"},{"location":"ros-bridge/","text":"FSDS ROS bridge A ROS wrapper over the AirSim C++ Car client library. This code is based on the original AirSim ROS wrapper for the Multirotor API and provides an interface between AirSim + Unreal Engine and your ros-based autonomous system. The fsds_ros_bridge is supposed to be launched pointing at the Autonomous System's ROS master so that it can publish and subscribe to topics within the autonomous system. Physically this node should run on the airsim simulation server (that is the one that also runs the Unreal) project. The node connects to the AirSim plugin, periodically retrieves sensor data (images, lidar, imu, gps) and publishes it on ROS topics. It listens for car setpoints on other another and forwards these to the AirSim plugin. Running Make sure you have built the ROS workspace . The ros bridge consists of a different few nodes to achieve the highest performance and keep the codebase clean. Everything can be launched using the fsds_ros_bridge.launch launchfile. cd ros source devel/setup.bash roslaunch fsds_ros_bridge fsds_ros_bridge.launch This launches the following nodes: * /fsds/ros_bridge node responsible for IMU, GPS, lidar, vehicle setpoints and go/finish signals. * /fsds/camera/CAMERANAME node is run for each camera configured in the settings.json . The nodes are launched using the cameralauncher.py script. Published topics Topic name Description Message Rate (hz) /fsds/gps This the current GPS coordinates of the drone in airsim. Read all about the gps simulation model here . Data is in the fsds/FSCar frame. sensor_msgs/NavSatFix 10 /fsds/imu Velocity, orientation and acceleration information. Read all about the IMU model here . Data is in the fsds/FSCar (enu) frame. sensor_msgs/Imu 250 /fsds/testing_only/odom Ground truth car position and orientation in ENU frame about the CoG of the car ( fsds/FSCar ). The units are m for distance and rad for angles. The message is in the fsds/map frame. This is a frame that is not (yet) used anywhere else and is just here so you can easely reference it if needed. THIS WILL NOT BE STREAMED DURING COMPETITION. nav_msgs/Odometry 250 /fsds/testing_only/track Ground truth cone position and color with respect to the starting location of the car in ENU. Currently this only publishes the initial position of cones that are part of the track spline. Any cones placed manually in the world are not published here. Additionally, the track is published once and the message is latched (meaning it is always available for a newly created subscriber). THIS WILL NOT BE STREAMED DURING COMPETITION. fs_msgs/Track Latched /fsds/camera/CAMERA_NAME One of this topic type will exist for every camera specified in the settings.json file. On this topic, camera frames are published. The format will be bgra8. CAMERA_NAME will be replaced by the corresponding in the Cameras object in the settings.json file. IMAGE_TYPE is determand by the SensorType field. When choosing 0, it will be 'Scene'. sensor_msgs/Image ~18 /fsds/lidar/LIDARNAME Publishes the lidar points for each lidar sensor. All points are in the fsds/LIDARNAME frame. Transformations between the fsds/LIDARNAME and fsds/FSCar frame are being published regularly. More info on the lidar sensor can be found here sensor_msgs/PointCloud 10 /fsds/signal/go GO signal that is sent every second by the ROS bridge.The car is only allowed to drive once this message has been received. If no GO signal is received for more than 4 seconds, the AS can assume that fsds_ros_bridge has been shut down. This message also includes the mission type and track. More info about signal topics can be found in the integration handbook fs_msgs/GoSignal 1 /tf_static See Coordinate frames and transforms tf2_msgs/TFMessage 1 Subscribed topics Topic name Description Message /fsds/control_command This message includes the dimensionless values throttle, steering and brake. Throttle and brake range from 0 to 1. For steering -1 steers full to the left and +1 steers full to the right. The contents of this message fill the essential parts of the msr::airlib::CarApiBase::CarControl struct. This is the only way to control the car when the airsim ROS client is connected (keyboard will no longer work!). fs_msgs/ControlCommand /fsds/signal/finished Finished signal that is sent by the AS to stop the mission. The ros bridge will forward the signal to the operator which in turn will stop the ros bridge and finish the run. fs_msgs/FinishedSignal Services /fsds/reset fsds_ros_bridge/Reset Resets car to start location. Units If a topic streams a standard ROS message (like sensor_msgs/Imu ) then the units will be the recommended units in the message documentation. Custom messages (from the fs_msgs package) use the units specified in the message documentation as well. If in doubt, interpret distances in meters, angles in radians and rates in m/s and rad/s, etc. Coordinate frames and transforms The primary frame is the fsds/FSCar frame, which is fixed at the center of the car following the ENU coordinate system convention. The center of the car is the Unreal Engine car pawn position, which in turn is also the center of gravity. The ros bridge regularly publishes static transforms between the fsds/FSCar frame and each of the cameras and lidars. Naming of these frames is fsds/SENSORNAME . For example, a lidar named Example will publish it's points in the fsds/Example frame. The position and orientation of a camera named Test will become available in the frame /fsds/Test . PLEASE NOTE : the transforms published on the /tf_static topic are a direct conversion of the transforms specified in the settings.json file but expressed in a ENU coordinate system instead of in a NED coordinate system (which is what the settings.json file uses). Read more about the differences between ENU and NED here . For a quick illustration of the two frames, see the image below: Only static transforms within the vehicle are published. Transforms to the ground truth are disabled because this would take away the state estimation challenge of the competition. Parameters /fsds/ros_bridge/update_gps_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.1 seconds (10hz). Timer callback frequency for updating and publishing the gps sensordata. This value must be equal or higher to the update frequency of the sensor configured in the settings.json /fsds/ros_bridge/update_imu_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.004 seconds (250hz). Timer callback frequency for updating and publishing the imu messages. This value must be equal or higher to the minimual sample rate of the sensor configured in the settings.json /fsds/ros_bridge/update_odom_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.004 seconds (250hz). Timer callback frequency for updating and publishing the odometry. /fsds/ros_bridge/publish_static_tf_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 1 seconds (1 hz). The frequency at which the static transforms are published. /fsds/ros_bridge/update_lidar_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.1 seconds (10 hz). The frequency at which the lidar is publshed. Visualization This package contains some useful launch and config files which will help you in visualizing the data being streamed through the above topics. To open Rviz with this configuration file, run roslaunch fsds_ros_bridge rviz.launch . To open Multiplot with this configuration file, run roslaunch fsds_ros_bridge plot.launch Monitoring Performance monitoring of the ROS Bridge is described here","title":"ROS Bridge"},{"location":"ros-bridge/#fsds-ros-bridge","text":"A ROS wrapper over the AirSim C++ Car client library. This code is based on the original AirSim ROS wrapper for the Multirotor API and provides an interface between AirSim + Unreal Engine and your ros-based autonomous system. The fsds_ros_bridge is supposed to be launched pointing at the Autonomous System's ROS master so that it can publish and subscribe to topics within the autonomous system. Physically this node should run on the airsim simulation server (that is the one that also runs the Unreal) project. The node connects to the AirSim plugin, periodically retrieves sensor data (images, lidar, imu, gps) and publishes it on ROS topics. It listens for car setpoints on other another and forwards these to the AirSim plugin.","title":"FSDS ROS bridge"},{"location":"ros-bridge/#running","text":"Make sure you have built the ROS workspace . The ros bridge consists of a different few nodes to achieve the highest performance and keep the codebase clean. Everything can be launched using the fsds_ros_bridge.launch launchfile. cd ros source devel/setup.bash roslaunch fsds_ros_bridge fsds_ros_bridge.launch This launches the following nodes: * /fsds/ros_bridge node responsible for IMU, GPS, lidar, vehicle setpoints and go/finish signals. * /fsds/camera/CAMERANAME node is run for each camera configured in the settings.json . The nodes are launched using the cameralauncher.py script.","title":"Running"},{"location":"ros-bridge/#published-topics","text":"Topic name Description Message Rate (hz) /fsds/gps This the current GPS coordinates of the drone in airsim. Read all about the gps simulation model here . Data is in the fsds/FSCar frame. sensor_msgs/NavSatFix 10 /fsds/imu Velocity, orientation and acceleration information. Read all about the IMU model here . Data is in the fsds/FSCar (enu) frame. sensor_msgs/Imu 250 /fsds/testing_only/odom Ground truth car position and orientation in ENU frame about the CoG of the car ( fsds/FSCar ). The units are m for distance and rad for angles. The message is in the fsds/map frame. This is a frame that is not (yet) used anywhere else and is just here so you can easely reference it if needed. THIS WILL NOT BE STREAMED DURING COMPETITION. nav_msgs/Odometry 250 /fsds/testing_only/track Ground truth cone position and color with respect to the starting location of the car in ENU. Currently this only publishes the initial position of cones that are part of the track spline. Any cones placed manually in the world are not published here. Additionally, the track is published once and the message is latched (meaning it is always available for a newly created subscriber). THIS WILL NOT BE STREAMED DURING COMPETITION. fs_msgs/Track Latched /fsds/camera/CAMERA_NAME One of this topic type will exist for every camera specified in the settings.json file. On this topic, camera frames are published. The format will be bgra8. CAMERA_NAME will be replaced by the corresponding in the Cameras object in the settings.json file. IMAGE_TYPE is determand by the SensorType field. When choosing 0, it will be 'Scene'. sensor_msgs/Image ~18 /fsds/lidar/LIDARNAME Publishes the lidar points for each lidar sensor. All points are in the fsds/LIDARNAME frame. Transformations between the fsds/LIDARNAME and fsds/FSCar frame are being published regularly. More info on the lidar sensor can be found here sensor_msgs/PointCloud 10 /fsds/signal/go GO signal that is sent every second by the ROS bridge.The car is only allowed to drive once this message has been received. If no GO signal is received for more than 4 seconds, the AS can assume that fsds_ros_bridge has been shut down. This message also includes the mission type and track. More info about signal topics can be found in the integration handbook fs_msgs/GoSignal 1 /tf_static See Coordinate frames and transforms tf2_msgs/TFMessage 1","title":"Published topics"},{"location":"ros-bridge/#subscribed-topics","text":"Topic name Description Message /fsds/control_command This message includes the dimensionless values throttle, steering and brake. Throttle and brake range from 0 to 1. For steering -1 steers full to the left and +1 steers full to the right. The contents of this message fill the essential parts of the msr::airlib::CarApiBase::CarControl struct. This is the only way to control the car when the airsim ROS client is connected (keyboard will no longer work!). fs_msgs/ControlCommand /fsds/signal/finished Finished signal that is sent by the AS to stop the mission. The ros bridge will forward the signal to the operator which in turn will stop the ros bridge and finish the run. fs_msgs/FinishedSignal","title":"Subscribed topics"},{"location":"ros-bridge/#services","text":"/fsds/reset fsds_ros_bridge/Reset Resets car to start location.","title":"Services"},{"location":"ros-bridge/#units","text":"If a topic streams a standard ROS message (like sensor_msgs/Imu ) then the units will be the recommended units in the message documentation. Custom messages (from the fs_msgs package) use the units specified in the message documentation as well. If in doubt, interpret distances in meters, angles in radians and rates in m/s and rad/s, etc.","title":"Units"},{"location":"ros-bridge/#coordinate-frames-and-transforms","text":"The primary frame is the fsds/FSCar frame, which is fixed at the center of the car following the ENU coordinate system convention. The center of the car is the Unreal Engine car pawn position, which in turn is also the center of gravity. The ros bridge regularly publishes static transforms between the fsds/FSCar frame and each of the cameras and lidars. Naming of these frames is fsds/SENSORNAME . For example, a lidar named Example will publish it's points in the fsds/Example frame. The position and orientation of a camera named Test will become available in the frame /fsds/Test . PLEASE NOTE : the transforms published on the /tf_static topic are a direct conversion of the transforms specified in the settings.json file but expressed in a ENU coordinate system instead of in a NED coordinate system (which is what the settings.json file uses). Read more about the differences between ENU and NED here . For a quick illustration of the two frames, see the image below: Only static transforms within the vehicle are published. Transforms to the ground truth are disabled because this would take away the state estimation challenge of the competition.","title":"Coordinate frames and transforms"},{"location":"ros-bridge/#parameters","text":"/fsds/ros_bridge/update_gps_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.1 seconds (10hz). Timer callback frequency for updating and publishing the gps sensordata. This value must be equal or higher to the update frequency of the sensor configured in the settings.json /fsds/ros_bridge/update_imu_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.004 seconds (250hz). Timer callback frequency for updating and publishing the imu messages. This value must be equal or higher to the minimual sample rate of the sensor configured in the settings.json /fsds/ros_bridge/update_odom_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.004 seconds (250hz). Timer callback frequency for updating and publishing the odometry. /fsds/ros_bridge/publish_static_tf_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 1 seconds (1 hz). The frequency at which the static transforms are published. /fsds/ros_bridge/update_lidar_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.1 seconds (10 hz). The frequency at which the lidar is publshed.","title":"Parameters"},{"location":"ros-bridge/#visualization","text":"This package contains some useful launch and config files which will help you in visualizing the data being streamed through the above topics. To open Rviz with this configuration file, run roslaunch fsds_ros_bridge rviz.launch . To open Multiplot with this configuration file, run roslaunch fsds_ros_bridge plot.launch","title":"Visualization"},{"location":"ros-bridge/#monitoring","text":"Performance monitoring of the ROS Bridge is described here","title":"Monitoring"},{"location":"statistics/","text":"ROS Bridge Monitoring ROS Bridge and Rpc performance monitoring is done by the Statistics (/ros/src/fsds_ros_bridge/include/statistics.h) class. This class is used by the AirsimROSWrapper class to monitor latency of rpc calls as well as the frequency of ceratain topics in the ROS network. The statistics gathered by the methods of this class and temporarily stored by private class members will be printed live at about 1Hz. This will be useful both for development of the simulator as well as for ensuring fairness in the competition, where a team will be allowed to retry a run if problems are diagnosed on the simulator side. The following describes how the class and the auxiliary classes Timer and ROSMsgCounter measure performance and are implemented, so that as a developer you can monitor new publishers, subscribers, rpc calls, or actually any line of code in the AirSimROSWrapper class. We are gathering statistics about: - Rpc calls: - getGpsData - getCarState - getImuData (vector) - simGetImages (vector) - getLidarData (vector) - setCarControls - ROS publishing frequency of the following publishers: - odom_local_ned_pub - global_gps_pub - cam_pub_vec_ (vector) - lidar_pub_vec_ (vector) - imu_pub_vec_ (vector) - ROS callback frequency of the following subscriber(s): - control_cmd_sub There will be one instance of this class for each Rpc call to be monitored. To make a latency measurement, the appropriate instance pointer has to be passed to a new Timer class (see below): ros_bridge::Statistics rpcCallStatistics = ros_bridge::Statistics(\"rpcCallName\"); { // Enter scope to be timed ros_bridge::Timer timer(&rpcCallStatistics); // Start timing // do the Rpc Call } // Go out of scope -> call the timer destructor // which automatically stores the time elapsed in the instance of // the class that was passed There will also be an instance of this class for each ROS publisher/subscriber. To count a new incoming or outgoing message the simple construct below can be used: ros_bridge::Statistics pubSubStatistics = ros_bridge::Statistics(\"pubSubName\");; // For a publisher: { // pass persistent Statistics object (by reference) ros_bridge::ROSMsgCounter counter(&pubSubStatistics); pubSub.publish(data); } // For a subscriber: void callback(msg) { // pass persistent Statistics object (by reference) ros_bridge::ROSMsgCounter counter(&pubSubStatistics); // Do something with msg } // scope ends, destructor is called and count is incremented for // the Statistics object In the 1Hz ROS timer, the Print function will be called (the wrapper which applies this action to all the instances) followed by the Reset function (the wrapper which applies this action to all the instances) which ensures that counters are set to 0 and that vectors of durations (latencies) are emptied.","title":"ROS Bridge statistics"},{"location":"statistics/#ros-bridge-monitoring","text":"ROS Bridge and Rpc performance monitoring is done by the Statistics (/ros/src/fsds_ros_bridge/include/statistics.h) class. This class is used by the AirsimROSWrapper class to monitor latency of rpc calls as well as the frequency of ceratain topics in the ROS network. The statistics gathered by the methods of this class and temporarily stored by private class members will be printed live at about 1Hz. This will be useful both for development of the simulator as well as for ensuring fairness in the competition, where a team will be allowed to retry a run if problems are diagnosed on the simulator side. The following describes how the class and the auxiliary classes Timer and ROSMsgCounter measure performance and are implemented, so that as a developer you can monitor new publishers, subscribers, rpc calls, or actually any line of code in the AirSimROSWrapper class. We are gathering statistics about: - Rpc calls: - getGpsData - getCarState - getImuData (vector) - simGetImages (vector) - getLidarData (vector) - setCarControls - ROS publishing frequency of the following publishers: - odom_local_ned_pub - global_gps_pub - cam_pub_vec_ (vector) - lidar_pub_vec_ (vector) - imu_pub_vec_ (vector) - ROS callback frequency of the following subscriber(s): - control_cmd_sub There will be one instance of this class for each Rpc call to be monitored. To make a latency measurement, the appropriate instance pointer has to be passed to a new Timer class (see below): ros_bridge::Statistics rpcCallStatistics = ros_bridge::Statistics(\"rpcCallName\"); { // Enter scope to be timed ros_bridge::Timer timer(&rpcCallStatistics); // Start timing // do the Rpc Call } // Go out of scope -> call the timer destructor // which automatically stores the time elapsed in the instance of // the class that was passed There will also be an instance of this class for each ROS publisher/subscriber. To count a new incoming or outgoing message the simple construct below can be used: ros_bridge::Statistics pubSubStatistics = ros_bridge::Statistics(\"pubSubName\");; // For a publisher: { // pass persistent Statistics object (by reference) ros_bridge::ROSMsgCounter counter(&pubSubStatistics); pubSub.publish(data); } // For a subscriber: void callback(msg) { // pass persistent Statistics object (by reference) ros_bridge::ROSMsgCounter counter(&pubSubStatistics); // Do something with msg } // scope ends, destructor is called and count is incremented for // the Statistics object In the 1Hz ROS timer, the Print function will be called (the wrapper which applies this action to all the instances) followed by the Reset function (the wrapper which applies this action to all the instances) which ensures that counters are set to 0 and that vectors of durations (latencies) are emptied.","title":"ROS Bridge Monitoring"},{"location":"system-overview/","text":"This document is intended to read top-to-bottom. Do yourself a favour and read the whole thing without skipping ;) Formula Student Driverless Simulation: System overview FSDS is built around Unreal Engine 4 (the game engine) and the AirSim plugin. The game engine does all the graphical rendering, collision simulation and car movement simulation. A separate component - the simulator - will handle all control of the simulation, provide external interfaces and store what is happening. ! We will also provide a video feed of the simulation for live streaming purposes. Since it is still unclear how this will work this is not included in this description. Autonomous systems Every Autonomous System (AS) will run on its separated environment. Ideally, this would be separate virtual machines but also Docker containers could be used. The ASs are expected to continuously run a ROS master. When the simulator is ready to do a drive for the given AS it will launch a ROS node connected to this AS's ROS master. This ROS node will exist outside of the AS's environment. Instead, it will run on the simulator computer. The simulator ROS node will publish sensor data and listen for car setpoints on a set of topics defined here . When the simulation is finished, the ROS node will disconnect from the AS's ROS master and sensor data stops coming in. During the competition, the teams will not be allowed to access their ASs. All remote access to the environments will be cut off completely. So every AS must be able to run multiple missions without human interference. To let the simulation know what is expected from it, the simulator will send signal messages a few seconds before the event start. These messages contain information about the mission (trackdrive, autocross, etc). When the AS receives a mission message it can expect to receive sensor data shortly after. Since the lap timer for all events start whenever the car crosses the start-line, the ASs can take all the time they need to launch their relevant algorithms for the mission (within a reasonable time). The Operator The operator is a continuously running program that is like the spider in the web. The operator offers a web interface to the event's officials. Using this web interface the officials can choose which team/car is going to drive and on which track. The official will also be able to send the start signal, view lap times, down or out cones, car off course's. There is also an emergency stop button in case the car is uncontrollable. Only 1 car at the time will be able to run on this AS. The official can select which team, and thus which AS, is currently selected. When the team changes, not only the AS but also the car inside the virtual world will change. The sensor suite (sensor types and locations, defined in a custom settings.json file) and the car livery (looks of the car) are updated. The operator keeps track of these details and passes them along to the virtual world to ensure accurate representation. When the operator wants to connect an AS to the simulated world, it launches the ROS bridge. Read more about the ROS bridge below. If the operator wants to disconnect the AS from the simulated world it stops the bridge node and the connections are stopped. During a mission, the operator keeps polling the world for 'referee state'. This is information that in a physical world would be relevant to the referee. Currently, this includes a list of down or out cones and the timestamp of when they went down or out, a list of lap times and a list of when the car went off-track. More information will for sure be added. What happens inside the simulation is stored in a single logbook. This includes all referee updates, which ASs were selected and which tracks were used. If something unexpected went wrong like a system's crash or error, a short description of what happened is included in this logbook. It gives a timeline of everything that happened to always go back afterwards and check what happened. The logbook is stored on disk so that in the event of a whole system crash we will still have the logbook. It is also shown within the referee's web interface. You can run the simulation stack yourself perfectly fine without the operator. The operator is just a tool for officials to easely manage the competition. Read more about how to run the simulation here. The ROS Bridge The ROS bridge node connects to the simulated world inside Unreal Engine using AirSim (more on that later). On the one hand, it requests sensor data and passes it along on ROS topics to the current AS. On the other hand, it receives car control commands from the AS and forwards it to the virtual world. So it acts as a bridge between the two. The node that is launched pointing at the AS's ROS master so that it can publish and subscribe to topics within the AS. Physically this node runs on the server where the Unreal world is being simulated. The node is launched by the operator. When the operator launches the ROS bridge it passes along some mission variables. This includes mission type (trackdrive or autocross) and information about how it can use data collected in previous runs. For example, first the AS will receive \"autocross on track A\" and it knows it cannot use any previous collected information. Then it receives \"trackdrive on track A\" and it knows it can use data collected in the first autocross drive to go faster. It is the responsibilty of the teams to detect when they are 'done'. After the required number of laps, the car has to come to a full stop. If the AS wants to store things (like track information), this is the time to wrap those up. In case of a successfull run the official will instruct the operator to stop the ROS bridge and the AS won't receive sensor data anymore. When the official presses the emergency brake, the connection between the ROS bridge is stopped immediately and the operator will send one last car setpoint to make the car come to a stop. Thers is no 'stop' signal from the simulator to the AS. At this point only ROS is supported, at this moment there are no plans to support other technologies. The virtual world (Unreal Engine and AirSim) The actual simulation takes place inside an Unreal Engine 4 world. Unreal takes care of the heavy lifting involved with a real-life simulation. All physics, lighting and world-building are handled by Unreal. AirSim is used to connect Unreal with the operator and ROS bridge. This plugin is added inside the Unreal world and takes control of most of the game logic. It receives the sensor suite and simulates the sensors, it moves the car according to trajectory setpoints and exposes an RPC API for external management. This RPC API is used by the simulator to interact with the world.","title":"System overview"},{"location":"system-overview/#formula-student-driverless-simulation-system-overview","text":"FSDS is built around Unreal Engine 4 (the game engine) and the AirSim plugin. The game engine does all the graphical rendering, collision simulation and car movement simulation. A separate component - the simulator - will handle all control of the simulation, provide external interfaces and store what is happening. ! We will also provide a video feed of the simulation for live streaming purposes. Since it is still unclear how this will work this is not included in this description.","title":"Formula Student Driverless Simulation: System overview"},{"location":"system-overview/#autonomous-systems","text":"Every Autonomous System (AS) will run on its separated environment. Ideally, this would be separate virtual machines but also Docker containers could be used. The ASs are expected to continuously run a ROS master. When the simulator is ready to do a drive for the given AS it will launch a ROS node connected to this AS's ROS master. This ROS node will exist outside of the AS's environment. Instead, it will run on the simulator computer. The simulator ROS node will publish sensor data and listen for car setpoints on a set of topics defined here . When the simulation is finished, the ROS node will disconnect from the AS's ROS master and sensor data stops coming in. During the competition, the teams will not be allowed to access their ASs. All remote access to the environments will be cut off completely. So every AS must be able to run multiple missions without human interference. To let the simulation know what is expected from it, the simulator will send signal messages a few seconds before the event start. These messages contain information about the mission (trackdrive, autocross, etc). When the AS receives a mission message it can expect to receive sensor data shortly after. Since the lap timer for all events start whenever the car crosses the start-line, the ASs can take all the time they need to launch their relevant algorithms for the mission (within a reasonable time).","title":"Autonomous systems"},{"location":"system-overview/#the-operator","text":"The operator is a continuously running program that is like the spider in the web. The operator offers a web interface to the event's officials. Using this web interface the officials can choose which team/car is going to drive and on which track. The official will also be able to send the start signal, view lap times, down or out cones, car off course's. There is also an emergency stop button in case the car is uncontrollable. Only 1 car at the time will be able to run on this AS. The official can select which team, and thus which AS, is currently selected. When the team changes, not only the AS but also the car inside the virtual world will change. The sensor suite (sensor types and locations, defined in a custom settings.json file) and the car livery (looks of the car) are updated. The operator keeps track of these details and passes them along to the virtual world to ensure accurate representation. When the operator wants to connect an AS to the simulated world, it launches the ROS bridge. Read more about the ROS bridge below. If the operator wants to disconnect the AS from the simulated world it stops the bridge node and the connections are stopped. During a mission, the operator keeps polling the world for 'referee state'. This is information that in a physical world would be relevant to the referee. Currently, this includes a list of down or out cones and the timestamp of when they went down or out, a list of lap times and a list of when the car went off-track. More information will for sure be added. What happens inside the simulation is stored in a single logbook. This includes all referee updates, which ASs were selected and which tracks were used. If something unexpected went wrong like a system's crash or error, a short description of what happened is included in this logbook. It gives a timeline of everything that happened to always go back afterwards and check what happened. The logbook is stored on disk so that in the event of a whole system crash we will still have the logbook. It is also shown within the referee's web interface. You can run the simulation stack yourself perfectly fine without the operator. The operator is just a tool for officials to easely manage the competition. Read more about how to run the simulation here.","title":"The Operator"},{"location":"system-overview/#the-ros-bridge","text":"The ROS bridge node connects to the simulated world inside Unreal Engine using AirSim (more on that later). On the one hand, it requests sensor data and passes it along on ROS topics to the current AS. On the other hand, it receives car control commands from the AS and forwards it to the virtual world. So it acts as a bridge between the two. The node that is launched pointing at the AS's ROS master so that it can publish and subscribe to topics within the AS. Physically this node runs on the server where the Unreal world is being simulated. The node is launched by the operator. When the operator launches the ROS bridge it passes along some mission variables. This includes mission type (trackdrive or autocross) and information about how it can use data collected in previous runs. For example, first the AS will receive \"autocross on track A\" and it knows it cannot use any previous collected information. Then it receives \"trackdrive on track A\" and it knows it can use data collected in the first autocross drive to go faster. It is the responsibilty of the teams to detect when they are 'done'. After the required number of laps, the car has to come to a full stop. If the AS wants to store things (like track information), this is the time to wrap those up. In case of a successfull run the official will instruct the operator to stop the ROS bridge and the AS won't receive sensor data anymore. When the official presses the emergency brake, the connection between the ROS bridge is stopped immediately and the operator will send one last car setpoint to make the car come to a stop. Thers is no 'stop' signal from the simulator to the AS. At this point only ROS is supported, at this moment there are no plans to support other technologies.","title":"The ROS Bridge"},{"location":"system-overview/#the-virtual-world-unreal-engine-and-airsim","text":"The actual simulation takes place inside an Unreal Engine 4 world. Unreal takes care of the heavy lifting involved with a real-life simulation. All physics, lighting and world-building are handled by Unreal. AirSim is used to connect Unreal with the operator and ROS bridge. This plugin is added inside the Unreal world and takes control of most of the game logic. It receives the sensor suite and simulates the sensors, it moves the car according to trajectory setpoints and exposes an RPC API for external management. This RPC API is used by the simulator to interact with the world.","title":"The virtual world (Unreal Engine and AirSim)"},{"location":"vehicle_model/","text":"Vehicle Dynamic model One of the most controversial subjects of any competition simulator is the vehicle dynamic model. This is the piece of the simulation that actually changes the state of the vehicle. In building this simulator for the FSOnline competition, our design philosophy was the following: All teams will use the same vehicle dynamic model . We are well aware that all teams have put effort into developing dynamic models of their own FSCar for controls and simulation purposes. However, we want the FSOnline DV Dynamic event to purely be a battle of autonomous software. Even if this will require teams to tweak their path planning and control algorithms, it will make sure that the winner of this event is truly the team that can take any race car and push it to its limits the most. The dynamic model will have a high enough fidelity such that it is virtually impossible to overfit to it/reverse the plant or run open loop . This will force teams to use system identification techniques similar to the ones that are used on a real car and no cheating or unfair advantage will be given to any teams. A third-party, open-source model would be ideal . This way, not even the developers of the simulation (Formula Student Team Delft) would have an edge over other teams. Everyone has access to the same code and has had no experience working with it or been involved in developing it. The Unreal Engine repository contains (as a third party library) the code for PhysXVehicles that was developed by Nvidia. This seemed like the perfect solution for our simulator, given that it complies with all the criteria of our design philosophy above. Airsim simply interacts with the PhysXCar API in these files, which have configurable parameters exposed in the uassets files here . These are the current (high-level) vehicle parameters (you will be notified if anything changes): Mass: 255 kg Max speed: ~27 m/s Drag coefficient: 0.3 [-] Chassis width: 1.44 m Chassis height: 1.22 m These parameters can only be found when opening the UE4Editor and opening the SUVCarPawn.uasset file located here .","title":"Vehicle model"},{"location":"vehicle_model/#vehicle-dynamic-model","text":"One of the most controversial subjects of any competition simulator is the vehicle dynamic model. This is the piece of the simulation that actually changes the state of the vehicle. In building this simulator for the FSOnline competition, our design philosophy was the following: All teams will use the same vehicle dynamic model . We are well aware that all teams have put effort into developing dynamic models of their own FSCar for controls and simulation purposes. However, we want the FSOnline DV Dynamic event to purely be a battle of autonomous software. Even if this will require teams to tweak their path planning and control algorithms, it will make sure that the winner of this event is truly the team that can take any race car and push it to its limits the most. The dynamic model will have a high enough fidelity such that it is virtually impossible to overfit to it/reverse the plant or run open loop . This will force teams to use system identification techniques similar to the ones that are used on a real car and no cheating or unfair advantage will be given to any teams. A third-party, open-source model would be ideal . This way, not even the developers of the simulation (Formula Student Team Delft) would have an edge over other teams. Everyone has access to the same code and has had no experience working with it or been involved in developing it. The Unreal Engine repository contains (as a third party library) the code for PhysXVehicles that was developed by Nvidia. This seemed like the perfect solution for our simulator, given that it complies with all the criteria of our design philosophy above. Airsim simply interacts with the PhysXCar API in these files, which have configurable parameters exposed in the uassets files here . These are the current (high-level) vehicle parameters (you will be notified if anything changes): Mass: 255 kg Max speed: ~27 m/s Drag coefficient: 0.3 [-] Chassis width: 1.44 m Chassis height: 1.22 m These parameters can only be found when opening the UE4Editor and opening the SUVCarPawn.uasset file located here .","title":"Vehicle Dynamic model"}]}